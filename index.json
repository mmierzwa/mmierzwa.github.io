[{"content":"","date":null,"permalink":"/","section":"byteloom - Marek Mierzwa","summary":"","title":"byteloom - Marek Mierzwa"},{"content":"","date":null,"permalink":"/posts/","section":"Posts","summary":"","title":"Posts"},{"content":"","date":null,"permalink":"/tags/acceptance-tests/","section":"Tags","summary":"","title":"Acceptance Tests"},{"content":"","date":null,"permalink":"/categories/","section":"Categories","summary":"","title":"Categories"},{"content":"This text will be rather reflective than instructional, which can be surprising compared to other articles on this blog. However after almost 7 years without any new text, this shouldn\u0026rsquo;t be any shock, even if you - somehow - managed to follow me here ;-)\nI was lucky enough to work at my first professional projects with engineers experienced enough to introduce me to TDD on day one. At that time, the one-and-only testing approach in this craft was the classic testing pyramid: you write a lot of unit test covering almost all your code in isolated manner, much less integration/acceptance tests, and some basic end-to-end tests, mainly for happy paths. (Later came other types and also terminology became to blur.) This looked more or less like this.\nA classic testing pyramid The main issue with integration, e2e, acceptance, etc. tests was that they were slow. In fact, I remember they were so slow (and sometimes difficult to run deterministically each time) that running them in the TDD loop was defeating the purpose of methodology. So we were avoiding them as much as possible and the pyramid shape of our test suite seemed perfectly reasonable.\nOn the other hand, while I could detect some issues with unit tests, many times they were able to detect only the issues related to more complex logic, and large portion of the backend is rather a simple pipeline that maps and passes the data: HTTP adapter -\u0026gt; controller -\u0026gt; DB/queue/external service. One simple acceptance test could potentially catch a dozen of bugs. In fact, many of errors I make can be detected only in integration with the database, HTTP server and external services.\nAs soon as I started using containers in my daily backend development (which was late, around 2018) I realized that the main reasons for writing little integration, acceptance or e2e tests might have just gone away. On a relatively average hardware available for most developers I could run my app along with the Postgres, RabbitMQ, Kafka, etc. Thanks to simulators like Localstack, I could mock most of my cloud environment. I could mock external APIs with Wiremock or similar tool. And everything running within an isolated networked environment in containers. If I needed, I could even inject some network-level issues to test connectivity resilience, as both Docker Compose and Kubernetes allowed for such scenarios.\nThe next realization was that in such environment my app can be run as close to the production configuration as possible. I don\u0026rsquo;t even have to change hostnames as the container network was kind enough to allow for aliases. (I only need to have a local PKI infrastructure working with my app.) Many, if not the most, of the bugs that sneaked into production was not related to the application logic, as it was often quite simple, but to the typical setup issues - wrong configuration, missing IoC dependencies setup, mistyped environment variable name in the code, etc. Now, I could test all those potential issues with no artificial code added to the production codebase just for testing purposes.\nAll those realizations and first-hand experience with real production bugs convinced me to change my approach for testing. While I still love TDD for reasons less related with testing (design, speeding up coding), I write much less unit tests now than I used to. I write unit tests mainly when I have more complex logic, validation, error handling, and edge cases that are impractical (because of number of cases and related performance) to code as integration/acceptance tests. It happens that after I write some unit tests to design my code, I delete them as soon I implement some functionality and ensure that it\u0026rsquo;s tested by higher-level tests, treating those tests as scaffold. I write as many integration/acceptance tests as needed to cover most of the app code (with no specific % target on the coverage itself). Together with the blessing of CI, this approach allows teams I work with to move fast with a great quality.\nThe shape of this testing approach looks more like this.\nMore pragmatic testing stack Of course, the exact proportions will look different for different types of projects. The key point here is that I believe our industry and craft have matured enough, in terms of experience gathered and technical expertise, to move from dogmas to more pragmatic approach. In conjunction with the advancement in tooling and technology, it created a good ground for writing the software efficiently without sacrificing the quality.\nI hope you, dear reader, will find this text somehow useful. If you do, please, let me know what you think in comments bellow and share this article with others.\nHave a wonderful day (or night) and see you soon!\n","date":"8 February 2025","permalink":"/blog/do-we-still-need-testing-pyramid/","section":"Posts","summary":"\u003cp\u003eThis text will be rather reflective than instructional, which can be surprising compared to other articles on this blog. However after almost 7 years without any new text, this shouldn\u0026rsquo;t be any shock, even if you - somehow - managed to follow me here ;-)\u003c/p\u003e","title":"Do we still need testing pyramid?"},{"content":"","date":null,"permalink":"/tags/","section":"Tags","summary":"","title":"Tags"},{"content":"","date":null,"permalink":"/categories/testing/","section":"Categories","summary":"","title":"Testing"},{"content":"","date":null,"permalink":"/tags/unit-tests/","section":"Tags","summary":"","title":"Unit Tests"},{"content":"","date":null,"permalink":"/categories/mobile/","section":"Categories","summary":"","title":"Mobile"},{"content":"In my previous post I showed how to introduce search as type behavior into Xamarin.Forms app with standard Forms Behaviors.\nLet\u0026rsquo;s see how to do it in other, more declarative and configurable manner, using Reactive Extensions (Rx) instead of Task and CancellationTokenSource.\nTo keep the code as short as possible I\u0026rsquo;ll further assume that you have read the previous article and have it opened as a reference. The implementation uses the same Forms Behavior as the previous one did, but the delay between search phrase changes and running the search will be handled differently.\nWith Reactive Extensions (Rx) you don\u0026rsquo;t handle the events directly. Instead, you are using an observer design pattern with Rx Observable object. This object is responsible for watching the source changes and reacting in a defined way.\nIn our case, the source is the SearchBar and changes are represented by TextChanged events. To create Observable from events Rx uses Observable.FromEventPattern() method. It handles both adding and removing the handlers (on disposal):\nprivate readonly IDisposable _subscription; public SearchAsYouTypeBehavior() { _subscription = Observable.FromEventPattern\u0026lt;TextChangedEventArgs\u0026gt;( handler =\u0026gt; AssociatedObject.TextChanged += handler, handler =\u0026gt; AssociatedObject.TextChanged -= handler); // ... } In the original concept, the delay was implemented with standard Task cancellation pattern. Rx is actually designed exactly for this kind of behaviors. To introduce a delay simply use Observable.Throttle() method:\n_subscription.Throttle(TimeSpan.FromMilliseconds(MinimumSearchIntervalMiliseconds)); Next we will have to ensure that all further operations are performed on the UI thread:\n_subscription.ObserveOn(SynchronizationContext.Current); Rx provides a convenient way of dealing the event streams with Linq. We can use the standard operators, such as Select or Where to transform the original object representing changes in virtually any way you wish. We will use this feature to select the search phrase that will be used in further processing:\n_subscription.Select(eventPattern =\u0026gt; AssociatedObject.Text) .DistinctUntilChanged(); The DistinctUntilChanged() method, in this case, ensures that observable subscription will not be run until the selected text has really been changed. A user can, for example, type and immediately delete a letter. Using DistinctUntilChanged() will prevent recognizing this as a change.\nFinally, here is the actual search run configuration:\n_subscription.Subscribe(query =\u0026gt; Device.BeginInvokeOnMainThread(() =\u0026gt; { SearchCommand?.Execute(query); }) ); Of course you can perform the entire configuration in single fluent expression. I broke down the code just for the better description of each step.\nReactive Extensions is an amazing tool which you can use solve many common problems in mobile apps development, like background fetching or recurrent refresh. Check out the 101 Rx Samples for more examples.\nHappy coding!\n","date":"23 June 2018","permalink":"/blog/search-as-you-type-in-xamarin-forms-part-2/","section":"Posts","summary":"\u003cp\u003eIn \u003ca href=\"/blog/search-as-you-type-in-xamarin-forms\"\u003emy previous post\u003c/a\u003e I showed how to introduce search as type behavior into Xamarin.Forms app with standard \u003ca href=\"https://docs.microsoft.com/en-us/xamarin/xamarin-forms/app-fundamentals/behaviors/\" target=\"_blank\" rel=\"noreferrer\"\u003eForms Behaviors\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eLet\u0026rsquo;s see how to do it in other, more declarative and configurable manner, using \u003ca href=\"https://github.com/dotnet/reactive\" target=\"_blank\" rel=\"noreferrer\"\u003eReactive Extensions (Rx)\u003c/a\u003e instead of \u003ccode\u003eTask\u003c/code\u003e and \u003ccode\u003eCancellationTokenSource\u003c/code\u003e.\u003c/p\u003e","title":"Search as you type in Xamarin.Forms - the Reactive Extensions way"},{"content":"","date":null,"permalink":"/tags/xamarin/","section":"Tags","summary":"","title":"Xamarin"},{"content":"","date":null,"permalink":"/tags/xamarin-forms/","section":"Tags","summary":"","title":"Xamarin Forms"},{"content":"Search as type functionality is quite often seen on the web as well as in mobile apps. Let\u0026rsquo;s see how to make it work in Xamarin.Forms.\nThe simplest way of implementing such behavior is to run a search on each phase change. In other words - every time user types or deletes a letter in a search box the search function (and results list update) is performed. As you may already see, this is not the most efficient way of doing this.\nUsually, when the user changes the search phrase the search is delayed for a short timespan (less than 500ms). If in the meantime next search phrase change occurs the search for the previous phrase is canceled and delay timer starts again. When the delay is not interrupted by further search phrase changes the actual search is being run.\nI personally find using Xamarin.Forms behaviors more elegant than hooking-up the event directly to controls (on page level). It\u0026rsquo;s also more flexible than writing custom controls. Actually creating custom controls and custom renderers should be your last choice (if you can achieve the same with Behavior or Effect).\nHere is the example of behavior that can be applied to Forms SearchBar.\nusing System; using System.Threading; using System.Threading.Tasks; using System.Windows.Input; using Behaviors; using Xamarin.Forms; namespace MyApp { public class SearchAsYouTypeBehavior : BehaviorBase\u0026lt;SearchBar\u0026gt; { public const int DefaultMinimumSearchIntervalMiliseconds = 300; private CancellationTokenSource _cancellationTokenSource; public static readonly BindableProperty SearchCommandProperty = BindableProperty.Create(nameof(SearchCommand), typeof(ICommand), typeof(SearchAsYouTypeBehavior), propertyChanged: SearchCommandChanged); public static readonly BindableProperty MinimumSearchIntervalMilisecondsProperty = BindableProperty.Create(nameof(MinimumSearchIntervalMiliseconds), typeof(int), typeof(SearchAsYouTypeBehavior), DefaultMinimumSearchIntervalMiliseconds); public ICommand SearchCommand { get =\u0026gt; (ICommand) GetValue(SearchCommandProperty); set =\u0026gt; SetValue(SearchCommandProperty, value); } public int MinimumSearchIntervalMiliseconds { get =\u0026gt; (int) GetValue(MinimumSearchIntervalMilisecondsProperty); set =\u0026gt; SetValue(MinimumSearchIntervalMilisecondsProperty, value); } protected override void OnDetachingFrom(SearchBar bindable) { base.OnDetachingFrom(bindable); AssociatedObject.TextChanged -= Search; } private static void SearchCommandChanged(BindableObject bindable, object oldValue, object newValue) { var behavior = (SearchAsYouTypeBehavior) bindable; behavior.SearchCommandChanged(newValue); } private void SearchCommandChanged(object newCommand) { if (newCommand is ICommand) AssociatedObject.TextChanged += Search; else AssociatedObject.TextChanged -= Search; } private async void Search(object sender, TextChangedEventArgs textChangedEventArgs) { _cancellationTokenSource?.Cancel(); _cancellationTokenSource = new CancellationTokenSource(); var cancellationToken = _cancellationTokenSource.Token; try { await Task.Delay(MinimumSearchIntervalMiliseconds, cancellationToken); if (cancellationToken.IsCancellationRequested) return; ExecuteSearch(); } catch (OperationCanceledException) { // swallow } } private void ExecuteSearch() { Device.BeginInvokeOnMainThread(() =\u0026gt; { SearchCommand?.Execute(null); if (!AssociatedObject.IsFocused) AssociatedObject.Focus(); }); } } } Since it defines ICommand as a bindable property it can be binded in standard MVVM way in XAML:\n\u0026lt;SearchBar SearchCommand=\u0026#34;{Binding SearchCommand}\u0026#34; Text=\u0026#34;{Binding SearchQuery}\u0026#34;\u0026gt; \u0026lt;SearchBar.Behaviors\u0026gt; \u0026lt;behaviors:SearchAsYouTypeBehavior SearchCommand=\u0026#34;{Binding SearchCommand}\u0026#34; /\u0026gt; \u0026lt;/SearchBar.Behaviors\u0026gt; \u0026lt;/SearchBar\u0026gt; You can also specify MinimumSearchIntervalMiliseconds to decide how long should be the delay between searches.\nIn next post, I\u0026rsquo;ll show you how to implement this behavior even simpler and more efficiently with Reactive Extensions (Rx).\nHappy coding!\n","date":"18 June 2018","permalink":"/blog/search-as-you-type-in-xamarin-forms/","section":"Posts","summary":"\u003cp\u003eSearch as type functionality is quite often seen on the web as well as in mobile apps. Let\u0026rsquo;s see how to make it work in Xamarin.Forms.\u003c/p\u003e","title":"Implementing search as type in Xamarin.Forms search bar"},{"content":"","date":null,"permalink":"/tags/android/","section":"Tags","summary":"","title":"Android"},{"content":"Detecting on-screen keyboard toggles and proper handling of such changes can be quite tricky. Android tries to deal with those events on its own but its behavior is often far from perfect. iOS, on the other hand, leaves all the work to an app developer. Both approaches have its advantages and disadvantages but sooner or later each mobile app dev will have to face this problem.\nWhile there are few posts or SO questions on this matter I found no comprehensive text so far.\nIn this article, I\u0026rsquo;ll focus on detection. I will show you how to instrument your Xamarin.Forms app so you could react on soft keyboard toggles in a unified manner both on iOS and Android.\nThe general idea is to have a platform component that, once started, will notify the entire app about two facts: that the software keyboard is displayed/hidden and what is its height.\nI decided to use the cross-platform messaging mechanism. I use it very rare cases when I really find no better and cleaner solution. It\u0026rsquo;s really easy to lose control over the app behaviour when MessagingCenter or MvvmCross Messenger is overused. In this case, however, I found this approach clean enough.\nJust one more thing before we start. In my Forms apps I use wrapper types to have strongly typed messages, so don\u0026rsquo;t be surprised when you see such constructs in further code (interface IMessenger and messages derived from Message base). I don\u0026rsquo;t get into much details about messaging implementation to not move away from the main topic.\nHere is the message that represents keyboard toggles:\npublic class KeyboardToggledMessage : Message { public bool IsDisplayed { get; } public double KeyboardHeight { get; } public KeyboardToggledMessage(object sender, bool isDisplayed, double keyboardHeight) : base(sender) { IsDisplayed = isDisplayed; KeyboardHeight = keyboardHeight; } } And this is the keyboard toggle detector abstraction that will be implemented on each platform separately:\npublic interface IKeyboardNotificationProvider { void StartNotifying(); void StopNotifying(); } iOS implementation is pretty simple since the UIKit operates on exactly the same concept that is being implemented here. It just needs to be adapted to the cross-platform form:\npublic class KeyboardNotificationProvider : IKeyboardNotificationProvider { private static IMessenger Messenger =\u0026gt; ServiceLocator.Messenger; private double _keyboardHeight; public void StartNotifying() { UIKeyboard.Notifications.ObserveWillShow(OnKeyboardShown); UIKeyboard.Notifications.ObserveWillHide(OnKeyboardHidden); } public void StopNotifying() { } private void OnKeyboardShown(object sender, UIKeyboardEventArgs e) { var frame = e.FrameEnd; _keyboardHeight = frame.Height; Messenger.Send(new KeyboardToggledMessage(this, true, _keyboardHeight)); } private void OnKeyboardHidden(object sender, UIKeyboardEventArgs e) { Messenger.Send(new KeyboardToggledMessage(this, false, _keyboardHeight)); } } The height of the keyboard is given in the same units as in Forms, so no additional calculations are required here.\nThe registration is done in AppDelegate.FinishedLaunching() method:\npublic class AppDelegate : Xamarin.Forms.Platform.iOS.FormsApplicationDelegate { public override bool FinishedLaunching(UIApplication application, NSDictionary launchOptions) { Xamarin.Forms.Forms.Init(); InitContainer(); LoadApplication(new App()); ServiceLocator.KeyboardNotificationProvider.StartNotifying(); return base.FinishedLaunching(application, launchOptions); } } The Android implementation is more complex. It\u0026rsquo;s based on the concept of global layout changes listeners.\nThere is no straightforward way of detecting if the keyboard has been displayed or hidden. Fortunately, we can make a quite confident guess about this event by checking if the system is currently expecting text input from the user.\nThe second obstacle is calculating the keyboard height. The implementation bellow caches and updates the height based on purely empiric constant KeyboardDisplayedToHiddenRatio. It is the ratio of the app screen height visible to the user (only app controls) to the height of area hidden to the user.\npublic class KeyboardNotificationProvider : IKeyboardNotificationProvider { private KeyboardListner _keyboardListner; private static ViewTreeObserver CurrentViewTreeObserver =\u0026gt; ActivityProvider.RootContentView.ViewTreeObserver; public void StartNotifying() { _keyboardListner = _keyboardListner ?? new KeyboardListner(); CurrentViewTreeObserver.AddOnGlobalLayoutListener(_keyboardListner); } public void StopNotifying() { CurrentViewTreeObserver.RemoveOnGlobalLayoutListener(_keyboardListner); } [Register(\u0026#34;mymd.mobile.droid.services.KeyboardListner\u0026#34;)] public class KeyboardListner : Java.Lang.Object, ViewTreeObserver.IOnGlobalLayoutListener { private static IMessenger Messenger =\u0026gt; ServiceLocator.Messenger; private const float KeyboardDisplayedToHiddenRatio = 0.15f; private double _keyboardHeight; private static InputMethodManager _inputManager; public KeyboardListner() { _inputManager = GetInputManager(); } public KeyboardListner(IntPtr handle, JniHandleOwnership transfer) : base(handle, transfer) { } public void OnGlobalLayout() { TryCalculateKeyboardHeight(); NotifyOnKeyboardToggled(); } private void TryCalculateKeyboardHeight() { var contentView = ActivityProvider.RootContentView; if (contentView == null) return; var windowVisibleDisplayFrame = new Rect(); contentView.GetWindowVisibleDisplayFrame(windowVisibleDisplayFrame); var visibleScreenHeight = contentView.RootView.Height; var potentialKeyboardHeight = visibleScreenHeight - windowVisibleDisplayFrame.Bottom; if (potentialKeyboardHeight \u0026gt; visibleScreenHeight * KeyboardDisplayedToHiddenRatio) _keyboardHeight = Math.Ceiling(potentialKeyboardHeight.ToFormsScreenValue()); } private void NotifyOnKeyboardToggled() { if (_inputManager.Handle == IntPtr.Zero) _inputManager = GetInputManager(); if (_inputManager.IsAcceptingText \u0026amp;\u0026amp; _keyboardHeight \u0026gt; 0) Messenger.Send(new KeyboardToggledMessage(this, true, _keyboardHeight)); else Messenger.Send(new KeyboardToggledMessage(this, false, _keyboardHeight)); } private static InputMethodManager GetInputManager() =\u0026gt; (InputMethodManager) ActivityProvider.CurrentActivity.GetSystemService(Context.InputMethodService); } } Additionally, Android screen units are not 1:1 with the Forms units since they strictly depend on screen density. That\u0026rsquo;s why we will need some additional extension class to calculate the final measure:\npublic static class ControlsExtension { public static DisplayMetrics DisplayMetrics =\u0026gt; Application.Context.Resources.DisplayMetrics; public static float DisplayDensity =\u0026gt; DisplayMetrics.Density; public static double ToFormsScreenValue(this int androidScreenValue) =\u0026gt; (double) androidScreenValue / DisplayDensity; } I also use this simple helper class to obtain the current activity (regardless of the fact that Forms usually use only single activity):\npublic class ActivityProvider { public static Activity CurrentActivity { get; set; } public static View RootContentView =\u0026gt; CurrentActivity.FindViewById(Android.Resource.Id.Content); } The initialisation is done in app\u0026rsquo;s MainActivity:\npublic class MainActivity : Xamarin.Forms.Platform.Android.FormsAppCompatActivity { protected override void OnCreate(Bundle bundle) { ActivityProvider.CurrentActivity = this; //... base.OnCreate(bundle); InitContainer(); Forms.Init(this, bundle); ServiceLocator.KeyboardNotificationProvider.StartNotifying(); LoadApplication(new App()); } protected override void OnDestroy() { base.OnDestroy(); ServiceLocator.KeyboardNotificationProvider.StopNotifying(); } protected override void OnResume() { base.OnResume(); ServiceLocator.KeyboardNotificationProvider.StartNotifying(); } } The example consumption can be implemented using the observer pattern:\npublic class KeyboardToggledObserver : MessageObserver\u0026lt;KeyboardToggledMessage\u0026gt; { private bool _previousKeyboardDisplayed; protected override void OnMessageArrived(KeyboardToggledMessage message) { if (_previousKeyboardDisplayed != message.IsDisplayed) { Debug.WriteLine($\u0026#34;[{nameof(KeyboardToggledObserver)}] Keboard toggled: displayed={message.IsDisplayed}, height={message.KeyboardHeight}\u0026#34;); _previousKeyboardDisplayed = !_previousKeyboardDisplayed; } } } Again, forgive me that I\u0026rsquo;m not giving the full messaging implementation here. I hope you will get the main point and implement this detail in a way that fits you the best. Let me know - preferably in comments below this post - if you would want to see it on this blog. I will write a separate article.\nHappy coding!\n","date":"26 May 2018","permalink":"/blog/detecting-on-screen-keyboard-toggle/","section":"Posts","summary":"\u003cp\u003eDetecting on-screen keyboard toggles and proper handling of such changes can be quite tricky. Android tries to deal with those events on its own but its behavior is often far from perfect. iOS, on the other hand, leaves all the work to an app developer. Both approaches have its advantages and disadvantages but sooner or later each mobile app dev will have to face this problem.\u003c/p\u003e","title":"How to detect screen keyboard appearance changes"},{"content":"","date":null,"permalink":"/tags/ios/","section":"Tags","summary":"","title":"Ios"},{"content":"Today\u0026rsquo;s mobile apps are rarely created as text-only. Most of them needs at least in-app icons for toolbars. In many cases you can find graphics for mobile platforms as ready to use resources on the Internet, i.e. Material Design icons. Sometimes they are prepared by graphic designers specially for your apps.\nNo matter which case is your\u0026rsquo;s, sooner or later you will probably need to adjust the images inside your app. One of the most common customisation is setting the tint color of the image. In Xamarin.Forms you can easily do it with Effects, which I will show in this post.\nThe inspiration for code samples shown bellow was this tutorial project. The original implementation was based on Custom Renderers rather than Effects. I personally prefer the later because I can apply many Effects on a common Forms element, mixing the visual changes and added behaviours which I find cumbersome with Custom Renderers approach.\nLet\u0026rsquo;s start with the PCL part:\nusing Xamarin.Forms; namespace Core.Effects { public class TintImageEffect : RoutingEffect { public const string GroupName = \u0026#34;MyCompany\u0026#34;; public const string Name = \u0026#34;TintImageEffect\u0026#34;; public Color TintColor { get; set; } public TintImageEffect() : base($\u0026#34;{GroupName}.{Name}\u0026#34;) { } } } As you probably already guessed, the TintColor is responsible for adjusting the image tint. If you set the alpha channel to fully transparent the tint will obviously be invisible.\nThe tricky part of iOS Effect implementation is setting the image rendering mode to AlwaysTemplate:\nusing System; using System.Linq; using UIKit; using Xamarin.Forms.Platform.iOS; using FormsTintImageEffect = Core.Effects.TintImageEffect; namespace iOS.Renderers { public class TintImageEffect : PlatformEffect { protected override void OnAttached() { try { var effect = (FormsTintImageEffect) Element.Effects.FirstOrDefault(e =\u0026gt; e is FormsTintImageEffect); if (effect == null || !(Control is UIImageView image)) return; image.Image = image.Image.ImageWithRenderingMode(UIImageRenderingMode.AlwaysTemplate); image.TintColor = effect.TintColor.ToUIColor(); } catch (Exception ex) { System.Diagnostics.Debug.WriteLine($\u0026#34;An error occurred when setting the {typeof(TintImageEffect)} effect: {ex.Message}\\n{ex.StackTrace}\u0026#34;); } } protected override void OnDetached() { } } } The Android implementation is based on standard PorterDuffColorFilter:\nusing System.Linq; using Android.Graphics; using Android.Widget; using Java.Lang; using Xamarin.Forms.Platform.Android; using FormsTintImageEffect = Core.Effects.TintImageEffect; namespace Droid.Renderers { public class TintImageEffect : PlatformEffect { protected override void OnAttached() { try { var effect = (FormsTintImageEffect) Element.Effects.FirstOrDefault(e =\u0026gt; e is FormsTintImageEffect); if (effect == null || !(Control is ImageView image)) return; var filter = new PorterDuffColorFilter(effect.TintColor.ToAndroid(), PorterDuff.Mode.SrcIn); image.SetColorFilter(filter); } catch (Exception ex) { System.Diagnostics.Debug.WriteLine( $\u0026#34;An error occurred when setting the {typeof(TintImageEffect)} effect: {ex.Message}\\n{ex.StackTrace}\u0026#34;); } } protected override void OnDetached() { } } } In order to enable platform Effects you must register them on both Android:\nusing Droid.Renderers; using Xamarin.Forms; // Effects: [assembly: ResolutionGroupName(Core.Effects.LinkButtonEffect.GroupName)] [assembly: ExportEffect(typeof(TintImageEffect), Core.Effects.TintImageEffect.Name)] and iOS:\nusing iOS.Renderers; using Xamarin.Forms; [assembly:ResolutionGroupName (Core.Effects.BorderEditorEffect.GroupName)] [assembly:ExportEffect (typeof(TintImageEffect), Core.Effects.TintImageEffect.Name)] If you already have some Effects registered with assembly:ResolutionGroupName attribute remember that this needs to be done only once per specific group name. Otherwise you will get an runtime error.\nFinally you can consume the new effect in your XAML:\n\u0026lt;!-- xmlns:effects=\u0026#34;clr-namespace:Core.Effects;assembly=Core\u0026#34; --\u0026gt; \u0026lt;Image ...\u0026gt; \u0026lt;Image.Effects\u0026gt; \u0026lt;effects:TintImageEffect TintColor=\u0026#34;Red\u0026#34; /\u0026gt; \u0026lt;/Image.Effects\u0026gt; \u0026lt;/Image\u0026gt; Happy coding!\n","date":"7 February 2018","permalink":"/blog/setting-tint-color-in-xamarin-form-image/","section":"Posts","summary":"\u003cp\u003eToday\u0026rsquo;s mobile apps are rarely created as text-only. Most of them needs at least in-app icons for toolbars. In many cases you can find graphics for mobile platforms as ready to use resources on the Internet, i.e. \u003ca href=\"https://material.io/icons/\" target=\"_blank\" rel=\"noreferrer\"\u003eMaterial Design icons\u003c/a\u003e. Sometimes they are prepared by graphic designers specially for your apps.\u003c/p\u003e","title":"Setting tint color in Xamarin.Form image"},{"content":"","date":null,"permalink":"/tags/design/","section":"Tags","summary":"","title":"Design"},{"content":"It\u0026rsquo;s very common to have multiple versions of the app during development - i.e. stable beta and store/production or alpha that contains the latest changes. Managing application configuration for multiple versions might be confusing when it\u0026rsquo;s not carefully designed and setup with the build process.\nPublishing app to App Store or Google Play developers often forget that the mobile application is running de facto in hostile environment. Advanced user can easily reverse engineer the installed package on rooted/jailbroken device or even an emulator and see the data that wasn\u0026rsquo;t supposed to be released on production. As you will see this is just a different aspect of the multi-version app config.\nThis article is a continuation of previous posts. If you haven\u0026rsquo;t read those I strongly recommend doing it now since I\u0026rsquo;ll refer to them here.\nEnvironments, versions, configurations #Most of the time I\u0026rsquo;m developing Xamarin apps (and other types as well) in IDE I do it in Debug config. This build is usually setup to produce full PDBs with no linking. Like I wrote in the first part of this series I usually add a compilation symbols that indicates i.e. offline app behaviour.\nNevertheless, Debug configuration is obviously not the best option for distribution and manual QA tests. I won\u0026rsquo;t go into much detail about versioning strategy in mobile app development. This topic will be covered in the next series about continuous integration and delivery.\nTypically, there is an alpha version that includes the latest changes, beta that acts as a release candidate and the store/production version. From the app configuration perspective those versions may vary in many different aspects, such as:\nbackend endpoint URLs (to different environments, like test, stage or production) timeout values number of retries for the backend calls and other, not only related to backend integration.\nConfigurations in practice #To differentiate between alpha/beta/store version in build we can use the same technique which is used to distinguish code that is to be built for different platforms in shared projects - compilation symbols. In order for this to work, symbols need to be defined in DefineConstants element of PropertyGroup for each relevant configuration.\nHere is an example for Alpha version in PCL project:\n\u0026lt;PropertyGroup Condition=\u0026#34; \u0026#39;$(Configuration)|$(Platform)\u0026#39; == \u0026#39;Alpha|AnyCPU\u0026#39; \u0026#34;\u0026gt; ... \u0026lt;DefineConstants\u0026gt;TRACE;DEBUG;ALPHA_BUILD\u0026lt;/DefineConstants\u0026gt; ... \u0026lt;/PropertyGroup\u0026gt; Now when we have ALPHA_BUILD, BETA_BUILD, STORE_BUILD etc. symbols defined we can use them in code. The most straightforward way to vary the app configuration is to use conditional compilation per-setting:\npublic static class ApplicationConfiguration { // test backend: #if ALPHA_BUILD public static readonly Uri MyServiceUrl = new Uri(\u0026#34;https://test.example.com/api\u0026#34;); // production backend: #elif BETA_BUILD || STORE_BUILD public static readonly Uri MyServiceUrl = new Uri(\u0026#34;https://example.com/api\u0026#34;); #endif } This might the right approach for one to few such settings. But soon, when the number of settings increases you will end up with the same issues like I described in my previous post when discussing the DI modules registration. Furthermore, using static configuration creates a strong coupling between your services and configuration itself.\nIn the same way as for service configuration pictured in the previous article inversion of control principle comes to rescue. You can design your app configuration in the same manner as the services:\npublic interface IApplicationConfiguration { Uri MyServiceUrl { get; } TimeSpan ServiceTimeout { get; } //... } public class TestApplicationConfiguration : IApplicationConfiguration { public Uri MyServiceUrl =\u0026gt; new Uri(\u0026#34;https://test.example.com/api\u0026#34;); public TimeSpan ServiceTimeout =\u0026gt; TimeSpan.FromSeconds(30); //... } public class ProductionApplicationConfiguration : IApplicationConfiguration { public Uri MyServiceUrl =\u0026gt; new Uri(\u0026#34;https://example.com/api\u0026#34;); public TimeSpan ServiceTimeout =\u0026gt; TimeSpan.FromSeconds(10); //... } Then the only place where you need to use conditional compilation is the dependency registration:\n#if ALPHA_BUILD builder.RegisterType\u0026lt;TestApplicationConfiguration\u0026gt;().As\u0026lt;IApplicationConfiguration\u0026gt;().SingleInstance(); #elif BETA_BUILD || STORE_BUILD builder.RegisterType\u0026lt;ProductionApplicationConfiguration\u0026gt;().As\u0026lt;IApplicationConfiguration\u0026gt;().SingleInstance(); #endif Security matters #The last solution has one drawback. When you release your app to the App Store or Google Play it can be downloaded and decompiled by anyone with sufficient skills and knowledge. This means that some details of your development/testing environment may leak outside your organisation. From my experience such environments are rarely the subject of careful protection in the same extent as the production (which is of course a bad practice). For this reason, it\u0026rsquo;s better to prone the store version from all such unnecessary information.\nThis can be done in few ways. The simplest is to add the conditional compilation to configuration classes:\n#if ALPHA_BUILD public class TestApplicationConfiguration : IApplicationConfiguration { public Uri MyServiceUrl =\u0026gt; new Uri(\u0026#34;https://test.example.com/api\u0026#34;); public TimeSpan ServiceTimeout =\u0026gt; TimeSpan.FromSeconds(30); //... } #endif #if BETA_BUILD || STORE_BUILD public class ProductionApplicationConfiguration : IApplicationConfiguration { public Uri MyServiceUrl =\u0026gt; new Uri(\u0026#34;https://example.com/api\u0026#34;); public TimeSpan ServiceTimeout =\u0026gt; TimeSpan.FromSeconds(10); //... } #endif It\u0026rsquo;s not very pretty but it does its job. We can improve it further.\nBut what about data? #Before I show you the second approach let\u0026rsquo;s think about the different problem for a while. Remember when I showed the strategies of mocking the data for development? My preferred way for doing it is adding embedded resource files into the common PCL/netstandard project and reading them on demand. Such files might potentially contain much more sensitive data than your test endpoint URLs. But JSON files cannot have conditional compilation included inside the file.\nThe solution for this problem, which can be also applied for regular C# code files, is to use the conditional inclusion into the project file:\n\u0026lt;ItemGroup Condition=\u0026#34; \u0026#39;$(Configuration)\u0026#39; == \u0026#39;Debug\u0026#39; \u0026#34;\u0026gt; \u0026lt;EmbeddedResource Include=\u0026#34;Data\\MockData.json\u0026#34; /\u0026gt; \u0026lt;/ItemGroup\u0026gt; In this example MockData.json file will be included in build only for the Debug configuration, where it belongs.\nWrapping up #As you can see creating the maintainable application configuration is a matter of using best OOP practices and basic common sense. None of the approaches I showed in this or previous articles is unfamiliar for the regular programmer. Still putting this all together requires some practice. I strongly encourage you practice using SOLID principles in your daily development. They will make your life much easier.\nHappy coding!\n","date":"21 January 2018","permalink":"/blog/enterprise-patterns-for-configuration-in-xamarin-app-part-3/","section":"Posts","summary":"\u003cp\u003eIt\u0026rsquo;s very common to have multiple versions of the app during development - i.e. stable beta and store/production or alpha that contains the latest changes. Managing application configuration for multiple versions might be confusing when it\u0026rsquo;s not carefully designed and setup with the build process.\u003c/p\u003e","title":"Enterprise patterns for configuration in Xamarin app. Part 3: Versioning and keeping secrets secret"},{"content":"","date":null,"permalink":"/tags/ioc/","section":"Tags","summary":"","title":"IoC"},{"content":"","date":null,"permalink":"/tags/solid/","section":"Tags","summary":"","title":"SOLID"},{"content":"In last post I described how to cope, in some extent, with different pace of delivering mobile app versus it\u0026rsquo;s supporting backend. The article also provided a simple hint for speeding up the mobile app development by introducing mocks instead of the external network services.\nIn this one I\u0026rsquo;m gonna give you some advice on managing the dependencies as your app gets more of them in time.\nManaging mocks #Let\u0026rsquo;s take a look at the example of registering dependencies from the previous post:\npublic void RegisterDependencies(Autofac.ContainerBuilder builder) { #if OFFLINE_MODE builder.RegisterType\u0026lt;MockBackendService\u0026gt;().As\u0026lt;IMyBackendService\u0026gt;().SingleInstance(); #else builder.RegisterType\u0026lt;HttpBackendService\u0026gt;().As\u0026lt;IMyBackendService\u0026gt;().SingleInstance(); #endif } So far so good. Initially the IMyBackendService could have only few methods. However it will start to grow as the functionality offered by the app and it\u0026rsquo;s supporting backend extends. Even if the app communicates with an single endpoint the new methods (in RPC style) or entities (in RESTful style) will be introduced. Eventually at some point the service will have to be broken down into smaller services. This is the direct consequence of Single Responsibility and Interface Segregation principles. It\u0026rsquo;s probably easier to spot when using the RESTful API.\nTo picture how this can look like see the interface that seems to represent to many responsibilities:\npublic interface IMyBackendService { Task\u0026lt;IEnumerable\u0026lt;Employee\u0026gt;\u0026gt; GetEmployees(); Task Update(Employee employee); Task\u0026lt;IEnumerable\u0026lt;Department\u0026gt;\u0026gt; GetDepartments(); Task Update(Department department); Task\u0026lt;IEnumerable\u0026lt;Product\u0026gt;\u0026gt; GetProducts(); Task Update(Product department); } The implementing class will have to deal with different kinds of entities: Employee, Department and Product (so as the mock). All IMyBackendService implementations will probably grow in time to hundreds lines or more depending of the corresponding additional logic.\nOn the consumer side the view models (or other services) will have access to all those kinds of data no matter if they need it or not. This greatly reduces traceability of the real logical connections inside the application. It also often introduces coupling that is hard maintain (even in short term). Of course the example above is still quite simple but you get the idea.\nThe remedy is to split the interface (and corresponding implementations):\npublic interface IEmployeesService { Task\u0026lt;IEnumerable\u0026lt;Employee\u0026gt;\u0026gt; GetEmployees(); Task Update(Employee employee); } public interface IDepartmentsService { Task\u0026lt;IEnumerable\u0026lt;Department\u0026gt;\u0026gt; GetDepartments(); Task Update(Department department); } public interface IProductsService { Task\u0026lt;IEnumerable\u0026lt;Product\u0026gt;\u0026gt; GetProducts(); Task Update(Product department); } All relevant view models and other consumers would have to be updated as well.\nNow let\u0026rsquo;s get back to the dependency registration from the beginning:\npublic void RegisterDependencies(Autofac.ContainerBuilder builder) { #if OFFLINE_MODE builder.RegisterType\u0026lt;MockEmployeesService\u0026gt;().As\u0026lt;IEmployeesService\u0026gt;().SingleInstance(); builder.RegisterType\u0026lt;MockDepartmentsService\u0026gt;().As\u0026lt;IDepartmentsService\u0026gt;().SingleInstance(); builder.RegisterType\u0026lt;MockProductsService\u0026gt;().As\u0026lt;IProductsService\u0026gt;().SingleInstance(); #else builder.RegisterType\u0026lt;HttpEmployeesService\u0026gt;().As\u0026lt;IEmployeesService\u0026gt;().SingleInstance(); builder.RegisterType\u0026lt;HttpDepartmentsService\u0026gt;().As\u0026lt;IDepartmentsService\u0026gt;().SingleInstance(); builder.RegisterType\u0026lt;HttpProductsService\u0026gt;().As\u0026lt;IProductsService\u0026gt;().SingleInstance(); #endif } This begins to look pretty awkward. It\u0026rsquo;s not hard to imagine how this will look like after adding few more services. How can we improve the readability of this method? Maybe method-level decomposition will do the trick:\npublic void RegisterDependencies(Autofac.ContainerBuilder builder) { #if OFFLINE_MODE RegisterMockServices(builder); #else RegisterHttpServices(builder); #endif } private void RegisterMockServices(Autofac.ContainerBuilder builder) { builder.RegisterType\u0026lt;MockEmployeesService\u0026gt;().As\u0026lt;IEmployeesService\u0026gt;().SingleInstance(); builder.RegisterType\u0026lt;MockDepartmentsService\u0026gt;().As\u0026lt;IDepartmentsService\u0026gt;().SingleInstance(); builder.RegisterType\u0026lt;MockProductsService\u0026gt;().As\u0026lt;IProductsService\u0026gt;().SingleInstance(); } private void RegisterHttpServices(Autofac.ContainerBuilder builder) { builder.RegisterType\u0026lt;HttpEmployeesService\u0026gt;().As\u0026lt;IEmployeesService\u0026gt;().SingleInstance(); builder.RegisterType\u0026lt;HttpDepartmentsService\u0026gt;().As\u0026lt;IDepartmentsService\u0026gt;().SingleInstance(); builder.RegisterType\u0026lt;HttpProductsService\u0026gt;().As\u0026lt;IProductsService\u0026gt;().SingleInstance(); } This solution has still some drawbacks. First - both cases are still handled within the same class which smells like breaking SRP rule again. Each new dependency will increase the number of lines in this class +2 (or +N if you have more such cases).\nFrom the less puristic perspective - if you\u0026rsquo;re developing with any modern IDE it will probably suggest that one of the two private methods should be removed since only one is being called in the current build configuration. I cannot count how many times I had accidentally removed the living code because of automatic clean-ups\u0026hellip;\nModules to the rescue #An alternative for registering everything in one place are modules. This concept is probably well known for many backend developers since the number of dependencies tends to grow much faster in those applications. Unfortunately, as far as I know, it have not been implemented in Xamarin.Forms DependencyService. From the DI containers I have worked with it\u0026rsquo;s available in Ninject and Autofac.\nThere are many interesting things you can do with modules but for now let\u0026rsquo;s see how they can help to overcome the issues mentioned above:\npublic class MockServicesModule : Autofac.Module { protected override void Load(Autofac.ContainerBuilder builder) { builder.RegisterType\u0026lt;MockEmployeesService\u0026gt;().As\u0026lt;IEmployeesService\u0026gt;().SingleInstance(); builder.RegisterType\u0026lt;MockDepartmentsService\u0026gt;().As\u0026lt;IDepartmentsService\u0026gt;().SingleInstance(); builder.RegisterType\u0026lt;MockProductsService\u0026gt;().As\u0026lt;IProductsService\u0026gt;().SingleInstance(); } } public class HttpServicesModule : Autofac.Module { protected override void Load(Autofac.ContainerBuilder builder) { builder.RegisterType\u0026lt;HttpEmployeesService\u0026gt;().As\u0026lt;IEmployeesService\u0026gt;().SingleInstance(); builder.RegisterType\u0026lt;HttpDepartmentsService\u0026gt;().As\u0026lt;IDepartmentsService\u0026gt;().SingleInstance(); builder.RegisterType\u0026lt;HttpProductsService\u0026gt;().As\u0026lt;IProductsService\u0026gt;().SingleInstance(); } } The registration entry point doesn\u0026rsquo;t change much comparing to the previous one:\npublic void RegisterDependencies(Autofac.ContainerBuilder builder) { #if OFFLINE_MODE builder.RegisterModule\u0026lt;MockServicesModule\u0026gt;(); #else builder.RegisterModule\u0026lt;HttpServicesModule\u0026gt;(); #endif } As you can see this eliminates the issues of the previous approach. All the registration cases have their own type (SRP). IDE or ReSharper will not remove your registrations or their usings by accident. I like this approach also because it makes the main registration point short and clean.\nWrapping up #The benefits of decomposing app IoC configuration with modules are not limited to the mocking use-case. You can easily create other environment-dependent configurations like UiTestsModule, LocalDevModule etc. Instead of using static compilation directives you can create dynamic runtime configuration (with couple more details like dependencies unregistration). I encourage you to experiment. IoC configuration is often much easier to maintain than tens of flags of switch-case logic in your services or view models.\nIn next articles I\u0026rsquo;ll continue with the configuration topics since there are quite a few more to cover. Dealing with sensitive mocked data or managing version-dependent configurations are just the examples.\nHappy coding!\n","date":"6 December 2017","permalink":"/blog/enterprise-patterns-for-configuration-in-xamarin-app-part-2/","section":"Posts","summary":"\u003cp\u003eIn \u003ca href=\"/blog/enterprise-patterns-for-configuration-in-xamarin-app-part-1\"\u003elast post\u003c/a\u003e I described how to cope, in some extent, with different pace of delivering mobile app versus it\u0026rsquo;s supporting backend. The article also provided a simple hint for speeding up the mobile app development by introducing mocks instead of the external network services.\u003c/p\u003e","title":"Enterprise patterns for configuration in Xamarin app. Part 2: Managing dependencies"},{"content":"Modern mobile apps are rarely developed as offline-only. They typically communicate with backend services that feed them with data, keeps in sync with their web equivalents or allows for various external integrations. The backend part is most often developed by different teams in their own pace. The mobile part can be often developed faster thus it waits for the full integration.\nEven if the backend service is ready to consume it\u0026rsquo;s not always most convenient to develop being connected to it. Sometimes you just make a small UI change and want to see the result immediately. Even the fastest Internet connection would not give you the speed of loading the mocked data from memory. It\u0026rsquo;s also easier to use the same approach in automated UI tests instead of depending on data changing in backend (if you don\u0026rsquo;t test the integration itself).\nThose are only two of many various scenarios where the proper design of application-level configuration can help in the development and testing process. Proper use of dependency injection, conditional compilation and few other techniques can make this design clean and maintainable. This spans from the local development to Continuous Integration and Deployment process. In this and next articles I will show you how to do this in Xamarin app.\nBasic mocking #Let\u0026rsquo;s start with the first scenario that I described at the beginning. Assuming that both teams agree on the API, the service can be mocked in the app allowing to develop it independently of backend.\nDependency inversion, one of the SOLID principles, suggests that all the functionality should be coded around abstractions, not the real implementations. In practice this means that our view model (in MVVM approach) or other consumers will operate on the service interface. The interface implementation will be typically instantiated by DI container. At this point the concept is most probably familiar to your development experience.\nOne of the main benefits of applying DI principle is that we can have more than one implementation of an interface we can switch. Consider this example:\npublic interface IMyBackendService { Task\u0026lt;IEnumerable\u0026lt;string\u0026gt;\u0026gt; GetValues(); } public class HttpBackendService { public async Task\u0026lt;IEnumerable\u0026lt;string\u0026gt;\u0026gt; GetValues() { // retrieve the values from backend using HttpClient, Refit etc. } } public class MockBackendService { private static readonly IEnumerable\u0026lt;string\u0026gt; _mockedData = new[] { \u0026#34;value1\u0026#34;, \u0026#34;value2\u0026#34;, \u0026#34;value3\u0026#34; }; public async Task\u0026lt;IEnumerable\u0026lt;string\u0026gt;\u0026gt; GetValues() { return await Task.FromResult(_mockedData); } } Now we have two IMyBackendService service implementations. First one provides the real data from the backend, using HTTP transport under the hood. Second returns the mocked data. MockBackendService could also retrieve the mocked values from the resource file (i.e. as JSON embedded assembly resource) making it easier to store and update.\nSometimes it\u0026rsquo;s worth to consider providing the mocked data on the HttpClientHandler/HttpMessageHandler level. If you are interested how to do this for Simple.OData client I encourage you to read my previous post on this topic - \u0026ldquo;Mocking HTTP response in Simple.OData client\u0026rdquo;.\nDependency registration #The crucial part here is to provide proper service implementation to the consumer (typically the view model). Obviously it should not be made on the consumer side since it should not be aware of the specific implementation. Good place for this is the service dependency registration. In Xamarin apps you can use one of many DI containers available for PCL/.Net Standard and your target platforms: SimpleInjector, TinyIoC, Unity, Autofac, Ninject just to name the few most popular. If you are creating Xamarin.Forms app you\u0026rsquo;re probably more familiar with built-in DependencyService.\nThe basic registering is quite similar in most of DI containers. Take a look on the following example registration code (Autofac):\npublic void RegisterDependencies(Autofac.ContainerBuilder builder) { #if OFFLINE_MODE builder.RegisterType\u0026lt;MockBackendService\u0026gt;().As\u0026lt;IMyBackendService\u0026gt;().SingleInstance(); #else builder.RegisterType\u0026lt;HttpBackendService\u0026gt;().As\u0026lt;IMyBackendService\u0026gt;().SingleInstance(); #endif } You will call something similar typically in your app\u0026rsquo;s entry point: Forms.Application, iOS AppDelegate or Android MainActivity depending on the platform. The method contains a conditional compilation switch. If the actual build configuration contains the OFFLINE_MODE compilation symbol MockBackendService service implementation will be registered and used consequently when dependency is resolved. Otherwise the HttpBackendService will be chosen.\nReal life development #When I start developing a new feature that depends on the backend service I briefly discuss with backend team the contracts of the API and what kind of data the app should expect in real. Then I start from the mocked service and switch to the real one as soon as it\u0026rsquo;s ready. This part can be done without the conditional compilation shown above since all the build configurations must work without the unfinished backend. However having the mocked service separated from the real one makes the switch quick and pretty easy.\nAfter I make sure that the contract hasn\u0026rsquo;t changed and/or making the necessary corrections I code the real implementation and add the conditional compilation logic (in slightly different form that I will show in the next post). I usually add the OFFLINE_MODE symbol to the Debug build configuration that I use only during the development and local testing. Other configurations, like ad-hoc and release builds doesn\u0026rsquo;t have it so the app built on them connect to the real backend service.\nIn the next article I will show you how to deal with more such dependencies in a nice way. I will also show how to protect your sensitive mocked data from leaking outside of your development environment.\nHappy coding!\n","date":"2 December 2017","permalink":"/blog/enterprise-patterns-for-configuration-in-xamarin-app-part-1/","section":"Posts","summary":"\u003cp\u003eModern mobile apps are rarely developed as offline-only. They typically communicate with backend services that feed them with data, keeps in sync with their web equivalents or allows for various external integrations. The backend part is most often developed by different teams in their own pace. The mobile part can be often developed faster thus it waits for the full integration.\u003c/p\u003e","title":"Enterprise patterns for configuration in Xamarin app. Part 1: Mocking external dependencies"},{"content":"XAML compilation is (or at least should be) one of the default optimisation steps in Xamarin.Forms app development. It really speeds up the app especially on Android. Sometimes however it can cause some nasty errors like this one:\n/path/to/project/Views/MyContentPage.xaml : error : Object reference not set to an instance of an object First time I saw it I had absolutely no clue what was the reason. The common solutions for similar issues with VS for Mac designer did not help. I started with trying to narrow down the problematic code using the standard bisection method. After few cut-compile-paste cycles I knew that the root cause was this line in my XAML file:\n\u0026lt;ContentPage.Resources\u0026gt; \u0026lt;ResourceDictionary\u0026gt; \u0026lt;!-- ... --\u0026gt; \u0026lt;converters:SurveyAnswerStringConverter x:Key=\u0026#34;SurveyAnswerStringConverter\u0026#34; /\u0026gt; \u0026lt;/ResourceDictionary\u0026gt; \u0026lt;/ContentPage.Resources\u0026gt; For some reason the XAML compiler could not comprehend creation of the converter. I had quite a lot of converters in my app but this one was specific in terms of inheritance hierarchy - it\u0026rsquo;s base class was abstract:\npublic class SurveyAnswerStringConverter : MissingDataConverterBase\u0026lt;string\u0026gt; { public override string Convert(string value, Type targetType, object parameter, CultureInfo culture) { return Convert(value, false, value, Resources.Survey_NotAnswered); } } public abstract class MissingDataConverterBase\u0026lt;TTarget\u0026gt; : ValueConverterBase\u0026lt;string, TTarget\u0026gt; { protected virtual TTarget Convert(string value, bool allowEmptyString, TTarget presentValue, TTarget missingValue) { if (allowEmptyString) return value == null ? missingValue : presentValue; return string.IsNullOrEmpty(value) ? missingValue : presentValue; } } Obviously this should not be a problem. It works pretty well without XAML compilation and does not crash in runtime. Nevertheless XAML compiler has some problem with it.\nFortunately the workaround is quite simple: instead of declaring the resource object in XAML I moved it to runtime, right after the XAML initialisation:\npublic MyContentPage() { InitializeComponent(); Resources.Add(\u0026#34;MissingDataStringConverter\u0026#34;, new MissingDataStringConverter()); } Divide and conquer #Just a one final thought. Even if your issue is different than this one I still strongly recommend the bisection method mentioned above.\nIt\u0026rsquo;s pretty straightforward. Cut the first half (more or less) of your XAML and compile. If it compiles - you know that the problematic code is here. Otherwise do the same with the second part. Divide this part in half and continue recursively until you find the buggy line(s).\nThis is exactly the same procedure as introduced in git bisect tool for finding a commit that introduced a particular bug and it\u0026rsquo;s really effective (comparing to random guesses).\nHappy coding!\n","date":"6 November 2017","permalink":"/blog/object-reference-not-set-in-xamarin-forms-xaml-compilation/","section":"Posts","summary":"\u003cp\u003eXAML compilation is (or at least should be) one of the default optimisation steps in Xamarin.Forms app development. It really speeds up the app especially on Android. Sometimes however it can cause some nasty errors like this one:\u003c/p\u003e","title":"Object reference not set in Xamarin.Forms XAML compilation"},{"content":"","date":null,"permalink":"/tags/xaml/","section":"Tags","summary":"","title":"Xaml"},{"content":"","date":null,"permalink":"/categories/.net/","section":"Categories","summary":"","title":".Net"},{"content":"3rd party libraries never seems to be documented enough. It\u0026rsquo;s the old truth that every software developer learns sooner or later. In most cases after dozens of hours spent on trying to figure out \u0026ldquo;what the hell is wrong with my/that code!?\u0026rdquo;. This post is about one of such \u0026ldquo;hidden features\u0026rdquo; in Microsoft oData client - Simple.OData.\nWell written communication libraries in .NET allow for replacing their HttpMessageHandler. Some might want to add diagnostic logging or low-level error handling to the HTTP processing pipeline. Another reason is writing unit tests. This was my case.\nI was writing unit tests for the OData proxy setup component. My first attempt was simple - derive from the HttpMessageHandler, allow for mocking the response content and headers and finally implement SendAsync:\nprivate class MockHttpMessageHandler : HttpMessageHandler { private const string DefaultResponseContent = \u0026#34;...\u0026#34;; private const string DefaultResponseMimeType = \u0026#34;application/json\u0026#34;; public HttpRequestMessage Request { get; private set; } public HttpResponseMessage Response { get; private set; } public void SetupResponse(HttpStatusCode statusCode, string mimeType = DefaultResponseMimeType, string content = DefaultResponseContent) { Response = new HttpResponseMessage(statusCode) { Content = new StringContent(content) { Headers = { ContentType = MediaTypeHeaderValue.Parse(mimeType) } } }; } protected override async Task\u0026lt;HttpResponseMessage\u0026gt; SendAsync(HttpRequestMessage request, CancellationToken cancellationToken) { Request = request; Response.RequestMessage = request; return await Task.FromResult(Response); } } Noting surprising here. The test code was also quite simple:\n[Test] public async Task ShouldCreateClientAddingAuthorizationCookie() { _messageHandler.SetupResponse(HttpStatusCode.OK); var client = _target.Create(); await Query(client); var requestHeaders = _messageHandler.Request.Headers; requestHeaders.Should().Contain(header =\u0026gt; IsCookieHeader(header)); var cookieHeader = requestHeaders.First(IsCookieHeader); cookieHeader.Value.Should().Contain(value =\u0026gt; IsAuthorizationCookie(value)); } The surprise came right after running it:\nMicrosoft.OData.Core.ODataException : An unexpected \u0026#39;EndOfInput\u0026#39; node was found when reading from the JSON reader. A \u0026#39;StartObject\u0026#39; node was expected. Error found near: \u0026lt;--- at Microsoft.OData.Core.Json.JsonReaderExtensions.ValidateNodeType (Microsoft.OData.Core.Json.JsonReader jsonReader, Microsoft.OData.Core.Json.JsonNodeType expectedNodeType) at Microsoft.OData.Core.Json.JsonReaderExtensions.ReadNext (Microsoft.OData.Core.Json.JsonReader jsonReader, Microsoft.OData.Core.Json.JsonNodeType expectedNodeType) ... at Microsoft.OData.Core.ODataReaderCore.ReadImplementation () at Microsoft.OData.Core.ODataReaderCore.ReadSynchronously () at Microsoft.OData.Core.ODataReaderCore.InterceptException[T] (System.Func`1[TResult] action) at Microsoft.OData.Core.ODataReaderCore.Read () at Simple.OData.Client.V4.Adapter.ResponseReader.ReadResponse (Microsoft.OData.Core.ODataReader odataReader, System.Boolean includeAnnotationsInResults) at Simple.OData.Client.V4.Adapter.ResponseReader+\u0026lt;GetResponseAsync\u0026gt;d__11.MoveNext () Reading the code of lower-level parsing API did not help much. After few hours of digging in the Simple.OData codebase I found the first clue - ResponseReader GetResponseAsync() method:\npublic override Task\u0026lt;ODataResponse\u0026gt; GetResponseAsync(HttpResponseMessage responseMessage, bool includeAnnotationsInResults = false) { return this.GetResponseAsync((IODataResponseMessageAsync) new ODataResponseMessage(responseMessage), includeAnnotationsInResults); } It turned out that the original HttpResponseMessage is wrapped with the internal implementation. One look on the ODataResponseMessage class helped me to understand what\u0026rsquo;s happening:\npublic Task\u0026lt;Stream\u0026gt; GetStreamAsync() { StreamContent content = this._response.Content as StreamContent; if (content != null) return content.ReadAsStreamAsync(); TaskCompletionSource\u0026lt;Stream\u0026gt; completionSource = new TaskCompletionSource\u0026lt;Stream\u0026gt;(); completionSource.SetResult(Stream.Null); return completionSource.Task; } Obviously the StringContent type in HttpResponseMessage was not expected by the author.\nHere is the final version of SetupResponse() method in the mock class:\npublic void SetupResponse(HttpStatusCode statusCode, string mimeType = DefaultResponseMimeType, string content = DefaultResponseContent) { var stream = new MemoryStream(); using (var writer = new StreamWriter(stream, Encoding.UTF8, 1024, true)) { writer.Write(content); writer.Flush(); } stream.Position = 0; Response = new HttpResponseMessage(statusCode) { Content = new StreamContent(stream) { Headers = { ContentType = MediaTypeHeaderValue.Parse(mimeType) } } }; } Remember to set the MemoryStream.Position property to the start of the buffer. Otherwise you will get exactly the same error as before but for a different reason.\nHappy coding!\n","date":"22 August 2017","permalink":"/blog/mocking-http-response-in-simple-odata-client/","section":"Posts","summary":"\u003cp\u003e3rd party libraries never seems to be documented enough. It\u0026rsquo;s the old truth that every software developer learns sooner or later. In most cases after dozens of hours spent on trying to figure out \u0026ldquo;what the hell is wrong with my/that code!?\u0026rdquo;. This post is about one of such \u0026ldquo;hidden features\u0026rdquo; in Microsoft oData client - \u003ca href=\"https://github.com/object/Simple.OData.Client\" target=\"_blank\" rel=\"noreferrer\"\u003eSimple.OData\u003c/a\u003e.\u003c/p\u003e","title":"Mocking HTTP response in Simple.OData client"},{"content":"","date":null,"permalink":"/tags/odata/","section":"Tags","summary":"","title":"Odata"},{"content":"Standard Xamarin.Forms Xamarin.Forms.Editor control offers edit capabilities similar to Entry but for multiline text. Unfortunately unlike Entry it doesn\u0026rsquo;t support displaying placeholder text out of the box. Implementing this functionality with custom renderers can be tricky. Let\u0026rsquo;s see how to do this on Android and iOS.\nFirst step, as for every custom renderer, is to create a custom Forms control:\nInternally Android Editor renderer uses EditText control. It natively supports placeholder text so the implementation is pretty simple:\niOS implementation is more tricky. The internal Editor control is UITextView which doesn\u0026rsquo;t provide such functionality. This behavior can be imitated in at least two different ways.\nThe first is to display the placeholder simply as the same text as the standard control content. This can be hard to style differently than standard text. Also changes detection can lead to bugs (i.e. if user enters the same text as the placeholder).\nThe second one is to introduce additional label which will act as the placeholder. Here is the sample code:\nThe placeholder appearance logic is handled by UITextView Changed and Ended events. Unsubscribing from those events is important at control disposal to prevent memory leaks.\nI decided to use Cirrious.FluentLayouts library to simplify the controls positioning. Feel free to choose whatever approach works for you in this matter.\nHappy coding!\n","date":"28 July 2017","permalink":"/blog/placeholder-text-in-xamarin-forms-editor/","section":"Posts","summary":"\u003cp\u003eStandard Xamarin.Forms \u003ca href=\"https://developer.xamarin.com/api/type/Xamarin.Forms.Editor\" target=\"_blank\" rel=\"noreferrer\"\u003eXamarin.Forms.Editor control\u003c/a\u003e offers edit capabilities similar to \u003ccode\u003eEntry\u003c/code\u003e but for multiline text. Unfortunately unlike \u003ccode\u003eEntry\u003c/code\u003e it doesn\u0026rsquo;t support displaying placeholder text out of the box. Implementing this functionality with custom renderers can be tricky. Let\u0026rsquo;s see how to do this on Android and iOS.\u003c/p\u003e","title":"Placeholder text in Xamarin.Forms Editor"},{"content":"","date":null,"permalink":"/tags/renderers/","section":"Tags","summary":"","title":"Renderers"},{"content":"Most of iPhone and iPad users can easily recognize icon badges - the pattern for application notifications typically presented in app icon or navigation bar. People that got used to this pattern might want to have the same user experience in their Xamarin Forms application. This post describes how to customize the navigation toolbar in iOS to dynamically display such elements.\nWhen I was asked to prepare the static (but still clickable and interactive) mockup views that include badge icon in nav bar I didn\u0026rsquo;t realize that the task is really non-trivial. It\u0026rsquo;s the one from category of tasks that are much easier to implement in native Xamarin. In this case the Xamarin Forms add few layers of indirection that makes the process painful.\nFirst thing I learned is that I cannot simply implement the custom renderer for ToolbarItem element because there is no publicly available implementation of such. The one on the PCL Forms side is only a view holder or representation of navigation item. On the native side the whole process of building the navigation toolbar is controlled by A PageRenderer. If any customization is required the developer needs to implement the custom page renderer (and probably the custom base ContentPage as well). Fortunately I found this Xamarin Forum thread and a blog post of Jason Farrel that helped me to understand the design of this part of Xamarin Forms. They were my starting point.\nSecond surprise was that I was not able to find any built-in support for in-app badge icons in iOS UIKit. I found few custom 3rd party components written in Objective C or Swift. The perspective of writing the bindings convinced me to implement the badge in Xamarin. I was intensively using the Gist sample I found during my research.\nI think that\u0026rsquo;s enough for the introduction. Let\u0026rsquo;s see some code!\nFirst thing is the new base ContentPage. To be strict this is not absolutely required. In next step you can implement the page renderer that will be applied to all your content pages. Nevertheless I still recommend this. When your app continues to grow you will probably need it anyway to share some properties between your views.\nusing Xamarin.Forms; namespace Core.Pages.Base { public class ApplicationContentPageBase : ContentPage { } } As said the second step is to build the custom platform-specific page renderer. Here you must decide how your app will notify the badge rendering logic about changes in badge values. There are several ways to achieve this. One option is to use the bindings and callbacks (events). In this case remember to unsubscribe from all the events at proper time to avoid memory leaks. Keep in mind that the notification mechanism will probably strongly depend on the source of badge values.\nI decided to use the Xamarin Forms MessagingCenter relay for this communication (together with bindings described later). Renderer is responsible for subscribing, receiving notifications and proper laying out the indicator on navigation button:\nusing System.Diagnostics; using Cirrious.FluentLayouts.Touch; using Core.Pages.Base; using Core.Views; using iOS.Controls; using iOS.Renderers; using UIKit; using Xamarin.Forms; using Xamarin.Forms.Platform.iOS; [assembly: ExportRenderer(typeof(ApplicationContentPageBase), typeof(ApplicationContentPageBaseRenderer))] namespace iOS.Renderers { public class ApplicationContentPageBaseRenderer : PageRenderer { private bool _initialized; private ApplicationContentPageBase Page =\u0026gt; (ApplicationContentPageBase) Element; public override void ViewDidLoad() { if (_initialized) return; MessagingCenter.Subscribe\u0026lt;BadgeToolbarItem, int\u0026gt;(this, BadgeToolbarItem.BadgeValueChangedMessage, UpdateBadgeValue); _initialized = true; base.ViewDidLoad(); } public override void ViewWillDisappear(bool animated) { base.ViewWillDisappear(animated); MessagingCenter.Unsubscribe\u0026lt;BadgeToolbarItem, int\u0026gt;(this, BadgeToolbarItem.BadgeValueChangedMessage); } private void UpdateBadgeValue(BadgeToolbarItem sender, int newValue) { var navigationItem = NavigationController.TopViewController.NavigationItem; var itemIndex = Page.ToolbarItems.Count - Page.ToolbarItems.IndexOf(sender) - 1; var item = navigationItem.RightBarButtonItems[itemIndex]; if (newValue \u0026lt; 1) { item.CustomView?.Dispose(); item.CustomView = null; return; } var badge = new UIBadgeLabel(newValue); var customItemView = new UIImageView(item.Image); customItemView.AddSubview(badge); customItemView.SubviewsDoNotTranslateAutoresizingMaskIntoConstraints(); customItemView.AddConstraints( badge.AtTopOf(customItemView), badge.AtRightOf(customItemView)); customItemView.UserInteractionEnabled = true; customItemView.AddGestureRecognizer(new UITapGestureRecognizer(() =\u0026gt; { var parameter = sender.CommandParameter; sender.Command.Execute(parameter); })); item.CustomView = customItemView; } } } I added UITapGestureRecognizer because after setting CustomView property on nav bar button the tap gesture stoped working. To simplify the positioning I used Cirrious.FluentLayouts package. I describe the UIBadgeLabel later in this text.\nThe Forms part of the messaging communication is still required. I added this logic to the binding property in custom ToolbarItem. You might consider to simplify it by putting this in background refresh logic or somewhere else.\nusing Xamarin.Forms; namespace Core.Views { public class BadgeToolbarItem : ToolbarItem { public const string BadgeValueChangedMessage = \u0026#34;MenuItemBadgeValueChanged\u0026#34;; public static BindableProperty BadgeValueProperty = BindableProperty.Create(nameof(BadgeValue), typeof(int), typeof(BadgeToolbarItem), 0, propertyChanged: BadgeValuePropertyChanged); public int BadgeValue { get =\u0026gt; (int) GetValue(BadgeValueProperty); set =\u0026gt; SetValue(BadgeValueProperty, value); } private static void BadgeValuePropertyChanged(BindableObject bindable, object oldValue, object newValue) { var item = (BadgeToolbarItem) bindable; if (oldValue != newValue) MessagingCenter.Send(item, BadgeValueChangedMessage, (int) newValue); } } } In order to use the bindable property the view model needs to have a broadcasting property. Here is an example:\nnamespace Core.PageModels.Customers { public class MyCustomersPageModel : INotifyPropertyChanged { private int _numberOfWarnings; public int NumberOfWarnings { get =\u0026gt; _numberOfWarnings; set { _numberOfWarnings = value; RaisePropertyChanged(); } } // update the number in your VM initialization/proper lifecycle moment/other asynchronous events } } In XAML all those elements can be used like below:\n\u0026lt;?xml version=\u0026#34;1.0\u0026#34; encoding=\u0026#34;UTF-8\u0026#34;?\u0026gt; \u0026lt;pages:ApplicationContentPageBase xmlns=\u0026#34;http://xamarin.com/schemas/2014/forms\u0026#34; xmlns:x=\u0026#34;http://schemas.microsoft.com/winfx/2009/xaml\u0026#34; xmlns:pages=\u0026#34;clr-namespace:Core.Pages.Base\u0026#34; xmlns:views=\u0026#34;clr-namespace:Core.Views\u0026#34; x:Class=\u0026#34;Core.Pages.Customers.MyCustomersPage\u0026#34;\u0026gt; \u0026lt;ContentPage.ToolbarItems\u0026gt; \u0026lt;ToolbarItem Text=\u0026#34;Info\u0026#34; Command=\u0026#34;{Binding ShowInfoCommand}\u0026#34; Order=\u0026#34;Primary\u0026#34; Icon=\u0026#34;{StaticResource Image.Toolbar.Info}\u0026#34; /\u0026gt; \u0026lt;views:BadgeToolbarItem Text=\u0026#34;Warnings\u0026#34; Command=\u0026#34;{Binding ShowWarningsCommand}\u0026#34; Order=\u0026#34;Primary\u0026#34; Icon=\u0026#34;{StaticResource Image.Toolbar.Warnings}\u0026#34; BadgeValue=\u0026#34;{Binding NumberOfWarnings}\u0026#34; /\u0026gt; \u0026lt;/ContentPage.ToolbarItems\u0026gt; And the last but not least - here is the UIBadgeLabel:\nusing System; using System.Globalization; using Cirrious.FluentLayouts.Touch; using UIKit; namespace iOS.Controls { public sealed class UIBadgeLabel : UILabel { private const double RadiusMultiplier = 0.6; public UIBadgeLabel(int badgeValue) : this(badgeValue, UIFont.SmallSystemFontSize - 2) { } public UIBadgeLabel(int badgeValue, nfloat fontSize) { Text = badgeValue.ToString(CultureInfo.InvariantCulture); TextAlignment = UITextAlignment.Center; BackgroundColor = UIColor.Red; TextColor = UIColor.White; Font = UIFont.SystemFontOfSize(fontSize); ClipsToBounds = true; Layer.CornerRadius = (nfloat)(fontSize * RadiusMultiplier); Layer.BorderColor = UIColor.White.CGColor; Layer.BorderWidth = 1; this.AddConstraints( this.Width().EqualTo((nfloat) (fontSize * RadiusMultiplier * 2)), this.Height().EqualTo((nfloat) (fontSize * RadiusMultiplier * 2))); } } } Again, all the positioning is done with FluentLayouts. The white border for the badge was made in purpose but feel free to adjust this and any other visual or non-visual property to your needs.\nI hope you will find this solution useful to some extent. I\u0026rsquo;m aware that it\u0026rsquo;s not totally universal and probably will require some changes like I mentioned in few points above. Still I believe it can be some a starting point for further work and thinking of the Android (or other platforms) implementation.\nHappy coding!\n","date":"2 July 2017","permalink":"/blog/badge-icon-notification-in-xamarin-forms-toolbaritems-on-ios/","section":"Posts","summary":"\u003cp\u003eMost of iPhone and iPad users can easily recognize icon badges - the pattern for application notifications typically presented in app icon or navigation bar. People that got used to this pattern might want to have the same user experience in their Xamarin Forms application. This post describes how to customize the navigation toolbar in iOS to dynamically display such elements.\u003c/p\u003e","title":"Badge icon notification in Xamarin.Forms ToolbarItems on iOS"},{"content":"Adding tap/click handling to Xamarin.Forms Label is fairly easy. You can do it both in XAML or code behind using GesureRecognizers collection like it is described in this recipe. Unfortunately if you plan to use this solution intensively in your app it will add quite a lot of repeatable code for setting up those handlers (especially in XAML)\nIn this short recipe I will show how to implement a custom control that is easy to use and potentially to extend if needed.\nIn most cases the custom controls in Xamarin.Forms are written together with custom platform renderers. This time there is no need doing this because all required functionality is contained within the standard Forms Label. All that needs to be done is to simplify its usage by exposing additional bindable properties. Those properties can be used in XAML to wire up the Labels behavior with model commands.\nHere is the C# code:\nusing System.Windows.Input; using Xamarin.Forms; namespace ExampleTappableLabelSolution { public class TappableLabel : Label { public static BindableProperty TappedCommandProperty = BindableProperty.Create(nameof(TappedCommand), typeof(ICommand), typeof(TappableLabel), propertyChanged: TappedCommandPropertyChanged); public static BindableProperty TappedCommandParameterProperty = BindableProperty.Create(nameof(TappedCommandParameter), typeof(object), typeof(TappableLabel), propertyChanged: TappedCommandParameterPropertyChanged); private readonly TapGestureRecognizer _tapGestureRecognizer = new TapGestureRecognizer(); public TappableLabel() { GestureRecognizers.Add(_tapGestureRecognizer); } public ICommand TappedCommand { get { return (ICommand) GetValue(TappedCommandProperty); } set { SetValue(TappedCommandProperty, value); } } public object TappedCommandParameter { get { return GetValue(TappedCommandParameterProperty); } set { SetValue(TappedCommandParameterProperty, value); } } private static void TappedCommandPropertyChanged(BindableObject bindable, object oldValue, object newValue) { var label = (TappableLabel) bindable; label._tapGestureRecognizer.Command = (ICommand) newValue; } private static void TappedCommandParameterPropertyChanged(BindableObject bindable, object oldValue, object newValue) { var label = (TappableLabel) bindable; if (newValue != null) label._tapGestureRecognizer.CommandParameter = newValue; } } } In order to use it in XAML first add the proper namespace declaration (assuming the custom control is in the assembly ExampleTappableLabelSolution):\n\u0026lt;ContentPage ... xmlns:views=\u0026#34;clr-namespace:ExampleTappableLabelSolution;assembly=ExampleTappableLabelSolution\u0026#34; The final usage may look like this:\n\u0026lt;views:TappableLabel Text=\u0026#34;{Tap me}\u0026#34; TappedCommand=\u0026#34;{Binding MyModelTapCommand}\u0026#34; /\u0026gt; Remember to add MyModelTapCommand command in your view model class.\nThe single tap gesture is one of the most popular and I find this exact implementation quite useful in my daily work. Notwithstanding this solution can be easily extended to handle other types of gestures or parametrize number of taps required to fire the command. This can be done by adding new bindable properties and logic in respective BindingPropertyChangedDelegate methods.\nHappy coding!\n","date":"21 June 2017","permalink":"/blog/tappable-label-in-xamarin-forms/","section":"Posts","summary":"\u003cp\u003eAdding tap/click handling to Xamarin.Forms Label is fairly easy. You can do it both in XAML or code behind using \u003ccode\u003eGesureRecognizers\u003c/code\u003e collection like it is \u003ca href=\"https://developer.xamarin.com/guides/xamarin-forms/application-fundamentals/gestures/tap/\" target=\"_blank\" rel=\"noreferrer\"\u003edescribed in this recipe\u003c/a\u003e. Unfortunately if you plan to use this solution intensively in your app it will add quite a lot of repeatable code for setting up those handlers (especially in XAML)\u003c/p\u003e","title":"Tappable label in Xamarin.Forms"},{"content":"","date":null,"permalink":"/tags/adal/","section":"Tags","summary":"","title":"Adal"},{"content":"Microsoft Azure Active Directory Authentication Libraries (ADAL) is a popular set wrapper around Azure Active Directory API distributed in the form of platform and language specific components. It\u0026rsquo;s especially useful in multi-platform applications that integrate with various AD APIs such as Outlook or Graph API. It not only wraps the oAuth endpoints but automates the entire application flow for retrieving, refreshing and persisting tokens.\nUnfortunately, among many features, ADAL does not provide the logout functionality out of the box. Let\u0026rsquo;s see how to implement this in few simple steps.\nFirst one is to define a common abstraction that can be referenced in PCL:\nNext is the implementation of Android provider. On this platform ADAL stores tokens in SharedPreferences under specific name and key. With this knowledge the implementation is straightforward:\niOS implementation is quite similar. It uses the KeyChain for the same purpose and the removal is pretty simple:\nAt the end let\u0026rsquo;s consider a sample scenario - clearing user credentials (tokens) in MVVMCross application on reinstallation. To achieve this the app needs to remember if it has been run since the last uninstall or if it\u0026rsquo;s the first time. This state can be persisted using i.e. the Xamarin.Settings plugin (in the example it\u0026rsquo;s wrapped with IApplicationSettings interface). In the first case, it would also be nice to not bother the user showing him the login screen.\nHere\u0026rsquo;s the sample code:\nHappy coding!\n","date":"16 June 2017","permalink":"/blog/cleaning-adal-token-cache/","section":"Posts","summary":"\u003cp\u003eMicrosoft \u003ca href=\"https://docs.microsoft.com/en-us/azure/active-directory/develop/active-directory-authentication-libraries\" target=\"_blank\" rel=\"noreferrer\"\u003eAzure Active Directory Authentication Libraries\u003c/a\u003e (ADAL) is a popular set wrapper around \u003ca href=\"https://docs.microsoft.com/en-us/azure/active-directory/\" target=\"_blank\" rel=\"noreferrer\"\u003eAzure Active Directory\u003c/a\u003e API distributed in the form of platform and language specific components. It\u0026rsquo;s especially useful in multi-platform applications that integrate with various AD APIs such as Outlook or Graph API. It not only wraps the oAuth endpoints but automates the entire application flow for retrieving, refreshing and persisting tokens.\u003c/p\u003e","title":"Cleaning ADAL token cache on Android and iOS"},{"content":"All the credits for solution described in this post goes my friend qbus. He saved few hours of my life and my sanity ;-)\nIn Xamarin.Forms application I have a button with non-default styling (defined in XAML). Target platforms are iOS and Android. Enabled and disabled states should obviously be distinguished with colors.\nThe most straightforward solution that makes use of Data Triggers works well for Android. For some reason it doesn\u0026rsquo;t want to work properly on iOS. If the button is initially disabled it displays correctly in this state but after changing IsEnabled to true and then back to false it does not anymore. Interestingly the background taken from style setter is good, but not the text color property. It turns out to be set to some awkward value: rgb(123, 123, 123) with alpha 89. I guess this is the default value for disabled state because it\u0026rsquo;s not defined anywhere in the app.\nThe problem is well described on the Xamarin Forum thread. Unfortunately none of the proposed workarounds worked for me.\nThe initial workaround implemented by the previous developer was implementing a custom renderer. In OnElementChanged(ElementChangedEventArgs\u0026lt;Button\u0026gt;) the button\u0026rsquo;s title color was set for UIControlState.Disabled state. For some weird reason this was working only for the initial disable state (as described above). The second solution, proposed by my friend, is using the NSAttributedString set as entire button title:\npublic class ButtonDisabledTextColorRenderer : ButtonRenderer { private static UIColor DisabledColor =\u0026gt; Color.FromHex(\u0026#34;#FFFFFF\u0026#34;).ToUIColor(); protected override void OnElementChanged(ElementChangedEventArgs\u0026lt;Button\u0026gt; e) { base.OnElementChanged(e); if (Control == null) return; var title = new NSAttributedString(Control.CurrentTitle, new UIStringAttributes {ForegroundColor = DisabledColor}); Control.SetAttributedTitle(title, UIControlState.Disabled); } } I should call this rather workaround than the real clean solution. Xamarin.Forms obviously has some bug in this case. Nevertheless it works.\nHappy coding!\n","date":"7 June 2017","permalink":"/blog/disabled-button-style-on-ios-in-xamarin-forms/","section":"Posts","summary":"\u003cp\u003eAll the credits for solution described in this post goes \u003ca href=\"https://www.facebook.com/qbus00\" target=\"_blank\" rel=\"noreferrer\"\u003emy friend qbus\u003c/a\u003e. He saved few hours of my life and my sanity ;-)\u003c/p\u003e","title":"Disabled button style on iOS in Xamarin.Forms"},{"content":"","date":null,"permalink":"/tags/forms/","section":"Tags","summary":"","title":"Forms"},{"content":"ModernHttpClient is a great wrapper around native HTTP clients offered by Xamarin. It wraps NSURLSession on iOS and OkHttp on Android. As it deals pretty well with SSL/TLS stack (especially in uncommon scenarios) it\u0026rsquo;s often used instead of built-in types. I switched to ModernHttpClient because of weird errors on connections to the preproduction environment in Android.\nOne disadvantage of using this library is that it\u0026rsquo;s not actively maintained (last update in repo was from Jan 2016). There are few annoying bugs that have not been (and probably won\u0026rsquo;t be) fixed. Because of one of them I was forced to customize the Android client a little bit. The result was the unusual reference to the NativeMessageHandler in iOS project. Locally everything was working fine but on the build machine the following error appeared (in the native iOS project):\nerror CS0012: The type `System.Net.Http.HttpClientHandler\u0026#39; is defined in an assembly that is not referenced. Consider adding a reference to assembly `System.Net.Http, Version=4.0.0.0, Culture=neutral, PublicKeyToken=b03f5f7f11d50a3a\u0026#39; This error seemed to be similar to the one described on SO. The suggested solution was to re-create the entire solution from bare template. Obviously I\u0026rsquo;m not a big fan of such methods.\nAfter trying to add few NuGets which seemed natural (System.Net.Http, System.Net.Primitives) I figured out that this was not the case. Libs from these packages were not those referenced by ModernHttpHandler. Good IDE, such as JetBrains Rider, with decompiler integrated with the smart navigation can be really helpful in such investigations.\nThe solution was adding reference to System.Net.Http assembly from standard Mono libs directly in iOS csproj file:\n\u0026lt;Reference Include=\u0026#34;System.Net.Http\u0026#34; /\u0026gt; Happy coding!\n","date":"6 June 2017","permalink":"/blog/cannot-find-referenced-system-net-http-httpclienthandler/","section":"Posts","summary":"\u003cp\u003e\u003ca href=\"https://github.com/paulcbetts/ModernHttpClient\" target=\"_blank\" rel=\"noreferrer\"\u003eModernHttpClient\u003c/a\u003e is a great wrapper around native HTTP clients offered by Xamarin. It wraps \u003ccode\u003eNSURLSession\u003c/code\u003e on iOS and \u003ccode\u003eOkHttp\u003c/code\u003e on Android. As it deals pretty well with SSL/TLS stack (especially in uncommon scenarios) it\u0026rsquo;s often used instead of built-in types. I switched to \u003ccode\u003eModernHttpClient\u003c/code\u003e because of  weird errors on connections to the preproduction environment in Android.\u003c/p\u003e","title":"ModernHttpClient and 'Type HttpClientHandler defined in unreferenced assembly' error"},{"content":"When you build a multi-platform application in .NET, especially for the mobile, you typically choose between two approaches. One is to code the shared UI layer commonly with Xamarin.Forms (you will still need to have some parts to be placed in platform projects, like custom renderers or providers). The second is to put the entire UI code in platform-specific projects. In this approach you can use the full power of each platform features (like fragments on Android). Both solutions allow for sharing common business logic between all the platforms. On the other hand full implementation of MVVM pattern in the second approach can be tricky and time consuming. The solution is to use 3rd party library; and here comes the MvvmCross. It covers many more areas than the pure MVVM pattern:\ncross-platform plug-ins, platform-specific helpers (i.e. Android recycler views with grouping or iOS table views with simplified customization API), IoC and messaging infrastructure. Thanks to the consistent design, all the plugins and extensions, it can seriously speed-up building UI-intensive cross-platform projects. And it still allows you to take all the benefits provided by MVVM design pattern.\nNow let’s see how to make Ninject and MvvmCross work together on your project. I will describe how to set it up for PCL, Xamarin.iOS and Xamarin.Android (the general rules remain the same for other platforms).\nYou can implement everything from scratch, but I decided to use a helper package MvvmCross.Adapter.Ninject. To avoid adding new dependency and to be able to track dependencies in depth, I just add this code to my projects (it\u0026rsquo;s only two classes in PCL and one for each platform). If you decide to add it as a NuGet package, remember to check for pre-release versions. Also, keep in mind that two platform-specific classes - NinjectMvxIosSetup and NinjectMvxDroidSetup - are missing in the package. In such case, remember to add them as I describe below.\nThe first thing to implement is the IMvxIoCProvider. This is the base type for DI container in MvvmCross and it\u0026rsquo;s basically only the facade around the real container. It is required by MvvmCross as an interface between its API and Ninject kernel methods. MvvmCross.Adapter.Ninject comes with NinjectMvxIoCProvider class for this (full code available here):\nSecond type, NinjectDependenciesProvider, is not necessarily required by MvvmCross but is a quite handy helper. It will help you separate the dependencies that are platform specific (GetPlatformSpecificModules()) from those defined in your common PCL project (GetPortableNinjectModules()).\nNext, you will need some code to plug this into the MvvmCross infrastructure on each platform. It will consist of two pieces: type derived from NinjectDependenciesProvider and the one derived from Mvx...Setup.\nProviders job is to return the dependencies in form of standard INinjectModule collections. Those classes are quite straightforward:\nMost probably you will want to have at least one for PCL and for each of supported platforms:\nMvx...Setup classes are responsible for bootstrapping MvvmCross on each platform. Like the NinjectDependenciesProvider they are just an abstract base classes that ensures the actual platform setup types configures the DI container properly:\nObviously, you can omit this level of inheritance and configure IoC directly in your MvxSetup.InitializeLastChance() method but I find this implementation more elegant. You will find those classes also in MvvmCross.Adapter.Ninject but for some reason, they are not present in NuGet package.\nThe final step is creating the platform-specific setup classes that derives from above abstract setups which boils down to implement GetNinjectDependenciesProvider() method:\nThis configuration will enable you to use the standard Mvx.Resolve\u0026lt;T\u0026gt;() to resolve the dependencies from Ninject container. More importantly, it will do the same for other standard MvvmCross components.\nHappy coding!\n","date":"8 May 2017","permalink":"/blog/how-to-setup-ninject-as-the-default-di-container-in-mvvmcross/","section":"Posts","summary":"\u003cp\u003eWhen you build a multi-platform application in .NET, especially for the mobile, you typically choose between two approaches. One is to code the shared UI layer commonly with Xamarin.Forms (you will still need to have some parts to be placed in platform projects, like custom renderers or providers). The second is to put the entire UI code in platform-specific projects. In this approach you can use the full power of each platform features (like fragments on Android). Both solutions allow for sharing common business logic between all the platforms. On the other hand full implementation of MVVM pattern in the second approach can be tricky and time consuming. The solution is to use 3rd party library; and here comes the MvvmCross. It covers many more areas than the pure MVVM pattern:\u003c/p\u003e","title":"How to setup Ninject as the default DI container in MvvmCross?"},{"content":"","date":null,"permalink":"/tags/mvvmcross/","section":"Tags","summary":"","title":"Mvvmcross"},{"content":"Have you ever encountered an error: Error executing task BuildApk: .../bin/packaged_resources does not exist? If so you probably know that solving this issue can be sometimes quite tricky. It may be hard to track when and where the true bug was introduced in the code base. Although some suggestions can be found on the Xamarin Forum the solution usually differ case by case.\nThe initial error may look like this:\nBuild FAILED. Errors: /.../MyApp.csproj (SignAndroidPackage) -\u0026gt; /Library/Frameworks/Mono.framework/External/xbuild/Xamarin/Android/Xamarin.Android.Common.targets (_BuildApkEmbed target) -\u0026gt; /Library/Frameworks/Mono.framework/External/xbuild/Xamarin/Android/Xamarin.Android.Common.targets: error : Error executing task BuildApk: obj/Debug/android/bin/packaged_resources does not exist It doesn\u0026rsquo;t tell to much. The true issue is usually related not with the BuildApk task itself but with the previous one - generating the app package resources with AAPT tool.\nTo track down the true error first turn on the detailed build logs. Here is a great instruction how to do this step by step and where to find the log output.\nNext step is to find the error in the detailed log. Look for the error APT string. Here is an example:\nFinal:\nExecuting package -f -m -M obj/Debug/android/manifest/AndroidManifest.xml -J /var/folders/ls/dtrrhlz12qb0cqwvqhs008b80000gn/T/zct7dmic.ctk --custom-package my.app.droid -F obj/Debug/android/bin/packaged_resources.bk -S obj/Debug/res/ -S ... Resources/layout/RoomDetailView.axml(2): error APT0000: No resource identifier found for attribute \u0026#39;compatElevation\u0026#39; in package \u0026#39;my.app.droid\u0026#39; Done executing task \u0026#34;Aapt\u0026#34; In my case the error was caused by wrongly defined axml attribute local:compatElevation:\n\u0026lt;android.support.design.widget.FloatingActionButton android:id=\u0026#34;@+id/roomDetailBookRoomButton\u0026#34; android:layout_width=\u0026#34;wrap_content\u0026#34; android:layout_height=\u0026#34;wrap_content\u0026#34; local:srcCompat=\u0026#34;@drawable/ic_lock_outline_white_24px\u0026#34; android:clickable=\u0026#34;true\u0026#34; local:MvxBind=\u0026#34;Click BookRoomCommand; Visibility RoomInfo, Converter=BookButtonVisibility\u0026#34; local:backgroundTint=\u0026#34;@color/mint\u0026#34; android:elevation=\u0026#34;15dp\u0026#34; local:compatElevation=\u0026#34;15dp\u0026#34; android:layout_centerInParent=\u0026#34;true\u0026#34;/\u0026gt; Like I wrote before - it\u0026rsquo;s usually different in each case but now you know how to check what is the issue with your app.\nHappy coding!\n","date":"2 May 2017","permalink":"/blog/error-executing-task-buildapk-packaged-resources-does-not-exist/","section":"Posts","summary":"\u003cp\u003eHave you ever encountered an error: \u003ccode\u003eError executing task BuildApk: .../bin/packaged_resources does not exist\u003c/code\u003e? If so you probably know that solving this issue can be sometimes quite tricky. It may be hard to track when and where the true bug was introduced in the code base. Although some suggestions can be \u003ca href=\"https://forums.xamarin.com/discussion/63356/the-file-obj-debug-android-bin-packaged-resources-does-not-exist\" target=\"_blank\" rel=\"noreferrer\"\u003efound on the Xamarin Forum\u003c/a\u003e the solution usually differ case by case.\u003c/p\u003e","title":"Error executing task BuildApk: packaged_resources does not exist"},{"content":"As it\u0026rsquo;s mentioned in MVVMCross documentation Android is quite specific in terms of navigation requirements. The entry point is statically indicated by MainLauncher = true attribute parameter on the activity. On rest of the platforms, specifically on iOS, this can be done dynamically by implementing IMvxAppStart class for registration in MvxApplication.Initialize().\nTo achieve this MVVMCross introduce a special lightweight type of activity - IMvxAndroidSplashScreenActivity. Unfortunately the default implementation does not support the features from MvvmCross.Droid.Support.V7.AppCompat like vector images or material design themes.\nThe workaround is pretty straightforward. MvxSplashScreenActivity can be rewritten to derive from MvxAppCompatActivity instead of MvxActivity:\npublic abstract class MvxSplashScreenAppCompatActivity : MvxAppCompatActivity\u0026lt;MvxNullViewModel\u0026gt;, IMvxAndroidSplashScreenActivity { // ... } For rest of the code see the original class.\nHappy coding!\n","date":"20 April 2017","permalink":"/blog/appcompat-support-in-mvvmcross-android-splash-screen/","section":"Posts","summary":"\u003cp\u003eAs it\u0026rsquo;s mentioned in \u003ca href=\"https://www.mvvmcross.com/documentation/fundamentals/navigation/navigation.html\" target=\"_blank\" rel=\"noreferrer\"\u003eMVVMCross documentation\u003c/a\u003e Android is quite specific in terms of navigation requirements. The entry point is statically indicated by \u003ccode\u003eMainLauncher = true\u003c/code\u003e attribute parameter on the activity. On rest of the platforms, specifically on iOS, this can be done dynamically by implementing \u003ccode\u003eIMvxAppStart\u003c/code\u003e class for registration in \u003ccode\u003eMvxApplication.Initialize()\u003c/code\u003e.\u003c/p\u003e","title":"AppCompat support in MVVMCross Android splash screen"},{"content":"","date":null,"permalink":"/tags/.net/","section":"Tags","summary":"","title":".Net"},{"content":"","date":null,"permalink":"/tags/scheduling/","section":"Tags","summary":"","title":"Scheduling"},{"content":"Running certain tasks in a scheduled manner may be an easy solution for many problems. One might be refreshing the application cache when the data needs to be fresh and warm no matter what the actual traffic is. Other could be the synchronization or periodical clean-up. There are obviously few good architectural patterns to do it in more elegant and efficient way - distributed queues, publish-subscribe models, enterprise service buses etc. But the simplicity of scheduling still might be an important decision variable.\nI was using Quartz.NET (2.x) for this purpose for quite a while in ASP.NET MVC5. It\u0026rsquo;s simple to use in simple scenarios but powerful enough to handle more complex when there is such a need. Recently I needed to implement a scheduling in an ASP.NET Core Web API service. I wanted to have all the Core features on board - importantly dependency injection, strongly-typed configuration and standard logging. It turned out not to be as straight forward as it seemed on the beginning. But was not so hard either. Here is how I did it.\nThere\u0026rsquo;s one thing worth to mention before I start. My ASP.NET Core service was configured to use .NET Framework 4.6.1 (net461 in project.json), not the .NET Core. I\u0026rsquo;m not sure if there is any .NET Standard- or PCL-compatible version of Quartz.NET. At least in the latest stable release I was using - 2.5. This is something to be checked before you start the development. Especially if running the service on non-Windows environments (like Docker containers) is your requirement. Read the ASP.NET Core documentation for more details on that.\nNow let\u0026rsquo;s move on to the main part.\nThe first step is to create a custom implementation of Quartz IJobFactory. The default one does not allow to use DI on jobs creation. Passing in the IServiceProvider reference enables this scenario in NewJob() method:\npublic class QuartzJonFactory : IJobFactory { private readonly IServiceProvider _serviceProvider; public QuartzJonFactory(IServiceProvider serviceProvider) { _serviceProvider = serviceProvider; } public IJob NewJob(TriggerFiredBundle bundle, IScheduler scheduler) { var jobDetail = bundle.JobDetail; var job = (IJob)_serviceProvider.GetService(jobDetail.JobType); return job; } public void ReturnJob(IJob job) { } } The above example does not utilize job pooling. If you would like to apply this strategy you need to change NewJob() method and implement ReturnJob(). I would recommend this if jobs creation is time consuming or you notice that GC hits the performance doing an intensive work on your IJob objects. This might depend on your triggers configuration.\nNext step is setting-up the dependencies. I did it in ASP.NET Core fashion which is just an extension method on IServiceCollection:\npublic static void UseQuartz(this IServiceCollection services, params Type[] jobs) { services.AddSingleton\u0026lt;IJobFactory, QuartzJonFactory\u0026gt;(); services.Add(jobs.Select(jobType =\u0026gt; new ServiceDescriptor(jobType, jobType, ServiceLifetime.Singleton))); services.AddSingleton(provider =\u0026gt; { var schedulerFactory = new StdSchedulerFactory(); var scheduler = schedulerFactory.GetScheduler(); scheduler.JobFactory = provider.GetService\u0026lt;IJobFactory\u0026gt;(); scheduler.Start(); return scheduler; }); } Here is a utility method that configures the job and its trigger:\npublic static class QuartzServicesUtilities { public static void StartJob\u0026lt;TJob\u0026gt;(IScheduler scheduler, TimeSpan runInterval) where TJob : IJob { var jobName = typeof(TJob).FullName; var job = JobBuilder.Create\u0026lt;TJob\u0026gt;() .WithIdentity(jobName) .Build(); var trigger = TriggerBuilder.Create() .WithIdentity($\u0026#34;{jobName}.trigger\u0026#34;) .StartNow() .WithSimpleSchedule(scheduleBuilder =\u0026gt; scheduleBuilder .WithInterval(runInterval) .RepeatForever()) .Build(); scheduler.ScheduleJob(job, trigger); } } It is very basic and specific to my recent project. You may want to add more flexibility here - for example by exposing a configuration builder with a fluent syntax.\nNow it\u0026rsquo;s time code the actual IJob implementation. Here is an example:\npublic class SomeJob : IJob { private readonly ILogger\u0026lt;SomeJob\u0026gt; _log; private readonly JobConfiguration _configuration; // other dependencies will probably go here public SomeJob(IOptions\u0026lt;JobConfiguration\u0026gt; configuration, ILogger\u0026lt;SomeJob\u0026gt; log) { _log = log; _configuration = configuration.Value; } public void Execute(IJobExecutionContext context) { Task.Run(Execute); } private async Task Execute() { try { // implement your scheduled job logic } catch (Exception ex) { _log.LogError(1, ex, \u0026#34;An error occurred during execution of scheduled job\u0026#34;); } } } I\u0026rsquo;ve included two standard ASP.NET Core dependencies as an example. First is a strongly-typed configuration. I prefer it over the standard Quartz.NET JobDataMap because of consistency with rest of the Web API and clean design. Second is the ASP.NET Core logging infrastructure. If you plan to use it instead of Common.Logging features delivered with Quartz.NET keep in mind to create your scheduled jobs after the logging is configured.\nHere is an example code that you can use in Startup class to wire-up all those pieces:\npublic void ConfigureServices(IServiceCollection services) { // here goes all the IoC configuration services.UseQuartz(typeof(SomeJob)); } public void Configure(IApplicationBuilder app, IHostingEnvironment env, ILoggerFactory loggerFactory) { // here you might want to include other config like logging // and fetch the schedule interval probably from configuration var scheduler = app.ApplicationServices.GetService\u0026lt;IScheduler\u0026gt;(); QuartzServicesUtilities.StartJob\u0026lt;SomeJob\u0026gt;(scheduler, someInterval); } And voilà - that\u0026rsquo;s practically all the code you need for the basic stuff. The rest may be as complex as you can imagine in your own IJob classes.\nHappy coding!\n","date":"16 April 2017","permalink":"/blog/scheduling-in-asp-net-core-with-quartz-net/","section":"Posts","summary":"\u003cp\u003eRunning certain tasks in a scheduled manner may be an easy solution for many problems. One might be refreshing the application cache when the data needs to be fresh and warm no matter what the actual traffic is. Other could be the synchronization or periodical clean-up. There are obviously few good architectural patterns to do it in more elegant and efficient way - distributed queues, publish-subscribe models, enterprise service buses etc. But the simplicity of scheduling still might be an important decision variable.\u003c/p\u003e","title":"Scheduling in ASP.NET Core with Quartz.NET"},{"content":"An error Failure INSTALL_PARSE_FAILED_MANIFEST_MALFORMED appears from time to time in world of Android Java developers. However among Xamarin devs it seems to appear much rarely. Today I came across one and the standard Android solution did not work for me. Curious about the solution I found (with a little help of my friends)?\nFirst of all, here is the full error that is reported by tooling during the app deployment:\nDeploying package to \u0026#39;emulator-5554\u0026#39; Detecting installed packages Installing shared runtime Installing platform framework Installing application on device Deployment failed because of an internal error: Unexpected install output: pkg: /data/local/tmp/solidbrain.space.droid-Signed.apk Failure [INSTALL_PARSE_FAILED_MANIFEST_MALFORMED] Deployment failed. Internal error. My first try was, obviously, looking into the AndroidManifest.xml:\n\u0026lt;?xml version=\u0026#34;1.0\u0026#34; encoding=\u0026#34;utf-8\u0026#34;?\u0026gt; \u0026lt;manifest xmlns:android=\u0026#34;http://schemas.android.com/apk/res/android\u0026#34; package=\u0026#34;somenamespace.app.droid\u0026#34; android:installLocation=\u0026#34;auto\u0026#34; android:versionCode=\u0026#34;1\u0026#34; android:versionName=\u0026#34;1.0\u0026#34;\u0026gt; \u0026lt;uses-sdk android:minSdkVersion=\u0026#34;19\u0026#34; android:targetSdkVersion=\u0026#34;25\u0026#34; /\u0026gt; \u0026lt;uses-permission android:name=\u0026#34;android.permission.ACCESS_NETWORK_STATE\u0026#34; /\u0026gt; \u0026lt;uses-permission android:name=\u0026#34;android.permission.INTERNET\u0026#34; /\u0026gt; \u0026lt;uses-permission android:name=\u0026#34;android.permission.WRITE_EXTERNAL_STORAGE\u0026#34; /\u0026gt; \u0026lt;application android:label=\u0026#34;app\u0026#34; android:icon=\u0026#34;@drawable/Android_App_Icon\u0026#34;\u0026gt;\u0026lt;/application\u0026gt; \u0026lt;/manifest\u0026gt; As you can see everything seems to be fine with it. I\u0026rsquo;ve got a clue from my colleague that the tooling modifies the manifest file during the packaging. So the resulting signed app package might contain a AndroidManifest.xml file with a quite different content.\nSecond clue came from the SO question mentioned before. The answers was focused around the casing of namespaces in the manifest.\nThird piece of information were the changes I made that day in the project. In order to fix some random errors that was appearing in my app from time to time I added missing links between Java and Mono world (native services or activities). Those were constructors with IntPtr native object handle and registration attributes (helpful in debugging and troubleshooting) like this one:\n[Register(\u0026#34;SomeSpace.App.Droid.Views.LoginView\u0026#34;)] public class SomeActivity : AppCompatActivity { } Did you already spot the issue? Yup, the namespaces should be lower-case (apart from the type part):\n[Register(\u0026#34;somespace.app.droid.views.LoginView\u0026#34;)] public class SomeActivity : AppCompatActivity { } Apparently the packing tooling includes those types registration within the AndroidManifest.xml.\n","date":"11 April 2017","permalink":"/blog/xamarin-android-failure-install-parse-failed-manifest-malformed/","section":"Posts","summary":"\u003cp\u003eAn error \u003ccode\u003eFailure INSTALL_PARSE_FAILED_MANIFEST_MALFORMED\u003c/code\u003e appears from \u003ca href=\"http://stackoverflow.com/questions/37066617/failure-install-parse-failed-manifest-malformed\" target=\"_blank\" rel=\"noreferrer\"\u003etime to time\u003c/a\u003e in world of Android Java developers. However among Xamarin devs it seems to appear much rarely. Today I came across one and the standard Android solution did not work for me. Curious about the solution I found (\u003ca href=\"https://www.youtube.com/watch?v=nCrlyX6XbTU\" target=\"_blank\" rel=\"noreferrer\"\u003ewith a little help of my friends\u003c/a\u003e)?\u003c/p\u003e","title":"Xamarin.Android - Failure INSTALL_PARSE_FAILED_MANIFEST_MALFORMED"},{"content":"If you ever encountered the error AAPT: Unknown option '--no-version-vectors' during the Xamarin build you probably found this page looking for a solution. Like I did. Then you probably first tried setting AndroidSdkBuildToolsVersion to the latest you have. Just like me. Or uninstall all the older versions. If this didn\u0026rsquo;t work (like in my case) stay with me.\nThe suggestion of changing the build tools version in Droid csproj didn\u0026rsquo;t even look promising for me. I had only one version installed - 25.0.3 - and this was the latest one (at the time I\u0026rsquo;m writing this post). Nevertheless I checked this out, just in case, as this was the accepted solution in this thread. The error did not disappear.\nI find a reasonable level of laziness practical. In fact this is usually a motivation for me to automate things. And it often pays off. Unfortunately not this time. As a typical lazy developer I skipped the rest of the discussion in the forum thread and started digging on my own. After a wasted hour I got back to the thread and found out that the build version tools solution didn\u0026rsquo;t work for others as well. Importantly one of later posts was pointing the solution.\nTrue reason was the duplicated MSBuild target imports in csproj file:\n\u0026lt;Import Project=\u0026#34;..\\packages\\Xamarin.Android.Support.Vector.Drawable.25.1.0\\build\\MonoAndroid70\\Xamarin.Android.Support.Vector.Drawable.targets\u0026#34; Condition=\u0026#34;Exists(\u0026#39;..\\packages\\Xamarin.Android.Support.Vector.Drawable.25.1.0\\build\\MonoAndroid70\\Xamarin.Android.Support.Vector.Drawable.targets\u0026#39;)\u0026#34; /\u0026gt; \u0026lt;Import Project=\u0026#34;..\\packages\\Xamarin.Android.Support.Animated.Vector.Drawable.25.1.0\\build\\MonoAndroid70\\Xamarin.Android.Support.Animated.Vector.Drawable.targets\u0026#34; Condition=\u0026#34;Exists(\u0026#39;..\\packages\\Xamarin.Android.Support.Animated.Vector.Drawable.25.1.0\\build\\MonoAndroid70\\Xamarin.Android.Support.Animated.Vector.Drawable.targets\u0026#39;)\u0026#34; /\u0026gt; ... \u0026lt;Import Project=\u0026#34;..\\packages\\Xamarin.Android.Support.Vector.Drawable.25.1.1\\build\\MonoAndroid70\\Xamarin.Android.Support.Vector.Drawable.targets\u0026#34; /\u0026gt; \u0026lt;Import Project=\u0026#34;..\\packages\\Xamarin.Android.Support.Animated.Vector.Drawable.25.1.1\\build\\MonoAndroid70\\Xamarin.Android.Support.Animated.Vector.Drawable.targets\u0026#34; /\u0026gt; This was probably caused by NuGet package manager (rather the clone used in Xamarin Studio or JetBrain\u0026rsquo;s Rider I\u0026rsquo;m using).\nLesson learned - RTFM ;-) And don\u0026rsquo;t be too lazy, as I am.\nHappy coding!\n","date":"4 April 2017","permalink":"/blog/aapt-unknown-option-no-version-vectors/","section":"Posts","summary":"\u003cp\u003eIf you ever encountered the error \u003ccode\u003eAAPT: Unknown option '--no-version-vectors'\u003c/code\u003e during the Xamarin build you probably found \u003ca href=\"https://forums.xamarin.com/discussion/63482/aapt-error-unknown-option-no-version-vectors\" target=\"_blank\" rel=\"noreferrer\"\u003ethis page\u003c/a\u003e looking for a solution. Like I did. Then you probably first tried setting \u003ccode\u003eAndroidSdkBuildToolsVersion\u003c/code\u003e to the latest you have. Just like me. Or uninstall all the older versions. If this didn\u0026rsquo;t work (like in my case) stay with me.\u003c/p\u003e","title":"AAPT: Unknown option '--no-version-vectors'"},{"content":"","date":null,"permalink":"/categories/developer-story/","section":"Categories","summary":"","title":"Developer Story"},{"content":"If you are using Azure Active Directory services you probably at least considered using ADAL as a client library in you application. It\u0026rsquo;s easy to setup, use and it offers a unified API across the most popular platforms - iOS, Android, UWP, web - both .Net and native. Unfortunately sometimes things just does not work out of the box without deeper understanding how some features are implemented. This post is about one of them - credentials cache persistency on iOS.\nRunning my Xamarin.iOS application on simulator I quickly noticed that I must login every time the app starts. This was an obvious sign that previously acquired tokens were not properly persisted as they should. On the other hand on the device everything was working fine. In some point this started to be irritating so I dug.\nFor saving the users credentials (in form of oAuth access tokens and refresh tokens) ADAL tries to use the built-in platform secure stores. The exact implementation is platform-specific and it\u0026rsquo;s placed in TokenCachePlugin (here is the implementation for iOS).\nThe secure store for iOS (and Mac as well) is KeyChain. So what is the difference between device and simulator? For Xamarin.iOS apps built with Xamarin Studio or Visual Studio in the Debug|iPhoneSimulator build configuration are not signed. As the Apple documentation states:\nFrom a high level perspective, Keychain Services uses an app’s code signature with its embedded entitlements to ensure that only an authorized app can access a particular keychain item. By default, only the app that created an item can access it in the future.\nSo, from the KeyChain perspective, the unsigned app deployed to simulator simply cannot store or retrieve the records.\nThe solution is fairly simple. In your iOS project settings, iOS Bundle Signing select Debug|iPhoneSimulator configuration and set Provisioning Profile to the same which you are using for the device (not the Automatic!):\nHow to configure provisioning profile If you installed the unsigned package on the simulator before setting this up remember to uninstall it explicitly. Then install the signed package. Simple update from IDE won\u0026rsquo;t work in this case. Thankfully this is a one time operation.\nHappy coding!\nMarek\n","date":"20 March 2017","permalink":"/blog/non-persistent-keychain-on-ios-simulator/","section":"Posts","summary":"\u003cp\u003eIf you are using Azure Active Directory services you probably at least considered using \u003ca href=\"https://docs.microsoft.com/en-us/azure/active-directory/develop/active-directory-authentication-libraries\" target=\"_blank\" rel=\"noreferrer\"\u003eADAL\u003c/a\u003e as a client library in you application. It\u0026rsquo;s easy to setup, use and it offers a unified API across the most popular platforms - iOS, Android, UWP, web - both .Net and native. Unfortunately sometimes things just does not work out of the box without deeper understanding how some features are implemented. This post is about one of them - credentials cache persistency on iOS.\u003c/p\u003e","title":"Why ADAL does not persist user credentials on iOS simulator?"},{"content":"Working on a bug fix in MVVMCross-based mobile application I noticed a strange behavior. The navigation to other view model I put in async Init\u0026lt;TInit\u0026gt;(TInit parameters) which as executed on the first view model in my app was running twice. After a short debugging session it turned out that MvxViewModel\u0026lt;TInit\u0026gt; Init() is called from the view controllers ViewDidLoad() method. Obviously there was something I was missing in terms of ViewDidLoad() semantics.\nQuick search on SO reviled the mystery of iOS SDK (most relevant questions are here and here). The ViewDidLoad() is not considered to be \u0026ldquo;safe\u0026rdquo; in any other aspect than just updating the UI. It can be run any number of times on loading the view. So if you, like me, have some logic that should be run only once you need to put it somewhere else.\nUnfortunately my Init() from VM could not be moved somewhere else easily. I could try to move the logic to some form of queued tasks with \u0026ldquo;run-only-once\u0026rdquo; semantics. The other choice, which I actually took, was preventing the double-run in the VM itself. This can be done by introducing the guard flag and checking it in Init():\npublic class MyViewModel : MvxViewModel\u0026lt;SomeInitType\u0026gt; ... bool IsInitialized; ... protected Init\u0026lt;SomeInitType\u0026gt;(SomeInitType parameters) { if (IsInitialized) { return; } // do the initialization here } Simple as that. Actually I moved this to my base view model class so other VMs could use it (but still decide whenever to allow for multiple init).\nHappy coding!\nMarek\n","date":"16 March 2017","permalink":"/blog/why-mvvmcross-vm-init-twice-on-ios/","section":"Posts","summary":"\u003cp\u003eWorking on a bug fix in MVVMCross-based mobile application I noticed a strange behavior. The navigation to other view model I put in \u003ccode\u003easync Init\u0026lt;TInit\u0026gt;(TInit parameters)\u003c/code\u003e which as executed on the first view model in my app was running twice. After a short debugging session it turned out that \u003ccode\u003eMvxViewModel\u0026lt;TInit\u0026gt;\u003c/code\u003e \u003ccode\u003eInit()\u003c/code\u003e is called from the view controllers \u003ccode\u003eViewDidLoad()\u003c/code\u003e method. Obviously there was something I was missing in terms of \u003ccode\u003eViewDidLoad()\u003c/code\u003e semantics.\u003c/p\u003e","title":"Why does MVVMCross view model initialize twice on iOS?"},{"content":"Considering Xamarin there can be many reasons for need of screen resolution detection in mobile app. You may have more complex logic of loading your resources possibly split between PCL and Android/iOS projects. Other might want to send this information with REST request for reporting. Whatever your reason is, here is a very short text on how to do it in IoC-friendly way.\nFirst we will need some abstraction for the platform-agnostic part of the solution:\npublic interface ISystemInfoService { Platform Platform { get; } Resolution Resolution { get; } } Both fields are enums. Platform is obviously a platform name. Resolution is combined set of all resolutions from both Android and iOS:\npublic enum Platform { Android, iOS } public enum Resolution { Ldpi, Mdpi, Hdpi, Xhdpi, Xxhdpi, Xxxhdpi, X1, X2, X3 } Of course you may want to prefer bare strings over the enums.\nLast two parts are the platform-specific providers that you will bind in your DI configuration.\nIn Android version I\u0026rsquo;m using the MvvmCross helper class - IMvxAndroidCurrentTopActivity - to obtain reference to app resources. This can be achieved in many ways:\npublic class DroidSystemInfoService : ISystemInfoService { private readonly IMvxAndroidCurrentTopActivity _topActivity; public DroidSystemInfoService(IMvxAndroidCurrentTopActivity topActivity) { _topActivity = topActivity; } public Platform Platform =\u0026gt; Platform.Android; public Resolution Resolution { get { var density = _topActivity.Activity.Resources.DisplayMetrics.DensityDpi; switch (density) { case DisplayMetricsDensity.Low: return Resolution.Ldpi; case DisplayMetricsDensity.Medium: return Resolution.Mdpi; case DisplayMetricsDensity.High: return Resolution.Hdpi; case DisplayMetricsDensity.Xhigh: return Resolution.Xhdpi; case DisplayMetricsDensity.Xxhigh: return Resolution.Xxhdpi; case DisplayMetricsDensity.Xxxhigh: return Resolution.Xxxhdpi; default: return Resolution.Xhdpi; } } } } And here is the iOS version:\npublic class IosSystemInfoService : ISystemInfoService { public Platform Platform =\u0026gt; Platform.iOS; public Resolution Resolution { get { switch ((int) UIScreen.MainScreen.Scale) { case 1: return Resolution.X1; case 2: return Resolution.X2; case 3: return Resolution.X3; default: return Resolution.X3; } } } } Happy coding!\nMarek\n","date":"11 March 2017","permalink":"/blog/cross-platform-resolution-detection/","section":"Posts","summary":"\u003cp\u003eConsidering Xamarin there can be many reasons for need of screen resolution detection in mobile app. You may have more complex logic of loading your resources possibly split between PCL and Android/iOS projects. Other might want to send this information with REST request for reporting. Whatever your reason is, here is a very short text on how to do it in IoC-friendly way.\u003c/p\u003e","title":"Detecting device resolution in Xamarin apps"},{"content":"Working with mobile can be quite challenging for a developer with a web dev background. At least that is my experience so far. Comparing to typical HTML web elements with CSS styling some features might be missing.\nAn example for this kind of issues I faced recently is lack explicit padding for UILabel (and not only). You can either let the label to fit tightly around the text content or set the label\u0026rsquo;s size (either statically or with auto-sizing).\nThere are few solutions for this around the Internet (like this or this). Most of them are implemented in Swift/Obj-C, some are somehow incomplete even if provided with Xamarin code. Here is a short compilation of my findings.\nThe steps are easy:\nSub-class UILabel class Override DrawText(CGRect) method Override CGSize IntrinsicContentSize property - if you miss this one your content will probably not fit (result will depend on the UIView.ContentMode) Fortunately both method and property are implemented as virtual.\nHere is the ready example:\npublic class UiExtraPaddingLabel : UILabel { private readonly UIEdgeInsets _edgeInsets; public UiExtraPaddingLabel(nfloat padding) : this(padding, padding, padding, padding) { } public UiExtraPaddingLabel(nfloat top, nfloat left, nfloat bottom, nfloat right) { _edgeInsets = new UIEdgeInsets(top, left, bottom, right); } public override void DrawText(CGRect rect) { base.DrawText(_edgeInsets.InsetRect(rect)); } public override CGSize IntrinsicContentSize { get { var originalSize = base.IntrinsicContentSize; originalSize.Width += _edgeInsets.Left + _edgeInsets.Right; originalSize.Height += _edgeInsets.Top + _edgeInsets.Bottom; return originalSize; } } } Cheers! Marek\n","date":"20 February 2017","permalink":"/blog/xamarin-is-uilabel-with-padding/","section":"Posts","summary":"\u003cp\u003eWorking with mobile can be quite challenging for a developer with a web dev background. At least that is my experience so far. Comparing to typical HTML web elements with CSS styling some features might be missing.\u003c/p\u003e","title":"How to add padding to UILabel in Xamain.iOS"},{"content":"","date":null,"permalink":"/tags/blog/","section":"Tags","summary":"","title":"Blog"},{"content":"","date":null,"permalink":"/tags/mobile/","section":"Tags","summary":"","title":"Mobile"},{"content":"Those of you who enter this blog from time to time probably noticed that there was not update since 4 months. I hope this will change due to my intensive learning in mobile development area.\nThere will be new posts about my little wins and fails related with Xamarin, iOS and Android platforms - some of them published here and some cross-blogged from Solidbrain blog. Of corse I still want to continue the enterprise search topic I started some time ago. I hope you\u0026rsquo;ll enjoy the reading.\nCheers! Marek\n","date":"20 February 2017","permalink":"/blog/short-break/","section":"Posts","summary":"\u003cp\u003eThose of you who enter this blog from time to time probably noticed that there was not update since 4 months. I hope this will change due to my intensive learning in mobile development area.\u003c/p\u003e","title":"Short break"},{"content":"","date":null,"permalink":"/tags/csharp/","section":"Tags","summary":"","title":"Csharp"},{"content":"","date":null,"permalink":"/tags/il/","section":"Tags","summary":"","title":"Il"},{"content":"","date":null,"permalink":"/tags/roslyn/","section":"Tags","summary":"","title":"Roslyn"},{"content":"It\u0026rsquo;s always good to have static code analysis in your build process. I guess no one these days argues with this statement. This usually forces developer to make conscious decisions on code-level performance, reliability, security, design etc. Few times CA warnings saved me from producing a quite nasty bugs. Sometimes however FxCop yields some really strange stuff.\nThis post will describe the one I stuck with some time ago. But more interestingly it shows that sometimes .Net developer must look deep under the hood of high-level language abstraction to solve certain issues.\nThe story #I wrote a simple unit test class for Web API controller:\npublic class NewsWebApiControllerTests { ... // some mocks and testing methods goes here ... private readonly IEnumerable\u0026lt;NewsListItem\u0026gt; _expectedNews = new List\u0026lt;NewsListItem\u0026gt; { ... // really long list initialization... ... } // ...and other similar fields declarations for testing expectations ... } As you can see - nothing special. It was running locally pretty fine so I made a commit, push and\u0026hellip; after few minutes saw this error in Team City log:\n[11:21:22][Abb.One.InsidePlus\\Abb.One.InsidePlus.UnitTests\\Abb.One.InsidePlus.UnitTests.csproj] RunCodeAnalysis (12s) [11:21:22][RunCodeAnalysis] CodeAnalysis (12s) [11:21:22][CodeAnalysis] Running Code Analysis... [11:21:22][CodeAnalysis] C:\\Program Files (x86)\\Microsoft Visual Studio 12.0\\Team Tools\\Static Analysis Tools\\FxCop\\FxCopCmd.exe ... [11:21:35][CodeAnalysis] d:\\BuildAgent\\Work\\TrunkInside\\Abb.One.InsidePlus\\Abb.One.InsidePlus.UnitTests\\Web\\Controllers\\NewsWebApiControllerTests.cs(147, 0): warning CA1809: Microsoft.Performance : \u0026#39;NewsWebApiControllerTests.NewsWebApiControllerTests()\u0026#39; has 80 local variables, some of which may have been generated by the compiler. Refactor \u0026#39;NewsWebApiControllerTests.NewsWebApiControllerTests()\u0026#39; so that it uses fewer than 64 local variables. [11:21:35][CodeAnalysis] Code Analysis Complete -- 0 error(s), 1 warning(s) Our build process recipe in TC treats all such warnings as errors so the build failed.\nI double-checked the build output on local configuration (I have the same FxCop rules applied in my VS2015 build) but found no CA1809 warning there. There was nothing useful on MSDN documentation for this case. The issue was even more mysterious because the test class doesn\u0026rsquo;t actually have any constructor. I quickly concluded that it\u0026rsquo;s rather about the code \u0026ldquo;generated by the compiler\u0026rdquo; as suggested in the message above.\nQuick fix #Since I didn\u0026rsquo;t have any constructor method to apply the suppression attribute (or do it in GlobalSupressions.cs file) I created one:\n[SuppressMessage(\u0026#34;Microsoft.Performance\u0026#34;, \u0026#34;CA1809:AvoidExcessiveLocals\u0026#34;)] public NewsWebApiControllerTests() { } This really did the trick - no CA warning was reported from that time in this class.\nGoing deeper #What was still bothering me was the question - why this warning appeared on the build server and on my local machine it did not?\nAfter a little digging in build logs from both environments I found the first difference. The version on build machine was 12.0 while my local builds were made on 14.0. My guess was that Roslyn compiler (14.0) was producing more compact IL code than the previous versions.\nThe last thing to check was the actual IL output from both versions. It\u0026rsquo;s quite easy with ILSpy The result was not a big surprise in face of above findings. This is the part of constructor IL generated by the classic compiler (12.0):\n/* TC: Abb.One.InsidePlus.UnitTests.dll */ .method /*060000E6*/ public hidebysig specialname rtspecialname instance void .ctor() cil managed { .custom /*0C00023C:0A00000F*/ instance void [mscorlib/*23000001*/]System.Diagnostics.CodeAnalysis.SuppressMessageAttribute/*0100009C*/::.ctor(string, string) /* 0A00000F */ = ( 01 00 15 4D 69 63 72 6F 73 6F 66 74 2E 50 65 72 // ...Microsoft.Per 66 6F 72 6D 61 6E 63 65 1B 43 41 31 38 30 39 3A // formance.CA1809: 41 76 6F 69 64 45 78 63 65 73 73 69 76 65 4C 6F // AvoidExcessiveLo 63 61 6C 73 00 00 ) // cals.. // Code size 4438 (0x1156) .maxstack 6 .locals /*11000048*/ init (class [Abb.One.InsidePlus.Lib/*23000002*/]Abb.One.InsidePlus.Lib.Models.ViewModels.NewsListItem/*0100003F*/ V_0, class [Abb.One.InsidePlus.Lib/*23000002*/]Abb.One.InsidePlus.Lib.Models.ViewModels.NewsListItemLink/*01000129*/ V_1, class [Abb.One.InsidePlus.Lib/*23000002*/]Abb.One.InsidePlus.Lib.Models.ViewModels.NewsListItemLink/*01000129*/ V_2, /* more local variables declarations */ class [mscorlib/*23000001*/]System.Collections.Generic.List`1/*010000CC*/\u0026lt;class [Abb.One.InsidePlus.Lib/*23000002*/]Abb.One.InsidePlus.Lib.Models.NewsLink/*0100012B*/\u0026gt; V_78, class [Abb.One.InsidePlus.Lib/*23000002*/]Abb.One.InsidePlus.Lib.Models.NewsLink/*0100012B*/ V_79) IL_0000: ldarg.0 IL_0001: newobj instance void [Abb.One.InsidePlus.Lib/*23000002*/]Abb.One.InsidePlus.Lib.Models.ViewModels.NewsListItem/*0100003F*/::.ctor() /* 0A0001E0 */ IL_0006: stloc.0 IL_0007: ldloc.0 IL_0008: newobj instance void [Abb.One.InsidePlus.Lib/*23000002*/]Abb.One.InsidePlus.Lib.Models.ViewModels.NewsListItemLink/*01000129*/::.ctor() /* 0A0001E1 */ IL_000d: stloc.1 IL_000e: ldloc.1 IL_000f: ldstr \u0026#34;http://www.example.com\u0026#34; /* 70004469 */ IL_0014: callvirt instance void [Abb.One.InsidePlus.Lib/*23000002*/]Abb.One.InsidePlus.Lib.Models.ViewModels.NewsListItemLink/*01000129*/::set_Url(string) /* 0A0001E2 */ /* ...around 1.5k lines of IL code goes here */ IL_1154: nop IL_1155: ret } // end of method NewsWebApiControllerTests::.ctor and this the one is from Roslyn (14.0):\n/* Local: Abb.One.InsidePlus.UnitTests.dll */ .method /*06000099*/ public hidebysig specialname rtspecialname instance void .ctor() cil managed { .custom /*0C0001A7:0A000005*/ instance void [mscorlib/*23000001*/]System.Diagnostics.CodeAnalysis.SuppressMessageAttribute/*01000006*/::.ctor(string, string) /* 0A000005 */ = ( 01 00 15 4D 69 63 72 6F 73 6F 66 74 2E 50 65 72 // ...Microsoft.Per 66 6F 72 6D 61 6E 63 65 1B 43 41 31 38 30 39 3A // formance.CA1809: 41 76 6F 69 64 45 78 63 65 73 73 69 76 65 4C 6F // AvoidExcessiveLo 63 61 6C 73 00 00 ) // cals.. // Code size 3803 (0xedb) .maxstack 10 IL_0000: ldarg.0 IL_0001: newobj instance void [Abb.One.InsidePlus.Lib/*23000005*/]Abb.One.InsidePlus.Lib.Models.ViewModels.NewsListItem/*01000074*/::.ctor() /* 0A00015D */ IL_0006: dup IL_0007: newobj instance void [Abb.One.InsidePlus.Lib/*23000005*/]Abb.One.InsidePlus.Lib.Models.ViewModels.NewsListItemLink/*01000128*/::.ctor() /* 0A00015E */ IL_000c: dup IL_000d: ldstr \u0026#34;http://www.example.com\u0026#34; /* 7000116C */ IL_0012: callvirt instance void [Abb.One.InsidePlus.Lib/*23000005*/]Abb.One.InsidePlus.Lib.Models.ViewModels.NewsListItemLink/*01000128*/::set_Url(string) /* 0A00015F */ ... /* around 1.2k lines of IL code */ IL_0ec7: callvirt instance void class [mscorlib/*23000001*/]System.Collections.Generic.List`1/*01000046*/\u0026lt;class [Abb.One.InsidePlus.Lib/*23000005*/]Abb.One.InsidePlus.Lib.Models.News/*01000075*/\u0026gt;/*1B000059*/::Add(!0) /* 0A000184 */ IL_0ecc: nop IL_0ecd: stfld class [mscorlib/*23000001*/]System.Collections.Generic.IEnumerable`1/*0100002E*/\u0026lt;class [Abb.One.InsidePlus.Lib/*23000005*/]Abb.One.InsidePlus.Lib.Models.News/*01000075*/\u0026gt; Abb.One.InsidePlus.UnitTests.Web.Controllers.NewsWebApiControllerTests/*02000015*/::_NewsForGermany /* 0400003B */ IL_0ed2: ldarg.0 IL_0ed3: call instance void [mscorlib/*23000001*/]System.Object/*01000014*/::.ctor() /* 0A00001D */ IL_0ed8: nop IL_0ed9: nop IL_0eda: ret } // end of method NewsWebApiControllerTests::.ctor It\u0026rsquo;s clearly visible on above IL listings that the code generated by Roslyn doesn\u0026rsquo;t use local variables on class initialization (readonly fields) but operates more intensively on stack. I guess this is for sake of memory usage optimizations.\nThe proper permanent fix for such errors was obviously upgrading the compiler version (together with MSBuild tooling) on the build environment.\nWrapping up #It was not obvious at the first look that CA warning could have something to do with compiler version. As you can see sometimes you must just go deeper into the rabbit hole to understand what is really happening with your code. And this is not only about coding. I think about the entire process from writing it to running on production. This might not be straightforward but I personally find a joy in solving the lower-level issues from time to time. I hope that it was interesting for you as well or at least somehow inspiring ;-)\nCheers! Marek\n","date":"18 August 2016","permalink":"/blog/under-the-hood/","section":"Posts","summary":"\u003cp\u003eIt\u0026rsquo;s always good to have static code analysis in your build process. I guess no one these days argues with this statement. This usually forces developer to make conscious decisions on code-level performance, reliability, security, design etc. Few times CA warnings saved me from producing a quite nasty bugs. Sometimes however FxCop yields some really strange stuff.\u003cbr\u003e\nThis post will describe the one I stuck with some time ago. But more interestingly it shows that sometimes .Net developer must look deep under the hood of high-level language abstraction to solve certain issues.\u003c/p\u003e","title":"Under the hood"},{"content":"","date":null,"permalink":"/tags/enterprise-search/","section":"Tags","summary":"","title":"Enterprise Search"},{"content":"Recently leafing through my notebook (yeah, I\u0026rsquo;m still using old-time paper) I found some ideas, quick-notes, self-brainstorming diagrams related with search systems. An original idea for this blog, at least for few first posts, was to present them. Still this requires introducing some of the basics of enterprise search and search systems in general (so I could spare some time in future just referring to those). This is what this post series is about.\nIn this part I will try to answer two questions - why do people need an enterprise search systems and how complex they can be?\nLittle piece of history #I still remember the times when most of public the pages in the Internet were registered in the web version of yellow pages - the web directories. Until the mid-90\u0026rsquo;s most of the Internet portals was maintaining such service built manually or semi-automatically. Even if the content discovery was made by a crawler the content classification and final categorization was a human domain.\nObviously when the number of resources in the Net was growing exponentially over the time and media other than text was becoming more and more popular the web directories were getting stale from day to day or even faster. The maintenance became a Sisyphean task.\nInternet search engines are the response for the explosion of information on the web. They can discover and retrieve the information automatically and serve it in a way that no manually-built directory can - extremely fast, with enormous throughput and in intelligent way. While content retrieval and discovery seems to be obvious the efficiency of serving mechanism in comparison to web directories requires some explanation here. Having such a huge content database a simple key word matching or category browsing with sorting by date (or other attribute) is usually very time consuming and ineffective. With the search adjusted to the natural language and sorting results by relevance this task becomes manageable and convenient.\nWhat\u0026rsquo;s the difference? #But how does it apply to the enterprise search? Internet search providers such as Google, Yahoo! or Bing deal with exabytes (1 exabyte = 10^18 byte) of publicly available content, mostly unstructured. Enterprise search systems are typically implemented in companies or other organizations of various size. They usually deal with much smaller amount of data (still too much for manually maintained databases). The most important differences are not in the amount but in it\u0026rsquo;s nature: the data describes specific domains, it\u0026rsquo;s usually structured or at least provided with some meaningful meta-data, in many cases it\u0026rsquo;s not public (secure content).\nAnother dimension of those systems is the source (or rather sources) and format of data - not only pages available on the web but also records from relational and non-relational databases, file shares, spreadsheets, other systems integrated with web services etc. Enterprise search in larger organizations typically allows for searching heterogeneous data.\nConcrete stuff #I will give two examples to picture how wide this area is and how many thing should be considered before implementing enterprise search in an organization.\nDocuments/media/software library. By this I mean all the kinds of systems designed for delivering digital version of user manuals, product documentation or any other form of written/recorded/drawn information or software packages in form of downloadable files. It\u0026rsquo;s almost always decorated with rich meta-data like description, author, publication date, id, revision, languages and many other. Good search is essential not only in its full text capabilities but also for filtering and browsing. No matter if the audience is external customer or employee (or both) the chance is quite big that it will require security trimming so user would find only the results he is authorized to see. In many cases enterprise search will also help building a fast navigation (faceted search) since many organizations describe their information in some kind of hierarchical classification system.\nIn case when the attachments contain text it\u0026rsquo;s findability is usually as important as for meta-data which brings the requirement for efficient parsing and indexing it also.\nIntranet portal. They are more popular in larger organizations than the small ones therefore the number of multiple data sources which are required to be searchable is usually big and it\u0026rsquo;s growing over the time. Most of the departments, business units or services want to be visible in this central point and commonly the search is the most important of its functionalities. Resources could be: intranet pages from multiple heterogeneous systems, classic CMSs, enterprise content management systems, master data systems, CRMs or even mail boxes and employees calendars (for personalized search).\nIn addition to what I mentioned describing libraries the biggest challenge here is to merge results from all those sources in a way that make sense for the user. This includes not only technical aspects but also careful user experience (UX) design and testing. It\u0026rsquo;s not easy to present such results as web pages, images, definitions, employee or customer data etc. together on a single page in a usable way.\nOther problem to solve is how to get the required data from sources and keep them up to date so the search results are not stale when they finally reach the user. Sometimes it\u0026rsquo;s not possible or reasonable to fetch all the data from source systems or those systems prefer other kind of integration with their internal search systems. In this case it could be beneficial to use federated search approach instead.\nI know that those examples definitely does not exhaust the list of possible applications but they rises some topics that I would like to continue in next posts. They are also cases from my professional life so they are not just some abstract concept.\nWrapping up #I hope this post gave you some overview of the subject of enterprise search and maybe even some inspiration. Next one will be more technical - I will introduce more architectural concepts and describe the base building blocks of enterprise search systems.\nCheers!\nMarek\n","date":"23 June 2016","permalink":"/blog/enterprise-search-basic-concepts-part-1/","section":"Posts","summary":"\u003cp\u003eRecently leafing through my notebook (yeah, I\u0026rsquo;m still using old-time paper) I found some ideas, quick-notes, self-brainstorming diagrams related with search systems. An original idea for this blog, at least for few first posts, was to present them. Still this requires introducing some of the basics of enterprise search and search systems in general (so I could spare some time in future just referring to those). This is what this post series is about.\u003c/p\u003e","title":"Enterprise search - the first look"},{"content":"","date":null,"permalink":"/categories/search/","section":"Categories","summary":"","title":"Search"},{"content":"*nix-like OSs are gaining much popularity even in .NET world since Microsoft decided to move on with their products into this direction (you can read more on Hanselman\u0026rsquo;s blog). Nevertheless it\u0026rsquo;s still more convenient to develop .NET apps on Windows (especially if it\u0026rsquo;s your target platform).\nIf you want to play with Jekyll on Windows I recommend you a great step-by-step manual written by Julian Thilo. It covers many pitfalls that are patiently waiting for any Windows user that never had anything to do with Ruby framework.\nUnfortunately there is still one more pitfall for Visual Studio (2015) users who, like me, would like to edit their posts directly on their IDE of choice. Jekyll expects all the input files (files to be transformed by Liquid) in UTF-8 without BOM (byte order mark). If you have any HTML or Markdown input file in your site folder it will be just copied as is to the target directory (_site by default) without any error or warning.\nAnd here Visual Studio has one very annoying behavior which is adding BOM by default to all UTF-8 encoded files on save. You can change this for specific file by using File \u0026gt; Advanced save options... but doing it for every single new file is even more irritating than manual conversion in other editor (like Notepad++). If it\u0026rsquo;s your use case I suggest installing one very useful VS plug-in - Fix File Encoding by Sergey Vlasov. Just don\u0026rsquo;t forget setting-up the proper file extensions in Tools \u0026gt; Options... \u0026gt; Fix File Encoding \u0026gt; General for both HTML and Markdown files:\n\\.(htm|html|md|markdown)$ Another handy plug-in is Markdown Mode which provides syntax highlighting and HTML preview for Markdown files. There is also a good spell checking add-on for VS - Visual Studio Spell Checker - which really does the job.\nCheers and happy writing! Marek\n","date":"8 May 2016","permalink":"/blog/editing-jekyll-md-in-vs2015/","section":"Posts","summary":"\u003cp\u003e*nix-like OSs are gaining much popularity even in .NET world since Microsoft decided to move on with their products into this direction (you can read more on \u003ca href=\"http://www.hanselman.com/blog/DevelopersCanRunBashShellAndUsermodeUbuntuLinuxBinariesOnWindows10.aspx\" target=\"_blank\" rel=\"noreferrer\"\u003eHanselman\u0026rsquo;s\u003c/a\u003e \u003ca href=\"http://www.hanselman.com/blog/AnUpdateOnASPNETCore10RC2.aspx\" target=\"_blank\" rel=\"noreferrer\"\u003eblog\u003c/a\u003e). Nevertheless it\u0026rsquo;s still more convenient to develop .NET apps on Windows (especially if it\u0026rsquo;s your target platform).\u003c/p\u003e","title":"Editing Jekyll markdown pages in Visual Studio 2015"},{"content":"","date":null,"permalink":"/tags/jekyll/","section":"Tags","summary":"","title":"Jekyll"},{"content":"","date":null,"permalink":"/tags/visual-studio/","section":"Tags","summary":"","title":"Visual Studio"},{"content":"So, here it is - my second technical blog. The first one died along with my frustration on SharePoint 2010 I had to work with some time ago\u0026hellip; The resignation burned out and so my willingness to describe my miserable experiences with this hopeless (at least from dev perspective) technology. It was not the best reason for sharing anything with the world.\nWhy I decided to start new one from the scratch? First of all from some time I\u0026rsquo;m doing the things I really enjoy. That\u0026rsquo;s a proper reason for writing a blog. After a long time of working in operations and release management in a more or less waterfall-style project I finally got back to development. Most of those things are related with .NET, applications security and enterprise search. I have a lot of notes on those topics both mental and written in some chaotic way.\nThe second may sound trivial but it\u0026rsquo;s really important part of my life nowadays, not only in development - sharing is caring. What\u0026rsquo;s the reason of learning and discovering exciting things if I don\u0026rsquo;t share them with others? How can I judge the value of my work if there is no one to criticize it (in constructive way)? When I share I learn faster and, if I stay open-minded, I have a unique chance change my perception. Like said - maybe trivia but still very important for me. I find a life story of a scientist Henry Cavendish especially enlightening speaking of those arguments.\nThird reason is that I simply enjoy playing with new toys. In this case it\u0026rsquo;s a static pages generator tool - Jekkyl. Since I\u0026rsquo;m also a relatively new user of Git using GitHub Pages might be a good exercise in using this VCS.\nI really hope you enjoy the reading of next posts but if you don\u0026rsquo;t I will probably keep writing.\nCheers! Marek\nUpdate: In April 2017 I moved all the content from Blogspot here (just for sake of consistency ;-) )\n","date":"7 May 2016","permalink":"/blog/hello-world/","section":"Posts","summary":"\u003cp\u003eSo, here it is - my second technical blog. The \u003ca href=\"http://byteloom.blogspot.com/\" target=\"_blank\" rel=\"noreferrer\"\u003efirst one\u003c/a\u003e died along with my frustration on SharePoint 2010 I had to work with some time ago\u0026hellip; The resignation burned out and so my willingness to describe my miserable experiences with this hopeless (at least from dev perspective) technology. It was not the best reason for sharing anything with the world.\u003c/p\u003e","title":"Hello World! Again..."},{"content":"","date":null,"permalink":"/tags/sharepoint-2010/","section":"Tags","summary":"","title":"Sharepoint 2010"},{"content":"Once again about the SharePoint 2010 taxonomy service.\nAs I wrote in my previous posts, loading data into MMD service automatically can be quite a challenge. First, you must remember about illegal characters in terms labels. Second, you must trace duplicates across sibling nodes in taxonomy trees. And this could not be the end of your problems especially if you plan to load some more data at one time.\nSince MMD service is transactional you must first perform the commit operation in order to see your changes in MMD picker. From the programmatic point of view MMD service is a classic WCF service with http/https endpoints and client proxy which is used in daily SharePoint development.\nWhen you try to perform some more complex set of operations (like adding or removing large number of terms and term sets) in one transaction you may come across the following error:\nAn error occurred while receiving the HTTP response to http://hostname:32843/3d1e97ecf7074067957b8efdac3ae1ab/MetadataWebService.svc. This could be due to the service endpoint binding not using the HTTP protocol. This could also be due to an HTTP request context being aborted by the server (possibly due to the service shutting down). See server logs for more details. Here is a full error description:\nSystem.ServiceModel.CommunicationException was caught Message=An error occurred while receiving the HTTP response to http://hostname:32843/3d1e97ecf7074067957b8efdac3ae1ab/MetadataWebService.svc. This could be due to the service endpoint binding not using the HTTP protocol. This could also be due to an HTTP request context being aborted by the server (possibly due to the service shutting down). See server logs for more details. Source=mscorlib StackTrace: Server stack trace: at System.ServiceModel.Channels.HttpChannelUtilities.ProcessGetResponseWebException(WebException webException, HttpWebRequest request, HttpAbortReason abortReason) at System.ServiceModel.Channels.HttpChannelFactory.HttpRequestChannel.HttpChannelRequest.WaitForReply(TimeSpan timeout) at System.ServiceModel.Channels.RequestChannel.Request(Message message, TimeSpan timeout) at System.ServiceModel.Channels.SecurityChannelFactory`1.SecurityRequestChannel.Request(Message message, TimeSpan timeout) at System.ServiceModel.Dispatcher.RequestChannelBinder.Request(Message message, TimeSpan timeout) at System.ServiceModel.Channels.ServiceChannel.Call(String action, Boolean oneway, ProxyOperationRuntime operation, Object[] ins, Object[] outs, TimeSpan timeout) at System.ServiceModel.Channels.ServiceChannelProxy.InvokeService(IMethodCallMessage methodCall, ProxyOperationRuntime operation) at System.ServiceModel.Channels.ServiceChannelProxy.Invoke(IMessage message) Exception rethrown at [0]: at System.Runtime.Remoting.Proxies.RealProxy.HandleReturnMessage(IMessage reqMsg, IMessage retMsg) at System.Runtime.Remoting.Proxies.RealProxy.PrivateInvoke(MessageData\u0026amp; msgData, Int32 type) at Microsoft.SharePoint.Taxonomy.Internal.IDataAccessReadWrite.Write(String data) at Microsoft.SharePoint.Taxonomy.Internal.TaxonomyProxyAccess.\u0026lt;\u0026gt;c__DisplayClass40.\u0026lt;write\u0026gt;b__3f(IMetadataWebServiceApplication serviceApplication) at Microsoft.SharePoint.Taxonomy.MetadataWebServiceApplicationProxy.\u0026lt;\u0026gt;c__DisplayClass2c.\u0026lt;runonchannel\u0026gt;b__2b() at Microsoft.Office.Server.Security.SecurityContext.RunAsProcess(CodeToRunElevated secureCode) at Microsoft.SharePoint.Taxonomy.MetadataWebServiceApplicationProxy.\u0026lt;\u0026gt;c__DisplayClass2c.\u0026lt;runonchannel\u0026gt;b__2a() at Microsoft.Office.Server.Utilities.MonitoredScopeWrapper.RunWithMonitoredScope(Action code) at Microsoft.SharePoint.Taxonomy.MetadataWebServiceApplicationProxy.RunOnChannel(CodeToRun codeToRun, Double operationTimeoutFactor) at Microsoft.SharePoint.Taxonomy.MetadataWebServiceApplicationProxy.RunOnChannel(CodeToRun codeToRun) at Microsoft.SharePoint.Taxonomy.Internal.TaxonomyProxyAccess.Write(String data) at Microsoft.SharePoint.Taxonomy.Internal.DataAccessManager.Write(String data) at Microsoft.SharePoint.Taxonomy.Internal.Sandbox.CommitSandbox() at Microsoft.SharePoint.Taxonomy.TermStore.CommitAll() at ... InnerException: System.Net.WebException Message=The underlying connection was closed: An unexpected error occurred on a receive. Source=System StackTrace: at System.Net.HttpWebRequest.GetResponse() at System.ServiceModel.Channels.HttpChannelFactory.HttpRequestChannel.HttpChannelRequest.WaitForReply(TimeSpan timeout) InnerException: System.IO.IOException Message=Unable to read data from the transport connection: An existing connection was forcibly closed by the remote host. Source=System StackTrace: at System.Net.Sockets.NetworkStream.Read(Byte[] buffer, Int32 offset, Int32 size) at System.Net.PooledStream.Read(Byte[] buffer, Int32 offset, Int32 size) at System.Net.Connection.SyncRead(HttpWebRequest request, Boolean userRetrievedStream, Boolean probeRead) InnerException: System.Net.Sockets.SocketException Message=An existing connection was forcibly closed by the remote host Source=System ErrorCode=10054 NativeErrorCode=10054 StackTrace: at System.Net.Sockets.NetworkStream.Read(Byte[] buffer, Int32 offset, Int32 size) InnerException: The bottom exception of the stack trace - System.Net.WebException (The underlying connection was closed: An unexpected error occurred on a receive) - suggests that problem related with web application/IIS configuration rather than MMS service itself. Andreas Cieslik already found the solution for this and posted on his blog. In short - you must increase the default request restrictions in services web.config file.\nWhen changed this the System.ServiceModel.CommunicationException disappeared but I\u0026rsquo;ve got another error:\nSystem.TimeoutException was caught Message=The request channel timed out while waiting for a reply after 00:00:29.9570296. Increase the timeout value passed to the call to Request or increase the SendTimeout value on the Binding. The time allotted to this operation may have been a portion of a longer timeout. and the full error description:\nSystem.TimeoutException was caught Message=The request channel timed out while waiting for a reply after 00:00:29.9570296\\. Increase the timeout value passed to the call to Request or increase the SendTimeout value on the Binding. The time allotted to this operation may have been a portion of a longer timeout. Source=mscorlib StackTrace: Server stack trace: at System.ServiceModel.Channels.RequestChannel.Request(Message message, TimeSpan timeout) at System.ServiceModel.Channels.SecurityChannelFactory`1.SecurityRequestChannel.Request(Message message, TimeSpan timeout) at System.ServiceModel.Dispatcher.RequestChannelBinder.Request(Message message, TimeSpan timeout) at System.ServiceModel.Channels.ServiceChannel.Call(String action, Boolean oneway, ProxyOperationRuntime operation, Object[] ins, Object[] outs, TimeSpan timeout) at System.ServiceModel.Channels.ServiceChannelProxy.InvokeService(IMethodCallMessage methodCall, ProxyOperationRuntime operation) at System.ServiceModel.Channels.ServiceChannelProxy.Invoke(IMessage message) Exception rethrown at [0]: at System.Runtime.Remoting.Proxies.RealProxy.HandleReturnMessage(IMessage reqMsg, IMessage retMsg) at System.Runtime.Remoting.Proxies.RealProxy.PrivateInvoke(MessageData\u0026amp; msgData, Int32 type) at Microsoft.SharePoint.Taxonomy.Internal.IDataAccessReadWrite.Write(String data) at Microsoft.SharePoint.Taxonomy.Internal.TaxonomyProxyAccess.\u0026lt;\u0026gt;c__DisplayClass40.\u0026lt;write\u0026gt;b__3f(IMetadataWebServiceApplication serviceApplication) at Microsoft.SharePoint.Taxonomy.MetadataWebServiceApplicationProxy.\u0026lt;\u0026gt;c__DisplayClass2c.\u0026lt;runonchannel\u0026gt;b__2b() at Microsoft.Office.Server.Security.SecurityContext.RunAsProcess(CodeToRunElevated secureCode) at Microsoft.SharePoint.Taxonomy.MetadataWebServiceApplicationProxy.\u0026lt;\u0026gt;c__DisplayClass2c.\u0026lt;runonchannel\u0026gt;b__2a() at Microsoft.Office.Server.Utilities.MonitoredScopeWrapper.RunWithMonitoredScope(Action code) at Microsoft.SharePoint.Taxonomy.MetadataWebServiceApplicationProxy.RunOnChannel(CodeToRun codeToRun, Double operationTimeoutFactor) at Microsoft.SharePoint.Taxonomy.MetadataWebServiceApplicationProxy.RunOnChannel(CodeToRun codeToRun) at Microsoft.SharePoint.Taxonomy.Internal.TaxonomyProxyAccess.Write(String data) at Microsoft.SharePoint.Taxonomy.Internal.DataAccessManager.Write(String data) at Microsoft.SharePoint.Taxonomy.Internal.Sandbox.CommitSandbox() at Microsoft.SharePoint.Taxonomy.TermStore.CommitAll() at ABB.Library.Publishing.SharePoint.CategoryImport.ClassificationProvider.CategoryTermStoreAdapter.Commit() in C:\\work\\Dev\\07SharePoint\\01Impl\\ABB.Library.Publishing.SharePoint\\ABB.Library.Publishing.SharePoint.CategoryImport\\ClassificationProvider\\CategoryTermStoreAdapter.cs:line 152 at ABB.Library.Publishing.SharePoint.CategoryImport.ImportProcess.Import() in C:\\work\\Dev\\07SharePoint\\01Impl\\ABB.Library.Publishing.SharePoint\\ABB.Library.Publishing.SharePoint.CategoryImport\\ImportProcess.cs:line 49 InnerException: System.TimeoutException Message=The HTTP request to \u0026#39;http://hostname:32843/3d1e97ecf7074067957b8efdac3ae1ab/MetadataWebService.svc\u0026#39; has exceeded the allotted timeout of 00:00:30\\. The time allotted to this operation may have been a portion of a longer timeout. Source=System.ServiceModel StackTrace: at System.ServiceModel.Channels.HttpChannelUtilities.ProcessGetResponseWebException(WebException webException, HttpWebRequest request, HttpAbortReason abortReason) at System.ServiceModel.Channels.HttpChannelFactory.HttpRequestChannel.HttpChannelRequest.WaitForReply(TimeSpan timeout) at System.ServiceModel.Channels.RequestChannel.Request(Message message, TimeSpan timeout) InnerException: System.Net.WebException Message=The operation has timed out Source=System StackTrace: at System.Net.HttpWebRequest.GetResponse() at System.ServiceModel.Channels.HttpChannelFactory.HttpRequestChannel.HttpChannelRequest.WaitForReply(TimeSpan timeout) InnerException: ... sendTimeout is a strictly WCF thing, but unfortunately SharePoint taxonomy server side API does not provide any method for passing client side timeouts. You can do this only through %SharePointRoot%\\\\WebClients\\\\Metadata\\\\client.config file:\n\u0026lt;?xml version=\u0026#34;1.0\u0026#34; encoding=\u0026#34;utf-8\u0026#34; ?\u0026gt; \u0026lt;configuration\u0026gt; \u0026lt;system.serviceModel\u0026gt; \u0026lt;client\u0026gt; \u0026lt;endpoint name=\u0026#34;http\u0026#34; contract=\u0026#34;Microsoft.SharePoint.Taxonomy.IMetadataWebServiceApplication\u0026#34; binding=\u0026#34;customBinding\u0026#34; bindingConfiguration=\u0026#34;MetadataWebServiceHttpBinding\u0026#34; /\u0026gt; \u0026lt;endpoint name=\u0026#34;https\u0026#34; contract=\u0026#34;Microsoft.SharePoint.Taxonomy.IMetadataWebServiceApplication\u0026#34; binding=\u0026#34;customBinding\u0026#34; bindingConfiguration=\u0026#34;MetadataWebServiceHttpsBinding\u0026#34; /\u0026gt; \u0026lt;/client\u0026gt; \u0026lt;bindings\u0026gt; \u0026lt;customBinding\u0026gt; \u0026lt;binding name=\u0026#34;MetadataWebServiceHttpBinding\u0026#34; receiveTimeout=\u0026#34;00:00:30\u0026#34; sendTimeout=\u0026#34;00:00:30\u0026#34; openTimeout=\u0026#34;00:00:30\u0026#34; closeTimeout=\u0026#34;00:00:30\u0026#34;\u0026gt; \u0026lt;security authenticationMode=\u0026#34;IssuedTokenOverTransport\u0026#34; allowInsecureTransport=\u0026#34;true\u0026#34; /\u0026gt; \u0026lt;binaryMessageEncoding\u0026gt; \u0026lt;readerQuotas maxStringContentLength=\u0026#34;2147483647\u0026#34; maxArrayLength=\u0026#34;2147483647\u0026#34; maxBytesPerRead=\u0026#34;2147483647\u0026#34; /\u0026gt; \u0026lt;/binaryMessageEncoding\u0026gt; \u0026lt;httpTransport transferMode=\u0026#34;StreamedResponse\u0026#34; maxReceivedMessageSize=\u0026#34;2147483647\u0026#34; authenticationScheme=\u0026#34;Anonymous\u0026#34; useDefaultWebProxy=\u0026#34;false\u0026#34; /\u0026gt; \u0026lt;/binding\u0026gt; \u0026lt;binding name=\u0026#34;MetadataWebServiceHttpsBinding\u0026#34; receiveTimeout=\u0026#34;00:00:30\u0026#34; sendTimeout=\u0026#34;00:00:30\u0026#34; openTimeout=\u0026#34;00:00:30\u0026#34; closeTimeout=\u0026#34;00:00:30\u0026#34;\u0026gt; \u0026lt;security authenticationMode=\u0026#34;IssuedTokenOverTransport\u0026#34; /\u0026gt; \u0026lt;binaryMessageEncoding\u0026gt; \u0026lt;readerQuotas maxStringContentLength=\u0026#34;2147483647\u0026#34; maxArrayLength=\u0026#34;2147483647\u0026#34; maxBytesPerRead=\u0026#34;2147483647\u0026#34; /\u0026gt; \u0026lt;/binaryMessageEncoding\u0026gt; \u0026lt;httpsTransport transferMode=\u0026#34;StreamedResponse\u0026#34; maxReceivedMessageSize=\u0026#34;2147483647\u0026#34; authenticationScheme=\u0026#34;Anonymous\u0026#34; useDefaultWebProxy=\u0026#34;false\u0026#34; /\u0026gt; \u0026lt;/binding\u0026gt; \u0026lt;/customBinding\u0026gt; \u0026lt;/bindings\u0026gt; \u0026lt;/system.serviceModel\u0026gt; \u0026lt;system.net\u0026gt; \u0026lt;connectionManagement\u0026gt; \u0026lt;add address=\u0026#34;*\u0026#34; maxconnection=\u0026#34;10000\u0026#34; /\u0026gt; \u0026lt;/connectionManagement\u0026gt; \u0026lt;/system.net\u0026gt; \u0026lt;/configuration\u0026gt; The values that must be increased are sendTimeout for proper bindings (line 21 for http and 42 for https). In my case it must have been changed to 10 minutes.\nPlease note that both settings - maximum request length for all SP web services and WCF settings for MMD service - are server wide and will affect the whole machine (or farm in load balanced environment, since you will probably have to provide the same configuration on all web service machines). This may not be an acceptable solution on many production configurations especially in public networks (due to possible DoS threat) - I\u0026rsquo;m not sure how this affects other client APIs (like JS). This is rather a workaround.\nHappy SharePointing!\nA notice for archeologists...🏺 This post was originaly published on my previous blog and moved here. Some links and resources might not be up to date. ","date":"2 October 2012","permalink":"/blog/sharepoint-taxonomies-commiting-large/","section":"Posts","summary":"\u003cp\u003eOnce again about the SharePoint 2010 taxonomy service.\u003c/p\u003e\n\u003cp\u003eAs I wrote in my previous posts, loading data into MMD service automatically can be quite a challenge. First, you must \u003ca href=\"/blog/sharepoint-taxonomies-labels-with\"\u003eremember about illegal characters in terms labels\u003c/a\u003e. Second, you must \u003ca href=\"/blog/sharepoint-taxonomies-struggling-with\"\u003etrace duplicates across sibling nodes\u003c/a\u003e in taxonomy trees. And this could not be the end of your problems especially if you plan to load some more data at one time.\u003c/p\u003e","title":"SharePoint Taxonomies - Committing large amount of data to the MMD service"},{"content":"If you have ever had to load large number of data into SharePoint MMD service or build taxonomies automatically you have likely encountered the following problem:\nMicrosoft.SharePoint.Taxonomy.TermStoreOperationException: There is already a term with the same default label and parent term. at Microsoft.SharePoint.Taxonomy.MetadataWebServiceApplicationProxy.\u0026lt;\u0026gt;c__DisplayClass2c.b__2b() at Microsoft.Office.Server.Security.SecurityContext.RunAsProcess(CodeToRunElevated secureCode) at Microsoft.SharePoint.Taxonomy.MetadataWebServiceApplicationProxy.\u0026lt;\u0026gt;c__DisplayClass2c.b__2a() at Microsoft.Office.Server.Utilities.MonitoredScopeWrapper.RunWithMonitoredScope(Action code) at Microsoft.SharePoint.Taxonomy.MetadataWebServiceApplicationProxy.RunOnChannel(CodeToRun codeToRun, Double operationTimeoutFactor) at Microsoft.SharePoint.Taxonomy.MetadataWebServiceApplicationProxy.RunOnChannel(CodeToRun codeToRun) at Microsoft.SharePoint.Taxonomy.Internal.TaxonomyProxyAccess.Write(String data) at Microsoft.SharePoint.Taxonomy.Internal.DataAccessManager.Write(String data) at Microsoft.SharePoint.Taxonomy.Internal.Sandbox.CommitSandbox() at Microsoft.SharePoint.Taxonomy.TermStore.CommitAll() Not very informative stack trace as you see\u0026hellip; First, this happens during the commit, not on creation of new term. Second, there is no information about the conflicting labels. (This is because the validation is done in SQL after building the whole changed structure, which you can check in ULS logs). This error could be quite frustrating, which I\u0026rsquo;ve experienced myself (I lost three days on this when I was implementing full synchronization of large tree structure with SP taxonomies). I will try to give some hints all of those which are struggling with duplicated labels.\nRemember about default label conversions #As I wrote in my previous post - \u0026ldquo;SharePoint Taxonomies - Labels with forbidden characters\u0026rdquo;, SharePoint performs some transformations on label strings: trim, replacing consecutive spaces into one and ampersand with its wider Unicode equivalent (\\uFF06).\nThe result of above is that label \u0026quot; black duck \u0026amp; goose \u0026quot; and \u0026quot;black duck ＆ goose\u0026quot; will be treated as the same label string.\nIf you decide to replace the characters forbidden in labels (;\u0026quot;\u0026lt;\u0026gt;|\u0026amp;tab) with some others you will have to keep in mind these transformation to.\nLabels are compared case invariant #This means that label \u0026quot;BlAcK DuCK\u0026quot; will be treated as the same string as label \u0026quot;black duck\u0026quot;.\nLabel changed after creation before commit is not really changed\u0026hellip; #This was my case (3 days of digging\u0026hellip;). Consider the following code:\n//parent (TermSetItem) and termStore is defined before this snippet //... var term1Label = \u0026#34;black duck\u0026#34;; var term1 = parent.CreateTerm(term1Label, 1033); var term2Label = \u0026#34;black duck\u0026#34;; var term2 = parent.CreateTerm(term2Label, 1033); term2.Name = \u0026#34;white goose\u0026#34;; //... termStore.CommitAll(); If you think this code will run without any problem you are wrong. As I did.\nYou must ensure the label uniqueness between sibling nodes in the time when they are created. Of course this fact is not documented in API on MSDN\u0026hellip;\nAs some used to say - there is a thin line between bug and feature ;-)\nHow to hunt the vampire down? #The last tip I can give you is a little piece of code you can run just before the commit in order to find the problematic labels:\n//this will give you all terms in the group (you can narrow the scope, i.e. to particular term set, if you like var terms = termGroup.TermSets.SelectMany(termSet =\u0026gt; termSet.GetAllTerms()); //this will actually return the terms with duplicated labels (for default LCID) grouped by parent-label pair var duplicates = terms. GroupBy(term =\u0026gt; string.Format(\u0026#34;{0}:{1}\u0026#34;, getParentGuid(term), term.Name.ToLowerInvariant())). Where(group =\u0026gt; group.Count() \u0026gt; 1). SelectMany(group =\u0026gt; group). ToList(); //now you can do with them whatever you like, i.e. write them out to the file: var termsStr = new StringBuilder(); termStr.Append(\u0026#34;Term GUID;Term default label;Parent GUID\u0026#34;); foreach (var term in duplicates) { termsStr.AppendFormat(\u0026#34;{0};\\\u0026#34;{1}\\\u0026#34;;{2}\\n\u0026#34;, term.Id, term.Name, getParentGuid(term)); } File.AppendAllText(@\u0026#34;c:\\duplicatedTerms.txt\u0026#34;, termsStr.ToString(), new UTF32Encoding()); string getParentGuid(Term term) { return term.Parent != null ? term.Parent.Id.ToString() : term.TermSet.Id.ToString(); } Unfortunately this won\u0026rsquo;t help if you change the label after creation like mentioned above.\nI hope this little guideline will save you some time.\nHappy SharePointing!\nA notice for archeologists...🏺 This post was originaly published on my previous blog and moved here. Some links and resources might not be up to date. ","date":"28 September 2012","permalink":"/blog/sharepoint-taxonomies-struggling-with/","section":"Posts","summary":"\u003cp\u003eIf you have ever had to load large number of data into SharePoint MMD service or build taxonomies automatically you have likely encountered the following problem:\u003c/p\u003e","title":"SharePoint Taxonomies - Struggling with duplicated default labels"},{"content":"Recently, when I was working on mechanism of automatic synchronization of tree structures provided by web service to SharePoint taxonomies, I came across an error like this:\nThe value \u0026#39;(\u0026lt;= 0320-775, 0550-5/7)\u0026#39; is invalid. It probably contains invalid characters or is too long. Parameter name: name with the following stacktrace fragment:\nat Microsoft.SharePoint.Taxonomy.Internal.CommonValidator.ValidateName(String name, String parameterName) at Microsoft.SharePoint.Taxonomy.TermSetItem.CreateTerm(String name, Int32 lcid, Guid newTermId) ... Looking for some help in SharePoint API documentation for method TermSetItem.CreateTerm() I found the explanation for this error:\nThe labelName value will be normailized to trim consecutive spaces into one and replace the \u0026amp; character with the wide character version of the character (\\uFF06). It must be non-empty, cannot exceed 255 characters, and cannot contain any of the following characters ;\u0026quot;\u0026lt; \u0026gt;|\u0026amp;tab\nYeah, this is SharePoint\u0026hellip; No surprise, I would say ;)\nSince I had to put the forbidden characters into the term labels somehow, my first thought was to encode them as HTML entities. But as you may noticed it is impossible - the ampersand character is replaced by similar looking unicode character with code \\uFF06.\nMy second try - why not to try the same as MS SharePoint developers? You will find many UTF-16 characters that look almost exactly the same as the characters from ASCII set. The replacements for the forbidden characters in UTF-8 are:\nOriginal ASCII char UTF-8 replacement code UTF-8 char \\ \\uFF5C ｜ \u0026quot; \\uFF02 ＂ \u0026lt; \\uFF1C ＜ \u0026gt; \\uFF1E ＞ Unfortunetly I found no replacement for semicolon character so I\u0026rsquo;m replacing it with coma. You can find more information about these characters on FileFormat.\nOf course you will have to create the term grammatically to put them into term label (or copy them from some unicode table, such as FileFormat). Here is a utility method that should \u0026ldquo;normalize\u0026rdquo; the strings for CreateTerm() method:\npublic static string ReplaceIllegalCharacters(string termLabel) { return termLabel. Replace(\u0026#34;\\t\u0026#34;, \u0026#34; \u0026#34;). Replace(\u0026#34;;\u0026#34;, \u0026#34;,\u0026#34;). Replace(\u0026#34;\\\u0026#34;\u0026#34;, \u0026#34;\\uFF02\u0026#34;). Replace(\u0026#34;\u0026lt;\u0026#34;, \u0026#34;\\uFF1C\u0026#34;). Replace(\u0026#34;\u0026gt;\u0026#34;, \u0026#34;\\uFF1E\u0026#34;). Replace(\u0026#34;|\u0026#34;, \u0026#34;\\uFF5C\u0026#34;); } This works fine either in web interface and other clients (I\u0026rsquo;ve tested it with MS Word 2010 and Lotus Notes widget for SharePoint integration - Harmon.IE). I will put here some screenshots soon.\nSharePoint web UI term picker MS Word term picker Harmon.IE term picker SharePoint TermStore management tool Happy SharePointing!\nA notice for archeologists...🏺 This post was originaly published on my previous blog and moved here. Some links and resources might not be up to date. ","date":"20 September 2012","permalink":"/blog/sharepoint-taxonomies-labels-with/","section":"Posts","summary":"\u003cp\u003eRecently, when I was working on mechanism of automatic synchronization of tree structures provided by web service to SharePoint taxonomies, I came across an error like this:\u003c/p\u003e","title":"SharePoint Taxonomies - Labels with forbiden characters"},{"content":"Writing a component for parsing XML files with XMLSerializer I had to provide DTD validation (DTD file was already created long time ago so there was no sense for creating XSD schema). The component must have been able to work in console application and web app (as a SharePoint timer job) so there was no chance to guarantee the same paths for DTD file (which was always specified in doctype directive in processed files). In such situation I\u0026rsquo;ve decided to deliver the DTD file as embedded resource in component assembly.\nIf you had ever faced the similar problem you probably know that standard XmlResolver implementations in .Net framework will not able to find DTD in such location. If you try you will probably get an exception like this:\nSystem.InvalidOperationException: There is an error in XML document (0, 0). ---\u0026gt; System.Xml.XmlException: Cannot resolve external DTD subset - public ID = \u0026#39;\u0026#39;, system ID = \u0026#39;yourDTDFileDefinedInDoctype.dtd\u0026#39;. or something similar. Thats why you have to provide the XmlSerializer some your own mechanism that will find the DTD file - custom XmlResolver.\nThere is a good example how to do this on MSDN. I\u0026rsquo;ve changed it a little bit to cover the use case with embedded resources.\npublic class XmlEmbededResolver : XmlUrlResolver { private readonly string dtdResourcePathPrefix; public XmlEmbededResolver(string dtdResourcePathPrefix) { this.dtdResourcePathPrefix = dtdResourcePathPrefix; } public override object GetEntity(Uri absoluteUri, string role, Type ofObjectToReturn) { if (absoluteUri == null) { throw new ArgumentNullException(\u0026#34;absoluteUri\u0026#34;); } if (isDocumentTypeDefinitionFile(absoluteUri) \u0026amp;\u0026amp; (ofObjectToReturn == null || ofObjectToReturn == typeof(Stream))) { var filePath = dtdResourcePathPrefix + getFileName(absoluteUri); var resourceStream = Assembly. GetExecutingAssembly(). GetManifestResourceStream(filePath); if (resourceStream == null) { throw new FileNotFoundException(\u0026#34;Embeded DTD file not found\u0026#34;, filePath); } return resourceStream; } return base.GetEntity(absoluteUri, role, ofObjectToReturn); } } Parameter dtdResourcePathPrefix in constructor points the location of DTD (without file name) in the assembly (usually the default namespace + folder path).\nThe utility method isDocumentTypeDefinitionFile() is used for determining if entity passed by XmlSerializer to resolve is actually an DTD file (XmlResolver is used resolve any kind of external resources defined in XML file):\nprivate static bool isDocumentTypeDefinitionFile(Uri absoluteUri) { return absoluteUri.IsFile \u0026amp;\u0026amp; absoluteUri.AbsolutePath.EndsWith(\u0026#34;.dtd\u0026#34;); } getFileName(), as its name suggests, returns the name of DTD:\nprivate static string getFileName(Uri absoluteUri) { return absoluteUri.Segments.Last(); } Using the custom resolver is pretty simple - you just pass it as setting to XmlReader in Create() method:\nvar readerSettings = new XmlReaderSettings { ProhibitDtd = false, ValidationType = ValidationType.DTD, XmlResolver = new XmlEmbededResolver(dtdResourcePathPrefix) }; using (var reader = XmlReader.Create(xmlFileStream, readerSettings)) { var serializer = new XmlSerializer(typeof (YourXmlDataType)); var rawImportEntry = (YourXmlDataType)serializer.Deserialize(reader); // ... } I can easily imagine other implementations of XmlResolver - for example to provide external resources from the web services (of course if someone see any sense in creating such thing ;-)\nHappy coding!\nA notice for archeologists...🏺 This post was originaly published on my previous blog and moved here. Some links and resources might not be up to date. ","date":"3 July 2012","permalink":"/blog/custom-xmlresolver-for-embeded-dtd/","section":"Posts","summary":"\u003cp\u003eWriting a component for parsing XML files with \u003ccode\u003eXMLSerializer\u003c/code\u003e I had to provide DTD validation (DTD file was already created long time ago so there was no sense for creating XSD schema). The component must have been able to work in console application and web app (as a SharePoint timer job) so there was no chance to guarantee the same paths for DTD file (which was always specified in doctype directive in processed files). In such situation I\u0026rsquo;ve decided to deliver the DTD file as embedded resource in component assembly.\u003c/p\u003e","title":"Custom XmlResolver for embeded DTD"},{"content":"","date":null,"permalink":"/tags/xml/","section":"Tags","summary":"","title":"Xml"},{"content":"For the years I have developed .Net projects I\u0026rsquo;ve found many useful solutions on CodePlex repository. Some of them are only drafts of ideas but there are also many products that are ready to use on production environments. SharePoint 2010 Batch Edit which worked with recently is one of this from the second category.\nAs the name suggests, SharePoint 2010 Batch Edit fills the gap in mass updates functionality in SharePoint 2010. In OOB SP2010 you can select one or more items on list view and perform such operations as check-in, check-out, delete an so on, but there is no tool that would allow to update items data for more than one item at once. Of course you can always write your own application page and display it in dialog box but why you should do this if someone already has done it :-)\nThe feature works quite intuitively. After installation from PowerShell (or stsadm) and activation in administration central you will see a new button in items ribbon. When you select more than one item on list the button is enabled and after click you will see a dialog box with a form that allows you update your items.\nFile selection File selection (multiple files) I\u0026rsquo;ve tested Batch Edit with some basic field types added to lists content type: text, date, user, managed meta-data (MMD) and single value lookup. I haven\u0026rsquo;t experienced any problems with updating values of those types. There are two MMD fields update modes for multi-value fields: overwrite and append, which is quite useful (you can turn on the append mode with check-box on the top of the update form). I had a problem with updating multi-value lookup field - the field control was displayed correctly in form but no changes were applied on update.\nBatch Edit One thing could be confusing at first time. When you are looking for some field that is displayed in edit form for single item it will happen sometimes that it is not available for editing in batch mode. The good example is a standard field - Title in document libraries. The reason of this behavior is that the field must marked as ShowInNewForm and ShowInEdit form (these conditions are not meet for the Title field in doc. library). When you will dig into the code (TamTam.SP2010.BatchEdit.Layouts.TamTam.SP2010.BatchEdit.BatchEdit class) you will find out that also other field types are excluded - computed, file, integer and secondary lookup (SPField.CanBeDisplayedInEditForm - will return false for them).\nIf your list contain more than one content type Batch Edit will display field controls for all your content types. The change of content type itself is however not possible.\nFor lists with versioning turned on the update will cause check-out and check-in with new minor version. If some error will occur during the update (like trying to update value for a field marked as unique) you will see a clear message on the bottom of the form.\nI have noticed also a few minor issues using the Batch Edit, mostly in usability area. The first was loosing the focus (checked check-boxes) from previously selected list items after update. This could be annoying for someone that is trying to perform more than one batch update for the same set of elements on the list. For long-time operations (mass updates can take some time) the update form seems to be unresponsive. There should be some progress indicator that will inform the user that update is being performed. Also disabling the update and cancel button in such cases would be a nice thing. The last little bug I\u0026rsquo;ve found is that Batch Edit ignores \u0026ldquo;Launch forms in dialog\u0026rdquo; option set false (it\u0026rsquo;s a setting that you can change for the list). The update form is also displayed in dialog box.\nPS: If you faced with the problem of updating more than 100 items at once, read my previous post - \u0026ldquo;You cannot select more than 100 items at once\u0026rdquo; in SharePoint 2010.\nHappy SharePointing!\nA notice for archeologists...🏺 This post was originaly published on my previous blog and moved here. Some links and resources might not be up to date. ","date":"6 April 2012","permalink":"/blog/sharepoint-2010-batch-edit/","section":"Posts","summary":"\u003cp\u003eFor the years I have developed .Net projects I\u0026rsquo;ve found many useful solutions on \u003ca href=\"http://www.codeplex.com/\" target=\"_blank\" rel=\"noreferrer\"\u003eCodePlex\u003c/a\u003e repository. Some of them are only drafts of ideas but there are also many products that are ready to use on production environments. \u003ca href=\"http://sp2010batchedit.codeplex.com/\" target=\"_blank\" rel=\"noreferrer\"\u003eSharePoint 2010 Batch Edit\u003c/a\u003e which worked with recently is one of this from the second category.\u003c/p\u003e","title":"SharePoint 2010 Batch Edit"},{"content":"Looking for some solution for batch items update I\u0026rsquo;ve run into the following problem: when I clicked a checkbox in top right corner of the list (actually a document library) \u0026ldquo;All Items\u0026rdquo; view, the browser displayed an error: You cannot select more than 100 items at once.\nItems selection limit - error message The problem was discussed on the TechNet forum (Selecting more than 100 files in Document Library) and the limit of maximum 100 items per one batch update (like check-in, delete etc.) is described on MSDN:\nLimit: Bulk operations\nMaximum value: 100 items per bulk operation\nLimit type: Boundary\nNotes: The user interface allows a maximum of 100 items to be selected for bulk operations.\nWhat is interesting and what was not mentioned in TechNet topic, the limit is controlled on client side by a script, what I will show. What happens when you click on select-all-items check box? This is a pseudo-callstack of JS calls:\n\u0026lt;input type=\u0026#34;checkbox\u0026#34; title=\u0026#34;Select or deselect all items\u0026#34; onclick=\u0026#34;ToggleAllItems(event,this,21)\u0026#34; onfocus=\u0026#34;EnsureSelectionHandlerOnFocus(event,this,21)\u0026#34; /\u0026gt; onClick (the view HTML) -\u0026gt; ToggleAllItems(evt, cbx, ctxNum) (init.js) -\u0026gt; CoreInvoke(fn) (init.js) -\u0026gt; _ToggleAllItems(evt, cbx, ctxNum) (core.js) -\u0026gt; ToggleAllItems2(cbx, ctxNum, f) (core.js) -\u0026gt; In the last function - ToggleAllItems2 - you will find the following pieces of code:\nvar totalItems=CountTotalItems(ctxCur); // ... if (totalItems \u0026gt; g_MaximumSelectedItemsAllowed) { cbx.checked=false; alert(L_BulkSelection_TooManyItems); return; } Variables g_MaximumSelectedItemsAllowed and L_BulkSelection_TooManyItems are defined in the same file (core.js):\nvar g_MaximumSelectedItemsAllowed=100; var L_BulkSelection_TooManyItems=\u0026#34;You cannot select more than 100 items at once.\u0026#34;; You will find a similar code checking if the limits are also preserved when user checks items one by one.\nThe second interesting thing is that the number of items is checked only on client side, so if you change g_MaximumSelectedItemsAllowed value to, let say 200, you will be able to make a batch update for more items than the official limit allows.\nI\u0026rsquo;ve managed to change this by IE developers toolbar for 200 items:\nMaximum selected items limit overwrite Of course if you want to change this permanently (probably with changing the message string L_BulkSelection_TooManyItems also to not confuse users) you still will have to develop some more persistent solution - i.e. deploy your custom script into Layouts folder, link it on selected pages and so on. The code would look like this (I assume you have jQuery included):\n$(document).ready(function() { g_MaximumSelectedItemsAllowed = 200; L_BulkSelection_TooManyItems = \u0026#34;You cannot select more than \u0026#39; + g_MaximumSelectedItemsAllowed + \u0026#39; items at once.\u0026#34;; }); There is one other issue I should mention. Limit of 100 items has been set for some reason - probably the performance. I\u0026rsquo;ve managed to update value of a column for 200 items with BatchEdit from codeplex but checking-in the same number of items failed (I\u0026rsquo;ve got some strange JS error). Still I was able to check-in 110 items which was above the standard limit.\nHappy SharePointing!\nA notice for archeologists...🏺 This post was originaly published on my previous blog and moved here. Some links and resources might not be up to date. ","date":"27 March 2012","permalink":"/blog/you-cannot-select-more-than-100-items/","section":"Posts","summary":"\u003cp\u003eLooking for some solution for batch items update I\u0026rsquo;ve run into the following problem: when I clicked a checkbox in top right corner of the list (actually a document library) \u0026ldquo;All Items\u0026rdquo; view, the browser displayed an error: \u003ccode\u003eYou cannot select more than 100 items at once.\u003c/code\u003e\u003c/p\u003e","title":"\"You cannot select more than 100 items at once\" in SharePoint 2010"},{"content":"","date":null,"permalink":"/tags/debugging/","section":"Tags","summary":"","title":"Debugging"},{"content":"","date":null,"permalink":"/tags/performance/","section":"Tags","summary":"","title":"Performance"},{"content":"","date":null,"permalink":"/tags/web/","section":"Tags","summary":"","title":"Web"},{"content":"There are many popular tools for web performance profiling/debugging these days. From my personal tool set I could mention Firebug, IE Developers Toolbar (integrated with IE from version \u0026gt; 8) or Fiddler. The common problem with those tools is that they are installed and run locally on developers machine. Sometimes, for example when behaviour of the web app depends on loading all elements in the specific order or time, it is important to determine if those factors vary in different geographical locations. The download speed for China and Germany could be very different. If you face with this kind of problems I could recommend you a great distributed web performance profiler - WebPageTest.\nWith this tool you can run a series of tests for a given URL, defined location (many available around the world) and browser. This is the most basic operation but you can choose also the visual performance comparison with other pages, test for mobile browsers or trace-route.\nFor the basic web performance test you can specify (in advanced options) connection speed, number of test to repeat, network packet tracing (in tcpdump format for further analysis), authorization, adds blocking and many other stuff. You can script operations to perform on target page.\nIn tests summary among standard metrics like loading time for scripts, styles, images and other elements, timing charts and content breakdown charts you will see a detailed HTTP requests/response report (with timing/offset not only for download itself but with DNS lookup or initial connection time). Next report tabs show a performance review with scoring and suggestions how you could improve you web page (by tuning some of the HTTP server settings like compression, utilizing CDN, caching or combining/minimizing JS/CSS). This feature should be familiar to all users of PageSpeed (for Firebug as additional plug-in, for Apache as module etc.).\nIn the end you can see how the user in previously defined location and web browser would see your page when it started loading and when the page has been fully loaded. This is available both on screen shots or in a captured video (the last one should be set in advanced options prior to the test). You can also save all the results in HAR (HTTP Archive) or CSV format (and tcpdump if you selected the right option).\nIn the project documentation I found some information about REST API and CLI for batch processing:\nThis command-line tool performs simple one-off batch testing. It loads the set of URLs in the input file as a batch, submits all of them to WebPageTest server which performs the tests, and then downloads the results of the successful tests and reports the failing tests. This tool is mainly implemented by the APIs in our batch processing library and hence can also serves as a sample usage of the batch processing APIs.\nThis sounds very interesting and I think I\u0026rsquo;ll investigate this in future.\nTest settings panel - basic settings Test settings panel - advanced settings Test settings panel - video capture for page rendering Test results - summary for all test runs in test set Test results details - waterfall view Test results details - connection timings view Test results details - requests details Test results - performance review checklist Test results - performance review with scoring Test results - performance review glossary Test results - Page Speed optimization check Test results - content breakdown charts for the first page load Test results - content breakdown charts for the next page load Test results - content breakdown charts with domains specified Test results - page screen shots for the next page loading steps Test results - page rendering video Test results history. You can compare the results of multiple tests here Have a nice profiling with WebPageTest!\nA notice for archeologists...🏺 This post was originaly published on my previous blog and moved here. Some links and resources might not be up to date. ","date":"21 March 2012","permalink":"/blog/webpagetest-distributed-web-profiler/","section":"Posts","summary":"\u003cp\u003eThere are many popular tools for web performance profiling/debugging these days. From my personal tool set I could mention \u003ca href=\"http://getfirebug.com/\" target=\"_blank\" rel=\"noreferrer\"\u003eFirebug\u003c/a\u003e, IE Developers Toolbar (integrated with IE from version \u0026gt; 8) or \u003ca href=\"http://www.fiddler2.com/fiddler2/\" target=\"_blank\" rel=\"noreferrer\"\u003eFiddler\u003c/a\u003e. The common problem with those tools is that they are installed and run locally on developers machine. Sometimes, for example when behaviour of the web app depends on loading all elements in the specific order or time, it is important to determine if those factors vary in different geographical locations. The download speed for China and Germany could be very different. If you face with this kind of problems I could recommend you a great distributed web performance profiler - \u003ca href=\"http://www.webpagetest.org/\" target=\"_blank\" rel=\"noreferrer\"\u003eWebPageTest\u003c/a\u003e.\u003c/p\u003e","title":"WebPagetest - distributed web profiler"},{"content":"Have you ever had a problem with a user complaining on and on that when he clicks some button in your application his computer hangs but it works fine on any other machine where you have tested it? The web service is consuming all the available memory in completely indeterministic way? Or maybe your web application is magically crashing on production machine when it works in the same usage scenario on development and test environment? If not, you are probably in about 1% of the most luckiest developers in the world (or you are not the developer).\nLast year I attended in Norwegian Developers Conference 2011 in Oslo where among many other great presentations I\u0026rsquo;ve watched the one presented by Ingo Rammer - \u0026ldquo;Hardcore .NET Production Debugging\u0026rdquo;. The presenter is one of co-funders of consulting company ThinkTecture and specializes in profiling and debugging.\nHere is the presentation description from NDC2011 agenda:\nHardcore .NET Production Debugging\nBut \u0026hellip; it used to work yesterday! In this newest version of his classic session, Ingo Rammer will introduce the hardcore and low–level tools used for production debugging of .NET applications. You\u0026rsquo;ll learn how to attack the nastiest bugs in your applications, how to look at what\u0026rsquo;s causing that grinding halt of your ASP.NET application and how to find the cause of that horrible memory leak in your Windows Forms application. Knowledge of these production debugging tools like WinDbg and SOS is not only important for cases when you really don\u0026rsquo;t have access to Visual Studio and your source code, but these tools also reveal a lot more information than just the regular managed code debuggers.\nThe presentation title speaks for itself. It looks really hardcore - watching this guy with WinDBG and other debugging tools from MS in action for the first time impressed me a lot. I recommend this to all .Net developers. No matter if you write web/desktop applications or system services you will find many advices that could save hours of your work, nerves and - priceless - the trust of your clients.\nYou can watch the presentation on-line directly on NDC2011 page here.\nA notice for archeologists...🏺 This post was originaly published on my previous blog and moved here. Some links and resources might not be up to date. ","date":"21 March 2012","permalink":"/blog/ingo-rammer-hardcore-net-production/","section":"Posts","summary":"\u003cp\u003eHave you ever had a problem with a user complaining on and on that when he clicks some button in your application his computer hangs but it works fine on any other machine where you have tested it? The web service is consuming all the available memory in completely indeterministic way? Or maybe your web application is magically crashing on production machine when it works in the same usage scenario on development and test environment? If not, you are probably in about 1% of the most luckiest developers in the world (or you are not the developer).\u003c/p\u003e","title":"Ingo Rammer - Hardcore .NET Production Debugging"},{"content":"I\u0026rsquo;m working in a team that builds and maintains a big document management system. Since SharePoint itself is not the best option for storing large amount of files (which can be also quite large) and serving them (performance, content DB size limitations etc.) I was evaluating some options for content externalization. I will not get into much details about the reasons for using such solutions because there are many well written articles about this on the Net (like \u0026ldquo;SharePoint 2010: Storing Documents on the File System with Remote Blob Storage\u0026rdquo; by Damon Armstrong). I will focus on one - Metalogix StoragePoint.\nI had chance to work with trial Enterprise 3.2.0.17 version of this product (with SP2010), so if you plan using the Standard version some features described bellow might not work.\nGeneral concept #Metalogix StoragePoint is a content externalization solution for SharePoint that works on both EBS (External BLOB Storage) and RBS (Remote BLOB Storage) levels. The first one is an option for SQL pre-2008R2 backend and also adds some features (higher granularity of externalization scopes such as site collections or lists). RBS came up with SQL 2008R2, so this kind of integration works well with SP2010. StoragePoint installation requires additional DB in SQL in order to maintain mappings between SharePoint items and target BLOBs location (and for its internal configuration).\nStorage endpoints management Target storage is defined as Endpoint. For each Endpoint SP administrator defines storage type (like file system or cloud), storage specific configuration (i.e. UNC path, credentials) and choose if the synchronization will be performed synchronously or asynchronously. Generally operations (endpoint selection and writing the BLOB) performed synchronously are blocking the user control while asynchronous operations use timer job (BLOB Migration Agent) with cache (a special additionally configured endpoint) in order to behave like the name suggests.\nAlso additional rules can be applied here - how big must be the attachment to be stored externally, when the endpoint should be treated as offline, warning notifications, compression, encryption (128-bit AES) etc.\nStorage profiles management Externalization configurations are grouped into profiles. Each profile defines a scope (web application, content db, site collection or lower), one or more endpoints and additional externalization rules. Despite the fact that more than one endpoint can be defined in profile only one will be used for storing the particular BLOB.\nCentral administration integration StoragePoint integrates its admin pages with central administration so all this configuration and operations can be performed in SharePoint in one place.\nFeatures #It supports many target storage types (adapters) including clouds: file system, EMC Atmos, EMC Centera, Hitachi HCAP, Windows Azure, Amazon S3, Rackspace Cloud Files (CloudFS), Dell DX, Carringo CAStor and AT\u0026amp;T Synaptic. It\u0026rsquo;s a good choice even if you plan using only the file system connector since it supports SMB shares (OOB SQL FileStream RBS provider allows using only paths on local machine). What I\u0026rsquo;m missing is the support for FTP (available i.e. in AvePoint DocAve).\nAdministrator can define how BLOBs will be organized on target storage. He can choose flat or folder based tree structure (including lists). Files can preserve its names and extensions (with additional GUID and version number to avoid filename collisions). For some further custom development meta-data export (to the same place where BLOBs are stored; in XML format) may be also a useful feature.\nRules that can be defined in storage profile in order to choose the right target endpoint are based on file size (i.e. send all attachments smaller than 500MB to Amazon 3S cloud and bigger to the local file system) or file type (like doc, pdf etc.). They can be applied selectively to specific SharePoint lists or folders.\nStoragePoint allows also to create additional rules for BLOB archiving (scheduled). It includes: item modification/creation date, related meta-data modification date, file (attachment) modification date and version retention.\nStoragePoint jobs management Target storages are monitored by a SharePoint job. E-mail notifications with warnings can be setup when total BLOBs size exceeds the limit defined for particular endpoint. Orphaned BLOBs (files with deleted SharePoint items) are also cleaned up by separate scheduled job (it\u0026rsquo;s the standard RBS/EBS approach recommended by MS). In cases where StoragePoint is being installed on existing SharePoint farm and externalization is setup for existing data the BLOBs can be externalized in the same scheduled manner as well as migrating the files back to contend db. Migration of the content between endpoints is also supported.\nMore details about the product can be found in documentation.\nConclusion #In my opinion Metalogix StoragePoint is a very advanced EBS/RBR solution with rich functionality. It\u0026rsquo;s relatively easy for configuration and maintenance. The minus is lack of FTP support and simultaneous writing to all endpoints defined for one profile (or at least I didn\u0026rsquo;t manage to configure it in that way).\nI know that this description is very brief but It\u0026rsquo;s rather an general overview on product capabilities from developers perspective than exhaustive article.\nHappy SharePointing!\nA notice for archeologists...🏺 This post was originaly published on my previous blog and moved here. Some links and resources might not be up to date. ","date":"8 March 2012","permalink":"/blog/blob-externalization-for-sharepoint/","section":"Posts","summary":"\u003cp\u003eI\u0026rsquo;m working in a team that builds and maintains a big document management system. Since SharePoint itself is not the best option for storing large amount of files (which can be also quite large) and serving them (performance, content DB size limitations etc.) I was evaluating some options for content externalization. I will not get into much details about the reasons for using such solutions because there are many well written articles about this on the Net (like \u003ca href=\"http://www.simple-talk.com/content/article.aspx?article=1280\" target=\"_blank\" rel=\"noreferrer\"\u003e\u0026ldquo;SharePoint 2010: Storing Documents on the File System with Remote Blob Storage\u0026rdquo;\u003c/a\u003e by Damon Armstrong). I will focus on one - \u003ca href=\"http://www.metalogix.com/Products/StoragePoint.aspx\" target=\"_blank\" rel=\"noreferrer\"\u003eMetalogix StoragePoint\u003c/a\u003e.\u003c/p\u003e","title":"BLOB externalization for SharePoint - Metalogix StoragePoint"},{"content":"Lately I was looking for some example on how to replace the attachment in document library (SP2010) without changing the version number but without any results. If you\u0026rsquo;ve faced the same problem here is the solution:\nSPWeb webSite = SPContext.Current.Web; SPDocumentLibrary library = webSite.Lists[libraryName] as SPDocumentLibrary; if (library == null) { throw new SPException(\u0026#34;There is no document library named \u0026#34; + libraryName); } using (MemoryStream docStream = new MemoryStream()) { BinaryWriter docWriter = new BinaryWriter(docStream); docWriter.Write(yourContentString); SPFile file = find(library, fileName); if (file == null) { throw new SPException(\u0026#34;File \u0026#34; + fileName + \u0026#34; not found\u0026#34;); } file.CheckOut(); file.SaveBinary(docStream); file.CheckIn(string.Empty, SPCheckinType.OverwriteCheckIn); docWriter.Close(); } Utility method find() - find a file by name - is defined as follow:\nprivate static SPFile find(SPDocumentLibrary library, string fileName) { SPFileCollection files = library.RootFolder.Files; for (int fileIdx = 0; fileIdx \u0026lt; files.Count; fileIdx++) { if (files[fileIdx].Name == fileName) { return files[fileIdx]; } } return null; } Im assuming that the file has already been uploaded to document library. The most important parts are:\nRetrieve reference to the file (SPFile). I\u0026rsquo;ve done this by iterating through all files in root folder of document library, but actually you can do this however you want Check out the file (SPFile.CheckOut()): file.CheckOut(); Replace the attachment. Convenient method for doing this is SPFile.SaveBinary which takes stream with content as an argument (I\u0026rsquo;ve used MemoryStream to read the content from a text box in my PoC, but you can pass the FileStream or something else): file.SaveBinary(docStream); Check-in the file (SPFile.CheckIn()). In order to save the attachment without increasing the version number you must specify SPCheckinType as OverwriteCheckIn file.CheckIn(string.Empty, SPCheckinType.OverwriteCheckIn); The alternative method is by using SPListItem.SystemUpdate with incrementListItemVersion set false. SystemUpdate will not change item modification date and modified by fields values. In this approach you will work with document library item instead of file.\nAdditionally if you want to change the attachment name, you can do this by with SPFile.MoveTo method (after changing the original attachment content):\nif (!string.IsNullOrEmpty(newFileName)) { string newPath = string.Format(\u0026#34;{0}/{1}\u0026#34;, file.ParentFolder.Url, newFileName); file.MoveTo(newPath, true); } The VS solution with code samples for this article is available on GitHub\nHappy SharePointing!\nA notice for archeologists...🏺 This post was originaly published on my previous blog and moved here. Some links and resources might not be up to date. ","date":"6 March 2012","permalink":"/blog/replace-attachment-in-document-library/","section":"Posts","summary":"\u003cp\u003eLately I was looking for some example on how to replace the attachment in document library (SP2010) without changing the version number but without any results. If you\u0026rsquo;ve faced the same problem here is the solution:\u003c/p\u003e","title":"Replace attachment in document library without changing version number"},{"content":"Few days ago I faced with the problem of linking to default view page of custom document library (for purposes of redirection with Source parameter after uploading new document and filling it\u0026rsquo;s meta data form).\nAt first I tried using SPList.Forms collection indexed with PAGETYPE enumeration as I found in article \u0026ldquo;How To Always Link to the Right Application Pages\u0026rdquo;. But every time I was trying to get the default view object this way:\nvar defaltViewUrl = documentLibrary.Forms[PAGETYPE.PAGE_DEFAULTVIEW].Url; I was getting the following exception:\nArgumentNullException: \u0026#34;Value cannot be null. Parameter name: formType\u0026#34; Fortunately this was working fine:\nvar defaultViewUrl = string.Format(\u0026#34;{0}/{1}\u0026#34;, documentLibrary.ParentWeb.Url, documentLibrary.DefaultView.Url); I\u0026rsquo;ve checked that SPList.Forms collection returns valid forms/pages objects only for these three values:\nPAGETYPE.PAGE_DISPLAYFORM PAGETYPE.PAGE_EDITFORM PAGETYPE.PAGE_NEWFORM For every other it throws arg null exception as described above. This looks definitely like a bad design in Sharepoint API (one of many\u0026hellip;) - the collection should be indexed with other enum that contain valid set of values.\nYou can find some more detailed info about the reason of this strange behavior on the short StackOverflow thread started by my question. How Stefan figured out that error is caused by enum to string conversion? I guess this mystery was revealed with ILSpy or some similar tool ;-)\nCheers!\nA notice for archeologists...🏺 This post was originaly published on my previous blog and moved here. Some links and resources might not be up to date. ","date":"23 January 2012","permalink":"/blog/error-value-cannot-be-null-parameter/","section":"Posts","summary":"\u003cp\u003eFew days ago I faced with the problem of linking to default view page of custom document library (for purposes of redirection with Source parameter after uploading new document and filling it\u0026rsquo;s meta data form).\u003c/p\u003e","title":"Error: \"Value cannot be null. Parameter name: formType\" retrieving default view page of document library"},{"content":"As I wrote previously the one of my recent projects was to create a new upload page for specific Document Library (based on custom list template) that will use Telerik Upload component (Silverlight) and will entirely replace OOB upload.aspx page. Looking for some suggestions how to do this I found few solutions but none of them met my criteria:\nUsing SharePoint Designer for customizing default upload page - I\u0026rsquo;m not sure if this could apply to document library; I also rejected this option from start as non-programmatic approach because of problems in future development and maintenance New document template redirection trick - looks simple but this wasn\u0026rsquo;t enough elegant solution for me (I know - who cares, it\u0026rsquo;s SharePoint after all\u0026hellip; ;-) Creating a custom action in ribbon and hiding the old one with javascript - the first part looks quite nice but the second is another hack; also, \u0026ldquo;Add new document\u0026rdquo; link at the bottom of default document library view still points to the standard /_layouts/Upload.aspx page\u0026hellip; Creating a custom action in ribbon with custom rendering template for ribbon - very nice solution when you want to change upload pages for all lists on farm, but this is not applicable in my case; still \u0026ldquo;Add new document\u0026rdquo; link at the bottom remains There were also other approaches like changing all related links with jQuery on client side but I would prefer some simple, elegant and server side solution that will not cause any problems on migration to the next version of SharePoint.\nBecause document libraries does not have the New form (new documents are uploaded or created from Office templates), setting links in list template schema in Forms section won\u0026rsquo;t work:\n\u0026lt;Forms\u0026gt; \u0026lt;Form Type=\u0026#34;DisplayForm\u0026#34; Url=\u0026#34;DispForm.aspx\u0026#34; SetupPath=\u0026#34;pages\\form.aspx\u0026#34; WebPartZoneID=\u0026#34;Main\u0026#34; /\u0026gt; \u0026lt;Form Type=\u0026#34;EditForm\u0026#34; Url=\u0026#34;EditForm.aspx\u0026#34; SetupPath=\u0026#34;pages\\form.aspx\u0026#34; WebPartZoneID=\u0026#34;Main\u0026#34; /\u0026gt; \u0026lt;Form Type=\u0026#34;NewForm\u0026#34; Url=\u0026#34;MyCustomUploadPage.aspx\u0026#34; SetupPath=\u0026#34;pages\\form.aspx\u0026#34; WebPartZoneID=\u0026#34;Main\u0026#34; /\u0026gt; \u0026lt;/Forms\u0026gt; For the same reason setting New form template in custom content type (inherited from Document CT) definition:\n\u0026lt;FormTemplates xmlns=\u0026#34;http://schemas.microsoft.com/sharepoint/v3/contenttype/forms\u0026#34;\u0026gt; \u0026lt;Display\u0026gt;DocumentLibraryForm\u0026lt;/Display\u0026gt; \u0026lt;Edit\u0026gt;DocumentLibraryForm\u0026lt;/Edit\u0026gt; \u0026lt;New\u0026gt;DocumentLibraryForm\u0026lt;/New\u0026gt; \u0026lt;/FormTemplates\u0026gt; will not change the upload page. Both methods still works for regular list or CTs not inherited from Document.\nThis may sound obvious for experienced SharePoint developers, but it was not for me. When I understood this I\u0026rsquo;ve started looking for methods of changing upload links (marked on picture below) both in ribbon and in default list view.\nCustom Documents Library The first one - ribbon button - looks fairly simple. I left it for future development after finding some articles describing creation of Custom Action and hiding the existing buttons. Of course finally I will have to do this.\nThe second - \u0026lsquo;+ Add new document\u0026rsquo; link - was definitely more challenging for me. First I\u0026rsquo;ve looked for the CT definition for Document Library and I found Toolbar definition in default view:\n\u0026lt;Views\u0026gt; \u0026lt;View BaseViewID=\u0026#34;0\u0026#34; Type=\u0026#34;HTML\u0026#34; MobileView=\u0026#34;TRUE\u0026#34; TabularView=\u0026#34;FALSE\u0026#34; FreeForm=\u0026#34;TRUE\u0026#34;\u0026gt; ... \u0026lt;Toolbar Position=\u0026#34;After\u0026#34; Type=\u0026#34;Freeform\u0026#34;\u0026gt; \u0026lt;IfHasRights\u0026gt; \u0026lt;RightsChoices\u0026gt; \u0026lt;RightsGroup PermAddListItems=\u0026#34;required\u0026#34; /\u0026gt; \u0026lt;/RightsChoices\u0026gt; \u0026lt;Then\u0026gt; \u0026lt;Switch\u0026gt; \u0026lt;Expr\u0026gt; \u0026lt;GetVar Name=\u0026#34;MasterVersion\u0026#34; /\u0026gt; \u0026lt;/Expr\u0026gt; \u0026lt;Case Value=\u0026#34;4\u0026#34;\u0026gt;\u0026lt;HTML\u0026gt;\u0026lt;![CDATA[\u0026lt;div class=\u0026#34;tb\u0026#34;\u0026gt;\u0026lt;img src=\u0026#34;/_layouts/images/caladd.gif\u0026#34; alt=\u0026#34;\u0026#34; /\u0026gt; \u0026lt;a class=\u0026#34;ms-addnew\u0026#34; id=\u0026#34;idAddNewDoc\u0026#34; href=\u0026#34;]]\u0026gt;\u0026lt;/HTML\u0026gt; \u0026lt;HttpVDir /\u0026gt;\u0026lt;HTML\u0026gt;\u0026lt;![CDATA[/_layouts/Upload.aspx?List=]]\u0026gt;\u0026lt;/HTML\u0026gt; \u0026lt;ListProperty Select=\u0026#34;Name\u0026#34; /\u0026gt;\u0026lt;HTML\u0026gt;\u0026lt;![CDATA[\u0026amp;RootFolder=]]\u0026gt;\u0026lt;/HTML\u0026gt; \u0026lt;GetVar Name=\u0026#34;RootFolder\u0026#34; URLEncode=\u0026#34;TRUE\u0026#34; /\u0026gt;\u0026lt;HTML\u0026gt;\u0026lt;![CDATA[\u0026#34; onclick=\u0026#34;javascript:NewItem(\u0026#39;]]\u0026gt;\u0026lt;/HTML\u0026gt; \u0026lt;ScriptQuote NotAddingQuote=\u0026#34;TRUE\u0026#34;\u0026gt; \u0026lt;HttpVDir /\u0026gt; \u0026lt;/ScriptQuote\u0026gt;\u0026lt;HTML\u0026gt;\u0026lt;![CDATA[/_layouts/Upload.aspx?List=]]\u0026gt;\u0026lt;/HTML\u0026gt; \u0026lt;ListProperty Select=\u0026#34;Name\u0026#34; /\u0026gt;\u0026lt;HTML\u0026gt;\u0026lt;![CDATA[\u0026amp;RootFolder=]]\u0026gt;\u0026lt;/HTML\u0026gt; \u0026lt;GetVar Name=\u0026#34;RootFolder\u0026#34; URLEncode=\u0026#34;TRUE\u0026#34; /\u0026gt;\u0026lt;HTML\u0026gt;\u0026lt;![CDATA[\u0026#39;, true);javascript:return false;\u0026#34; target=\u0026#34;_self\u0026#34;\u0026gt;]]\u0026gt;\u0026lt;/HTML\u0026gt;\u0026lt;HTML\u0026gt;$Resources:core,Add_New_Document;\u0026lt;/HTML\u0026gt;\u0026lt;HTML\u0026gt;\u0026lt;![CDATA[\u0026lt;/a\u0026gt;\u0026lt;/div\u0026gt;]]\u0026gt;\u0026lt;/HTML\u0026gt; \u0026lt;/Case\u0026gt; \u0026lt;Default\u0026gt;\u0026lt;HTML\u0026gt;\u0026lt;![CDATA[ \u0026lt;table width=\u0026#34;100%\u0026#34; cellpadding=\u0026#34;0\u0026#34; cellspacing=\u0026#34;0\u0026#34; border=\u0026#34;0\u0026#34; \u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td colspan=\u0026#34;2\u0026#34; class=\u0026#34;ms-partline\u0026#34;\u0026gt;\u0026lt;img src=\u0026#34;/_layouts/images/blank.gif\u0026#34; width=\u0026#39;1\u0026#39; height=\u0026#39;1\u0026#39; alt=\u0026#34;\u0026#34; /\u0026gt;\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td class=\u0026#34;ms-addnew\u0026#34; style=\u0026#34;padding-bottom: 3px\u0026#34;\u0026gt; \u0026lt;img src=\u0026#34;/_layouts/images/rect.gif\u0026#34; alt=\u0026#34;\u0026#34; /\u0026gt; \u0026lt;a class=\u0026#34;ms-addnew\u0026#34; id=\u0026#34;idAddNewDoc\u0026#34; href=\u0026#34;]]\u0026gt;\u0026lt;/HTML\u0026gt; \u0026lt;HttpVDir /\u0026gt;\u0026lt;HTML\u0026gt;\u0026lt;![CDATA[/_layouts/Upload.aspx?List=]]\u0026gt;\u0026lt;/HTML\u0026gt; \u0026lt;ListProperty Select=\u0026#34;Name\u0026#34; /\u0026gt;\u0026lt;HTML\u0026gt;\u0026lt;![CDATA[\u0026amp;RootFolder=]]\u0026gt;\u0026lt;/HTML\u0026gt; \u0026lt;GetVar Name=\u0026#34;RootFolder\u0026#34; URLEncode=\u0026#34;TRUE\u0026#34; /\u0026gt;\u0026lt;HTML\u0026gt;\u0026lt;![CDATA[\u0026#34; onclick=\u0026#34;javascript:NewItem(\u0026#39;]]\u0026gt;\u0026lt;/HTML\u0026gt; \u0026lt;ScriptQuote NotAddingQuote=\u0026#34;TRUE\u0026#34;\u0026gt; \u0026lt;HttpVDir /\u0026gt; \u0026lt;/ScriptQuote\u0026gt;\u0026lt;HTML\u0026gt;\u0026lt;![CDATA[/_layouts/Upload.aspx?List=]]\u0026gt;\u0026lt;/HTML\u0026gt; \u0026lt;ListProperty Select=\u0026#34;Name\u0026#34; /\u0026gt;\u0026lt;HTML\u0026gt;\u0026lt;![CDATA[\u0026amp;RootFolder=]]\u0026gt;\u0026lt;/HTML\u0026gt; \u0026lt;GetVar Name=\u0026#34;RootFolder\u0026#34; URLEncode=\u0026#34;TRUE\u0026#34; /\u0026gt;\u0026lt;HTML\u0026gt;\u0026lt;![CDATA[\u0026#39;, true);javascript:return false;\u0026#34; target=\u0026#34;_self\u0026#34;\u0026gt;]]\u0026gt;\u0026lt;/HTML\u0026gt;\u0026lt;HTML\u0026gt;$Resources:core,Add_New_Document;\u0026lt;/HTML\u0026gt;\u0026lt;HTML\u0026gt;\u0026lt;![CDATA[\u0026lt;/a\u0026gt; \u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt;\u0026lt;td\u0026gt;\u0026lt;img src=\u0026#34;/_layouts/images/blank.gif\u0026#34; width=\u0026#39;1\u0026#39; height=\u0026#39;5\u0026#39; alt=\u0026#34;\u0026#34; /\u0026gt;\u0026lt;/td\u0026gt;\u0026lt;/tr\u0026gt; \u0026lt;/table\u0026gt;]]\u0026gt;\u0026lt;/HTML\u0026gt; \u0026lt;/Default\u0026gt; \u0026lt;/Switch\u0026gt; \u0026lt;/Then\u0026gt; \u0026lt;/IfHasRights\u0026gt; \u0026lt;/Toolbar\u0026gt; ... \u0026lt;/View\u0026gt; ... \u0026lt;/Views\u0026gt; which was promising but I\u0026rsquo;ve noticed that \u0026lsquo;Add new document\u0026rsquo; in the view that is actually displayed is different than the one defined above (it calls NewItem2() JS function instead of NewItem()).\nAfter next few hours of searching and digging in 14 SharePoint folder I finally found the answer why the link at the bottom of Document Library default view is rendered in such way. In SharePoint 2010 every List View is actually XsltListViewWebPart which is well described in MSDN (this was also something new for me as for SP rookie). As the name suggest rendering is based on (quite complex) XSL transformations that for default views are defined in vwstyles.xsl imported in main.xsl which is linked in the following line in View definition of list template:\n\u0026lt;XslLink Default=\u0026#34;TRUE\u0026#34;\u0026gt;main.xsl\u0026lt;/XslLink\u0026gt; In case of \u0026lsquo;Add new document\u0026rsquo; link transformations end on this template:\n\u0026lt;xsl:template name=\u0026#34;Freeform\u0026#34;\u0026gt; ... \u0026lt;xsl:variable name=\u0026#34;Url\u0026#34;\u0026gt; \u0026lt;xsl:choose\u0026gt; \u0026lt;xsl:when test=\u0026#34;List/@TemplateType=\u0026#39;119\u0026#39;\u0026#34;\u0026gt;\u0026lt;xsl:value-of select=\u0026#34;$HttpVDir\u0026#34;/\u0026gt;/_layouts/CreateWebPage.aspx?List=\u0026lt;xsl:value-of select=\u0026#34;$List\u0026#34;/\u0026gt;\u0026amp;RootFolder=\u0026lt;xsl:value-of select=\u0026#34;$XmlDefinition/List/@RootFolder\u0026#34;/\u0026gt;\u0026lt;/xsl:when\u0026gt; \u0026lt;xsl:when test=\u0026#34;$IsDocLib\u0026#34;\u0026gt;\u0026lt;xsl:value-of select=\u0026#34;$HttpVDir\u0026#34;/\u0026gt;/_layouts/Upload.aspx?List=\u0026lt;xsl:value-of select=\u0026#34;$List\u0026#34;/\u0026gt;\u0026amp;RootFolder=\u0026lt;xsl:value-of select=\u0026#34;$XmlDefinition/List/@RootFolder\u0026#34;/\u0026gt;\u0026lt;/xsl:when\u0026gt; \u0026lt;xsl:otherwise\u0026gt;\u0026lt;xsl:value-of select=\u0026#34;$ENCODED_FORM_NEW\u0026#34;/\u0026gt;\u0026lt;/xsl:otherwise\u0026gt; \u0026lt;/xsl:choose\u0026gt; \u0026lt;/xsl:variable\u0026gt; ... \u0026lt;/xsl:template\u0026gt; which renders the link to default Upload.aspx page in _layouts folder. As you see for Document Libraries this link is always the same, no matter what you will put in New form in CT definition or list template.\nThe simple, programmatic and server side solution for changing the link I was looking for and which worked for me was to deploy my own xsl (i.e. custom_views.xsl):\n\u0026lt;xsl:stylesheet xmlns:x=\u0026#34;http://www.w3.org/2001/XMLSchema\u0026#34; xmlns:d=\u0026#34;http://schemas.microsoft.com/sharepoint/dsp\u0026#34; version=\u0026#34;1.0\u0026#34; exclude-result-prefixes=\u0026#34;xsl msxsl ddwrt\u0026#34; xmlns:ddwrt=\u0026#34;http://schemas.microsoft.com/WebParts/v2/DataView/runtime\u0026#34; xmlns:asp=\u0026#34;http://schemas.microsoft.com/ASPNET/20\u0026#34; xmlns:__designer=\u0026#34;http://schemas.microsoft.com/WebParts/v2/DataView/designer\u0026#34; xmlns:xsl=\u0026#34;http://www.w3.org/1999/XSL/Transform\u0026#34; xmlns:msxsl=\u0026#34;urn:sppchemas-microsoft-com:xslt\u0026#34; xmlns:SharePoint=\u0026#34;Microsoft.SharePoint.WebControls\u0026#34; xmlns:ddwrt2=\u0026#34;urn:frontpage:internal\u0026#34; ddwrt:oob=\u0026#34;true\u0026#34;\u0026gt; \u0026lt;xsl:include href=\u0026#34;/_layouts/xsl/main.xsl\u0026#34;/\u0026gt; \u0026lt;xsl:include href=\u0026#34;/_layouts/xsl/internal.xsl\u0026#34;/\u0026gt; \u0026lt;xsl:template name=\u0026#34;Freeform\u0026#34;\u0026gt; \u0026lt;xsl:param name=\u0026#34;AddNewText\u0026#34;/\u0026gt; \u0026lt;xsl:param name=\u0026#34;ID\u0026#34;/\u0026gt; \u0026lt;xsl:variable name=\u0026#34;Url\u0026#34;\u0026gt; \u0026lt;xsl:choose\u0026gt; \u0026lt;xsl:when test=\u0026#34;List/@TemplateType=\u0026#39;119\u0026#39;\u0026#34;\u0026gt; \u0026lt;xsl:value-of select=\u0026#34;$HttpVDir\u0026#34;/\u0026gt;/_layouts/CreateWebPage.aspx?List=\u0026lt;xsl:value-of select=\u0026#34;$List\u0026#34;/\u0026gt;\u0026amp;RootFolder=\u0026lt;xsl:value-of select=\u0026#34;$XmlDefinition/List/@RootFolder\u0026#34;/\u0026gt; \u0026lt;/xsl:when\u0026gt; \u0026lt;xsl:when test=\u0026#34;$IsDocLib\u0026#34;\u0026gt; \u0026lt;xsl:value-of select=\u0026#34;$HttpVDir\u0026#34;/\u0026gt;/_layouts/LargeFileUploadWithSLToSP/LibraryUpload.aspx?documentLibraryId=\u0026lt;xsl:value-of select=\u0026#34;$List\u0026#34;/\u0026gt;\u0026amp;RootFolder=\u0026lt;xsl:value-of select=\u0026#34;$XmlDefinition/List/@RootFolder\u0026#34;/\u0026gt; \u0026lt;/xsl:when\u0026gt; \u0026lt;xsl:otherwise\u0026gt; \u0026lt;xsl:value-of select=\u0026#34;$ENCODED_FORM_NEW\u0026#34;/\u0026gt; \u0026lt;/xsl:otherwise\u0026gt; \u0026lt;/xsl:choose\u0026gt; \u0026lt;/xsl:variable\u0026gt; ... \u0026lt;/xsl:template\u0026gt; \u0026lt;/xsl:stylesheet\u0026gt; in a mapped VS solution folder (which will overwrite only the \u0026ldquo;Freeform\u0026rdquo; template) to Layouts and put the link to it in XslLink in View definition:\n\u0026lt;View BaseViewID=\u0026#34;1\u0026#34; Type=\u0026#34;HTML\u0026#34; WebPartZoneID=\u0026#34;Main\u0026#34; DisplayName=\u0026#34;$Resources:core,All_Documents;\u0026#34; DefaultView=\u0026#34;TRUE\u0026#34; DefaultViewForContentType=\u0026#34;TRUE\u0026#34; MobileView=\u0026#34;True\u0026#34; MobileDefaultView=\u0026#34;True\u0026#34; SetupPath=\u0026#34;pages\\viewpage.aspx\u0026#34; ImageUrl=\u0026#34;/_layouts/images/dlicon.png\u0026#34; Url=\u0026#34;Forms/AllItems.aspx\u0026#34;\u0026gt; \u0026lt;XslLink Default=\u0026#34;true\u0026#34;\u0026gt;custom_views.xsl\u0026lt;/XslLink\u0026gt; ... \u0026lt;/View\u0026gt; Here is a sample Visual Studio 2010 solution structure:\nVS Solution The results (with Telerik Silverlight upload component) are shown below:\nCustom upload solution (1) Custom upload solution (2) This satisfies all my requirements: it\u0026rsquo;s a full programmatic server side solution, it does not require any javascript tricks (high risk that in next version of SP will not work, when for example the css style names will change), it can be applied selectively to specific Document Libraries based on custom list templates (no need to change the default behavior on entire WFE) and it is simple.\nThe VS solution with code samples for this article is available on GitHub\nI hope this short article will spare your time.\nHappy SharePointing!\nA notice for archeologists...🏺 This post was originaly published on my previous blog and moved here. Some links and resources might not be up to date. ","date":"20 January 2012","permalink":"/blog/custom-upload-form-for-document-library/","section":"Posts","summary":"\u003cp\u003eAs I wrote previously the one of my recent projects was to create a new upload page for specific Document Library (based on custom list template) that will use Telerik Upload component (Silverlight) and will entirely replace OOB \u003ccode\u003eupload.aspx\u003c/code\u003e page. Looking for some suggestions how to do this I found few solutions but none of them met my criteria:\u003c/p\u003e","title":"Custom upload form for document library in SharePoint 2010 - programmatic approach"},{"content":"Welcome on my new blog (and also the first one).\nIn few days I plan to picture some of my first experiences with development in SharePoint 2010. They are related with my current project - custom upload form with Telerik upload control (Silverlight) that will entirely replace the standard OOB upload mechanism for custom document libraries.\nA notice for archeologists...🏺 This post was originaly published on my previous blog and moved here. Some links and resources might not be up to date. ","date":"18 January 2012","permalink":"/blog/hello-world/","section":"Posts","summary":"\u003cp\u003eWelcome on my new blog (and also the first one).\u003cbr\u003e\nIn few days I plan to picture some of my first experiences with development in SharePoint 2010. They are related with my current project - custom upload form with \u003ca href=\"http://www.telerik.com/help/silverlight/radupload-overview.html\" target=\"_blank\" rel=\"noreferrer\"\u003eTelerik upload control\u003c/a\u003e (Silverlight) that will entirely replace the standard OOB upload mechanism for custom document libraries.\u003c/p\u003e","title":"Hello world!"},{"content":"","date":null,"permalink":"/pages/","section":"","summary":"","title":""},{"content":"Hi! My name is Marek Mierzwa. Who I am?\nFirst and foremost - a human being 😊 A humanist, I guess 🤔 A proud husband and father of two wonderful children. And a dog caretaker 🐾 A software engineer \u0026amp; leader that helps product and technology teams to succeed. Beside of that, I love good food, enjoy the world and people 🥰 I like traveling and exploring (physically, mentally, spiritually), learning and trying new stuff.\nMy hobbies? DIY projects (various types), HAM radio (my callsign is SP9GO) electronics, astronomy/astronautics, coding (not only for living), tabletop RPGs, retro computing, puzzles, psychology, Lego and much more than I can think of right now. Some of those topics will show on this blog for sure.\nIf you want to read more on my professional info, my LinkedIn profile is the place to go.\n-----BEGIN GEEK CODE BLOCK VERSION 5.1----- GCS^/GE^ A+ B-:_:_:_:-- Cx(-) D+:-- CM++ MW++ UL++ MC+ Lbash/LC#+/Lgo+/Ljs/Lpy/Lsql/Lwps IO+:_ PGP G:mmierzwa E+ H+ PS+ PE++ TFF+/Tmon++/TRM++/TSG+/TST+++ RPG+(_) BK++ K++ -----END GEEK CODE BLOCK VERSION 5.1----- ","date":null,"permalink":"/about/","section":"","summary":"\u003cp\u003eHi! My name is Marek Mierzwa. Who I am?\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eFirst and foremost - a human being 😊 A humanist, I guess 🤔\u003c/li\u003e\n\u003cli\u003eA proud husband and father of two wonderful children. And a dog caretaker 🐾\u003c/li\u003e\n\u003cli\u003eA software engineer \u0026amp; leader that helps product and technology teams to succeed.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eBeside of that, I love good food, enjoy the world and people 🥰 I like traveling and exploring (physically, mentally, spiritually), learning and trying new stuff.\u003c/p\u003e","title":"about me"},{"content":"What Are Cookies #As is common practice with almost all professional websites this site uses cookies, which are tiny files that are downloaded to your computer, to improve your experience. This page describes what information they gather, how we use it and why we sometimes need to store these cookies. We will also share how you can prevent these cookies from being stored however this may downgrade or \u0026lsquo;break\u0026rsquo; certain elements of the sites functionality.\nFor more general information on cookies see the Wikipedia article on HTTP Cookies.\nHow We Use Cookies #We use cookies for a variety of reasons detailed below. Unfortunately in most cases there are no industry standard options for disabling cookies without completely disabling the functionality and features they add to this site. It is recommended that you leave on all cookies if you are not sure whether you need them or not in case they are used to provide a service that you use.\nDisabling Cookies #You can prevent the setting of cookies by adjusting the settings on your browser (see your browser Help for how to do this). Be aware that disabling cookies will affect the functionality of this and many other websites that you visit. Disabling cookies will usually result in also disabling certain functionality and features of the this site. Therefore it is recommended that you do not disable cookies.\nFor more details, see aboutcookies.org.\nThe Cookies We Set #If you create an account with us then we will use cookies for the management of the signup process and general administration. These cookies will usually be deleted when you log out however in some cases they may remain afterwards to remember your site preferences when logged out.\nWe use cookies when you are logged in so that we can remember this fact. This prevents you from having to log in every single time you visit a new page. These cookies are typically removed or cleared when you log out to ensure that you can only access restricted features and areas when logged in.\nThis site offers newsletter or email subscription services and cookies may be used to remember if you are already registered and whether to show certain notifications which might only be valid to subscribed/unsubscribed users.\nFrom time to time we offer user surveys and questionnaires to provide you with interesting insights, helpful tools, or to understand our user base more accurately. These surveys may use cookies to remember who has already taken part in a survey or to provide you with accurate results after you change pages.\nWhen you submit data to through a form such as those found on contact pages or comment forms cookies may be set to remember your user details for future correspondence.\nIn order to provide you with a great experience on this site we provide the functionality to set your preferences for how this site runs when you use it. In order to remember your preferences we need to set cookies so that this information can be called whenever you interact with a page is affected by your preferences.\nThird Party Cookies #In some special cases we also use cookies provided by trusted third parties. The following section details which third party cookies you might encounter through this site.\nThis site uses Google Analytics which is one of the most widespread and trusted analytics solution on the web for helping us to understand how you use the site and ways that we can improve your experience. These cookies may track things such as how long you spend on the site and the pages that you visit so we can continue to produce engaging content.\nFor more information on Google Analytics cookies, see the official Google Analytics page.\nThird party analytics are used to track and measure usage of this site so that we can continue to produce engaging content. These cookies may track things such as how long you spend on the site or pages you visit which helps us to understand how we can improve the site for you.\nFrom time to time we test new features and make subtle changes to the way that the site is delivered. When we are still testing new features these cookies may be used to ensure that you receive a consistent experience whilst on the site whilst ensuring we understand which optimisations our users appreciate the most.\nAs we sell products it\u0026rsquo;s important for us to understand statistics about how many of the visitors to our site actually make a purchase and as such this is the kind of data that these cookies will track. This is important to you as it means that we can accurately make business predictions that allow us to monitor our advertising and product costs to ensure the best possible price.\nThe Google AdSense service we use to serve advertising uses a DoubleClick cookie to serve more relevant ads across the web and limit the number of times that a given ad is shown to you.\nFor more information on Google AdSense see the official Google AdSense privacy FAQ.\nWe use adverts to offset the costs of running this site and provide funding for further development. The behavioural advertising cookies used by this site are designed to ensure that we provide you with the most relevant adverts where possible by anonymously tracking your interests and presenting similar things that may be of interest.\nIn some cases we may provide you with custom content based on what you tell us about yourself either directly or indirectly by linking a social media account. These types of cookies simply allow us to provide you with content that we feel may be of interest to you.\nSeveral partners advertise on our behalf and affiliate tracking cookies simply allow us to see if our customers have come to the site through one of our partner sites so that we can credit them appropriately and where applicable allow our affiliate partners to provide any bonus that they may provide you for making a purchase.\nWe also use social media buttons and/or plugins on this site that allow you to connect with your social network in various ways. For these to work the following social media sites including; Facebook, Twitter, Pinterest, LinkedIn, Yandex, YouTube, Twitter, Google+, Disqus, will set cookies through our site which may be used to enhance your profile on their site or contribute to the data they hold for various purposes outlined in their respective privacy policies.\nMore Information #Hopefully that has clarified things for you and as was previously mentioned if there is something that you aren\u0026rsquo;t sure whether you need or not it\u0026rsquo;s usually safer to leave cookies enabled in case it does interact with one of the features you use on our site.\nHowever if you are still looking for more information then you can contact us through one of our preferred contact methods.\n","date":null,"permalink":"/cookies/","section":"","summary":"\u003ch2 id=\"what-are-cookies\" class=\"relative group\"\u003eWhat Are Cookies \u003cspan class=\"absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100\"\u003e\u003ca class=\"group-hover:text-primary-300 dark:group-hover:text-neutral-700\" style=\"text-decoration-line: none !important;\" href=\"#what-are-cookies\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\u003c/span\u003e\u003c/h2\u003e\u003cp\u003eAs is common practice with almost all professional websites this site uses cookies, which are tiny files that are downloaded to your computer, to improve your experience. This page describes what information they gather, how we use it and why we sometimes need to store these cookies. We will also share how you can prevent these cookies from being stored however this may downgrade or \u0026lsquo;break\u0026rsquo; certain elements of the sites functionality.\u003c/p\u003e","title":"cookies policy"}]